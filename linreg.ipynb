{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmisra/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uproot as ur\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "for (path, dirnames, filenames) in os.walk('/home/dmisra/eic/zdc_neutron_samples/'):\n",
    "    paths.extend(os.path.join(path, name) for name in filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "\n",
    "for path in paths:\n",
    "    with ur.open(path) as file:\n",
    "       tree = file[\"events\"]\n",
    "       samples[os.path.basename(f'{path}')] = tree.arrays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Energy Deposition in All ZDC Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def components_edep(data, count):\n",
    "    SiPix_edep = []\n",
    "    Crystal_edep = []\n",
    "    WSi_edep = []\n",
    "    PbSi_edep = []\n",
    "    PbScint_edep = []\n",
    "    energylabels = []\n",
    "    \n",
    "    for i in range(count):\n",
    "        SiPix_energies = np.array(data[\"ZDC_SiliconPix_Hits.energy\"][i])\n",
    "        SiPix_edep.append(sum(SiPix_energies))\n",
    "\n",
    "        Crystal_energies = np.array(data[\"ZDCEcalHits.energy\"][i])\n",
    "        Crystal_edep.append(sum(Crystal_energies))\n",
    "\n",
    "        WSi_energies = np.array(data[\"ZDC_WSi_Hits.energy\"][i])\n",
    "        WSi_edep.append(sum(WSi_energies))\n",
    "\n",
    "        PbSi_energies = np.array(data[\"ZDC_PbSi_Hits.energy\"][i])\n",
    "        PbSi_edep.append(sum(PbSi_energies))\n",
    "\n",
    "        PbScint_energies = np.array(data[\"ZDCHcalHits.energy\"][i])\n",
    "        PbScint_edep.append(sum(PbScint_energies))\n",
    "\n",
    "        label = np.sqrt(data[\"MCParticles.momentum.x\"][0,0]**2 + data[\"MCParticles.momentum.y\"][0,0]**2 + data[\"MCParticles.momentum.z\"][0,0]**2)\n",
    "        energylabels.append(label)\n",
    "\n",
    "    return pd.DataFrame([SiPix_edep, Crystal_edep, WSi_edep, PbSi_edep, PbScint_edep, energylabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for linear regression\n",
    "data = [components_edep(samples[key],10000) for key in samples]\n",
    "data_df = pd.concat(data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f50455ae170>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor_10GeV = torch.from_numpy(data[0].values).T.float()\n",
    "data_tensor_20GeV = torch.from_numpy(data[1].values).T.float()\n",
    "data_tensor_50GeV = torch.from_numpy(data[2].values).T.float()\n",
    "data_tensor_100GeV = torch.from_numpy(data[3].values).T.float()\n",
    "data_tensor_150GeV = torch.from_numpy(data[4].values).T.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.from_numpy(data_df.values).T.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(tensor):\n",
    "    return tensor[:,:5]\n",
    "\n",
    "def labels(tensor):\n",
    "    return tensor[:,5].unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = features(data_tensor)\n",
    "y = labels(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train/test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "input_size = 5\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = nn.Linear(input_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = LinearRegression(input_size,output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.load_state_dict(torch.load('/home/dmisra/eic/model_0')())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_0.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.fill_(1)\n",
    "         layer.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model_0.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MSE Train Loss: 6791.57373046875 | MSE Test Loss: 6359.8251953125\n",
      "Epoch: 100 | MSE Train Loss: 2286.188720703125 | MSE Test Loss: 2239.558837890625\n",
      "Epoch: 200 | MSE Train Loss: 1657.0015869140625 | MSE Test Loss: 1638.7581787109375\n",
      "Epoch: 300 | MSE Train Loss: 1446.3629150390625 | MSE Test Loss: 1437.9261474609375\n",
      "Epoch: 400 | MSE Train Loss: 1318.3914794921875 | MSE Test Loss: 1314.1546630859375\n",
      "Epoch: 500 | MSE Train Loss: 1219.106689453125 | MSE Test Loss: 1217.0574951171875\n",
      "Epoch: 600 | MSE Train Loss: 1137.0411376953125 | MSE Test Loss: 1136.3319091796875\n",
      "Epoch: 700 | MSE Train Loss: 1067.81103515625 | MSE Test Loss: 1068.0235595703125\n",
      "Epoch: 800 | MSE Train Loss: 1008.7022705078125 | MSE Test Loss: 1009.5969848632812\n",
      "Epoch: 900 | MSE Train Loss: 957.7232055664062 | MSE Test Loss: 959.146728515625\n",
      "Epoch: 1000 | MSE Train Loss: 913.3485717773438 | MSE Test Loss: 915.1951293945312\n",
      "Epoch: 1100 | MSE Train Loss: 874.3941040039062 | MSE Test Loss: 876.5870971679688\n",
      "Epoch: 1200 | MSE Train Loss: 839.9321899414062 | MSE Test Loss: 842.4155883789062\n",
      "Epoch: 1300 | MSE Train Loss: 809.2312622070312 | MSE Test Loss: 811.9630737304688\n",
      "Epoch: 1400 | MSE Train Loss: 781.7094116210938 | MSE Test Loss: 784.6580810546875\n",
      "Epoch: 1500 | MSE Train Loss: 756.8998413085938 | MSE Test Loss: 760.041015625\n",
      "Epoch: 1600 | MSE Train Loss: 734.424072265625 | MSE Test Loss: 737.7390747070312\n",
      "Epoch: 1700 | MSE Train Loss: 713.9727172851562 | MSE Test Loss: 717.4469604492188\n",
      "Epoch: 1800 | MSE Train Loss: 695.2898559570312 | MSE Test Loss: 698.91162109375\n",
      "Epoch: 1900 | MSE Train Loss: 678.1622924804688 | MSE Test Loss: 681.9221801757812\n",
      "Epoch: 2000 | MSE Train Loss: 662.4107055664062 | MSE Test Loss: 666.3004760742188\n",
      "Epoch: 2100 | MSE Train Loss: 647.8818969726562 | MSE Test Loss: 651.8948974609375\n",
      "Epoch: 2200 | MSE Train Loss: 634.4453735351562 | MSE Test Loss: 638.5753784179688\n",
      "Epoch: 2300 | MSE Train Loss: 621.9876098632812 | MSE Test Loss: 626.2291870117188\n",
      "Epoch: 2400 | MSE Train Loss: 610.41015625 | MSE Test Loss: 614.7579956054688\n",
      "Epoch: 2500 | MSE Train Loss: 599.62646484375 | MSE Test Loss: 604.075927734375\n",
      "Epoch: 2600 | MSE Train Loss: 589.5604248046875 | MSE Test Loss: 594.106689453125\n",
      "Epoch: 2700 | MSE Train Loss: 580.1445922851562 | MSE Test Loss: 584.7833251953125\n",
      "Epoch: 2800 | MSE Train Loss: 571.3184814453125 | MSE Test Loss: 576.045166015625\n",
      "Epoch: 2900 | MSE Train Loss: 563.0286865234375 | MSE Test Loss: 567.8388061523438\n",
      "Epoch: 3000 | MSE Train Loss: 555.2268676757812 | MSE Test Loss: 560.1162109375\n",
      "Epoch: 3100 | MSE Train Loss: 547.8698120117188 | MSE Test Loss: 552.834228515625\n",
      "Epoch: 3200 | MSE Train Loss: 540.9183959960938 | MSE Test Loss: 545.95361328125\n",
      "Epoch: 3300 | MSE Train Loss: 534.33740234375 | MSE Test Loss: 539.4392700195312\n",
      "Epoch: 3400 | MSE Train Loss: 528.094970703125 | MSE Test Loss: 533.2594604492188\n",
      "Epoch: 3500 | MSE Train Loss: 522.1622314453125 | MSE Test Loss: 527.3853759765625\n",
      "Epoch: 3600 | MSE Train Loss: 516.5131225585938 | MSE Test Loss: 521.790771484375\n",
      "Epoch: 3700 | MSE Train Loss: 511.12359619140625 | MSE Test Loss: 516.4520263671875\n",
      "Epoch: 3800 | MSE Train Loss: 505.9722595214844 | MSE Test Loss: 511.34765625\n",
      "Epoch: 3900 | MSE Train Loss: 501.0393981933594 | MSE Test Loss: 506.4580993652344\n",
      "Epoch: 4000 | MSE Train Loss: 496.30731201171875 | MSE Test Loss: 501.7657470703125\n",
      "Epoch: 4100 | MSE Train Loss: 491.7596435546875 | MSE Test Loss: 497.2542419433594\n",
      "Epoch: 4200 | MSE Train Loss: 487.3817138671875 | MSE Test Loss: 492.9090881347656\n",
      "Epoch: 4300 | MSE Train Loss: 483.159912109375 | MSE Test Loss: 488.7167053222656\n",
      "Epoch: 4400 | MSE Train Loss: 479.0819396972656 | MSE Test Loss: 484.6650390625\n",
      "Epoch: 4500 | MSE Train Loss: 475.1366882324219 | MSE Test Loss: 480.7430114746094\n",
      "Epoch: 4600 | MSE Train Loss: 471.3140869140625 | MSE Test Loss: 476.94049072265625\n",
      "Epoch: 4700 | MSE Train Loss: 467.60479736328125 | MSE Test Loss: 473.2484130859375\n",
      "Epoch: 4800 | MSE Train Loss: 464.0000915527344 | MSE Test Loss: 469.6582946777344\n",
      "Epoch: 4900 | MSE Train Loss: 460.49273681640625 | MSE Test Loss: 466.16259765625\n",
      "Epoch: 5000 | MSE Train Loss: 457.0752868652344 | MSE Test Loss: 462.7543640136719\n",
      "Epoch: 5100 | MSE Train Loss: 453.7414855957031 | MSE Test Loss: 459.4271545410156\n",
      "Epoch: 5200 | MSE Train Loss: 450.48541259765625 | MSE Test Loss: 456.17529296875\n",
      "Epoch: 5300 | MSE Train Loss: 447.3016052246094 | MSE Test Loss: 452.9934997558594\n",
      "Epoch: 5400 | MSE Train Loss: 444.1853942871094 | MSE Test Loss: 449.8770446777344\n",
      "Epoch: 5500 | MSE Train Loss: 441.1321105957031 | MSE Test Loss: 446.8215026855469\n",
      "Epoch: 5600 | MSE Train Loss: 438.1376953125 | MSE Test Loss: 443.82281494140625\n",
      "Epoch: 5700 | MSE Train Loss: 435.1986083984375 | MSE Test Loss: 440.87744140625\n",
      "Epoch: 5800 | MSE Train Loss: 432.3111877441406 | MSE Test Loss: 437.9819641113281\n",
      "Epoch: 5900 | MSE Train Loss: 429.47259521484375 | MSE Test Loss: 435.1333923339844\n",
      "Epoch: 6000 | MSE Train Loss: 426.67974853515625 | MSE Test Loss: 432.32904052734375\n",
      "Epoch: 6100 | MSE Train Loss: 423.93011474609375 | MSE Test Loss: 429.56640625\n",
      "Epoch: 6200 | MSE Train Loss: 421.2213134765625 | MSE Test Loss: 426.8430480957031\n",
      "Epoch: 6300 | MSE Train Loss: 418.5511779785156 | MSE Test Loss: 424.15679931640625\n",
      "Epoch: 6400 | MSE Train Loss: 415.9176940917969 | MSE Test Loss: 421.5059509277344\n",
      "Epoch: 6500 | MSE Train Loss: 413.31903076171875 | MSE Test Loss: 418.8885803222656\n",
      "Epoch: 6600 | MSE Train Loss: 410.7535400390625 | MSE Test Loss: 416.3031311035156\n",
      "Epoch: 6700 | MSE Train Loss: 408.2196350097656 | MSE Test Loss: 413.7481384277344\n",
      "Epoch: 6800 | MSE Train Loss: 405.7159118652344 | MSE Test Loss: 411.22216796875\n",
      "Epoch: 6900 | MSE Train Loss: 403.2410888671875 | MSE Test Loss: 408.7240905761719\n",
      "Epoch: 7000 | MSE Train Loss: 400.7939147949219 | MSE Test Loss: 406.2527770996094\n",
      "Epoch: 7100 | MSE Train Loss: 398.3733825683594 | MSE Test Loss: 403.8069763183594\n",
      "Epoch: 7200 | MSE Train Loss: 395.97845458984375 | MSE Test Loss: 401.385986328125\n",
      "Epoch: 7300 | MSE Train Loss: 393.6081848144531 | MSE Test Loss: 398.9888916015625\n",
      "Epoch: 7400 | MSE Train Loss: 391.2618103027344 | MSE Test Loss: 396.6148376464844\n",
      "Epoch: 7500 | MSE Train Loss: 388.9384460449219 | MSE Test Loss: 394.2630615234375\n",
      "Epoch: 7600 | MSE Train Loss: 386.6373596191406 | MSE Test Loss: 391.932861328125\n",
      "Epoch: 7700 | MSE Train Loss: 384.3578796386719 | MSE Test Loss: 389.62359619140625\n",
      "Epoch: 7800 | MSE Train Loss: 382.0993957519531 | MSE Test Loss: 387.3348388671875\n",
      "Epoch: 7900 | MSE Train Loss: 379.8612976074219 | MSE Test Loss: 385.0657653808594\n",
      "Epoch: 8000 | MSE Train Loss: 377.6431884765625 | MSE Test Loss: 382.816162109375\n",
      "Epoch: 8100 | MSE Train Loss: 375.4444580078125 | MSE Test Loss: 380.58544921875\n",
      "Epoch: 8200 | MSE Train Loss: 373.2646179199219 | MSE Test Loss: 378.37322998046875\n",
      "Epoch: 8300 | MSE Train Loss: 371.1033630371094 | MSE Test Loss: 376.1791076660156\n",
      "Epoch: 8400 | MSE Train Loss: 368.96014404296875 | MSE Test Loss: 374.0026550292969\n",
      "Epoch: 8500 | MSE Train Loss: 366.834716796875 | MSE Test Loss: 371.8436279296875\n",
      "Epoch: 8600 | MSE Train Loss: 364.7267761230469 | MSE Test Loss: 369.70166015625\n",
      "Epoch: 8700 | MSE Train Loss: 362.6358642578125 | MSE Test Loss: 367.576416015625\n",
      "Epoch: 8800 | MSE Train Loss: 360.5616149902344 | MSE Test Loss: 365.46771240234375\n",
      "Epoch: 8900 | MSE Train Loss: 358.5039367675781 | MSE Test Loss: 363.3751525878906\n",
      "Epoch: 9000 | MSE Train Loss: 356.46246337890625 | MSE Test Loss: 361.2985534667969\n",
      "Epoch: 9100 | MSE Train Loss: 354.4368896484375 | MSE Test Loss: 359.2376708984375\n",
      "Epoch: 9200 | MSE Train Loss: 352.42706298828125 | MSE Test Loss: 357.19232177734375\n",
      "Epoch: 9300 | MSE Train Loss: 350.4326477050781 | MSE Test Loss: 355.1622009277344\n",
      "Epoch: 9400 | MSE Train Loss: 348.4534912109375 | MSE Test Loss: 353.1471862792969\n",
      "Epoch: 9500 | MSE Train Loss: 346.4893493652344 | MSE Test Loss: 351.1470642089844\n",
      "Epoch: 9600 | MSE Train Loss: 344.5401611328125 | MSE Test Loss: 349.1617126464844\n",
      "Epoch: 9700 | MSE Train Loss: 342.6056823730469 | MSE Test Loss: 347.1910400390625\n",
      "Epoch: 9800 | MSE Train Loss: 340.6856384277344 | MSE Test Loss: 345.2347106933594\n",
      "Epoch: 9900 | MSE Train Loss: 338.7798767089844 | MSE Test Loss: 343.29254150390625\n",
      "Epoch: 10000 | MSE Train Loss: 336.8883972167969 | MSE Test Loss: 341.36456298828125\n",
      "Epoch: 10100 | MSE Train Loss: 335.01080322265625 | MSE Test Loss: 339.450439453125\n",
      "Epoch: 10200 | MSE Train Loss: 333.1471252441406 | MSE Test Loss: 337.5502624511719\n",
      "Epoch: 10300 | MSE Train Loss: 331.2972412109375 | MSE Test Loss: 335.66387939453125\n",
      "Epoch: 10400 | MSE Train Loss: 329.4608459472656 | MSE Test Loss: 333.79071044921875\n",
      "Epoch: 10500 | MSE Train Loss: 327.6379089355469 | MSE Test Loss: 331.93121337890625\n",
      "Epoch: 10600 | MSE Train Loss: 325.82830810546875 | MSE Test Loss: 330.0849609375\n",
      "Epoch: 10700 | MSE Train Loss: 324.0317687988281 | MSE Test Loss: 328.251953125\n",
      "Epoch: 10800 | MSE Train Loss: 322.24835205078125 | MSE Test Loss: 326.4320068359375\n",
      "Epoch: 10900 | MSE Train Loss: 320.47808837890625 | MSE Test Loss: 324.6251220703125\n",
      "Epoch: 11000 | MSE Train Loss: 318.72052001953125 | MSE Test Loss: 322.8310241699219\n",
      "Epoch: 11100 | MSE Train Loss: 316.9754943847656 | MSE Test Loss: 321.0495910644531\n",
      "Epoch: 11200 | MSE Train Loss: 315.24322509765625 | MSE Test Loss: 319.2810363769531\n",
      "Epoch: 11300 | MSE Train Loss: 313.5235900878906 | MSE Test Loss: 317.52508544921875\n",
      "Epoch: 11400 | MSE Train Loss: 311.8162536621094 | MSE Test Loss: 315.7814636230469\n",
      "Epoch: 11500 | MSE Train Loss: 310.12115478515625 | MSE Test Loss: 314.05010986328125\n",
      "Epoch: 11600 | MSE Train Loss: 308.43817138671875 | MSE Test Loss: 312.3310852050781\n",
      "Epoch: 11700 | MSE Train Loss: 306.7674560546875 | MSE Test Loss: 310.62445068359375\n",
      "Epoch: 11800 | MSE Train Loss: 305.10870361328125 | MSE Test Loss: 308.9298095703125\n",
      "Epoch: 11900 | MSE Train Loss: 303.4619140625 | MSE Test Loss: 307.2470703125\n",
      "Epoch: 12000 | MSE Train Loss: 301.8267517089844 | MSE Test Loss: 305.5762634277344\n",
      "Epoch: 12100 | MSE Train Loss: 300.2033996582031 | MSE Test Loss: 303.9173278808594\n",
      "Epoch: 12200 | MSE Train Loss: 298.5916442871094 | MSE Test Loss: 302.2700500488281\n",
      "Epoch: 12300 | MSE Train Loss: 296.9913635253906 | MSE Test Loss: 300.6343994140625\n",
      "Epoch: 12400 | MSE Train Loss: 295.402587890625 | MSE Test Loss: 299.0104064941406\n",
      "Epoch: 12500 | MSE Train Loss: 293.82525634765625 | MSE Test Loss: 297.3979187011719\n",
      "Epoch: 12600 | MSE Train Loss: 292.2591857910156 | MSE Test Loss: 295.7968444824219\n",
      "Epoch: 12700 | MSE Train Loss: 290.7042236328125 | MSE Test Loss: 294.20697021484375\n",
      "Epoch: 12800 | MSE Train Loss: 289.1603698730469 | MSE Test Loss: 292.62835693359375\n",
      "Epoch: 12900 | MSE Train Loss: 287.6275329589844 | MSE Test Loss: 291.0608825683594\n",
      "Epoch: 13000 | MSE Train Loss: 286.10565185546875 | MSE Test Loss: 289.50457763671875\n",
      "Epoch: 13100 | MSE Train Loss: 284.59454345703125 | MSE Test Loss: 287.9591064453125\n",
      "Epoch: 13200 | MSE Train Loss: 283.09429931640625 | MSE Test Loss: 286.42462158203125\n",
      "Epoch: 13300 | MSE Train Loss: 281.6047058105469 | MSE Test Loss: 284.9009704589844\n",
      "Epoch: 13400 | MSE Train Loss: 280.12579345703125 | MSE Test Loss: 283.3882141113281\n",
      "Epoch: 13500 | MSE Train Loss: 278.6573486328125 | MSE Test Loss: 281.8858642578125\n",
      "Epoch: 13600 | MSE Train Loss: 277.1993713378906 | MSE Test Loss: 280.3942565917969\n",
      "Epoch: 13700 | MSE Train Loss: 275.7518615722656 | MSE Test Loss: 278.913330078125\n",
      "Epoch: 13800 | MSE Train Loss: 274.3145446777344 | MSE Test Loss: 277.4426574707031\n",
      "Epoch: 13900 | MSE Train Loss: 272.88751220703125 | MSE Test Loss: 275.9823913574219\n",
      "Epoch: 14000 | MSE Train Loss: 271.4706726074219 | MSE Test Loss: 274.53253173828125\n",
      "Epoch: 14100 | MSE Train Loss: 270.06396484375 | MSE Test Loss: 273.0929870605469\n",
      "Epoch: 14200 | MSE Train Loss: 268.6671447753906 | MSE Test Loss: 271.6634216308594\n",
      "Epoch: 14300 | MSE Train Loss: 267.2803649902344 | MSE Test Loss: 270.2439880371094\n",
      "Epoch: 14400 | MSE Train Loss: 265.9033508300781 | MSE Test Loss: 268.8345642089844\n",
      "Epoch: 14500 | MSE Train Loss: 264.5362548828125 | MSE Test Loss: 267.4351806640625\n",
      "Epoch: 14600 | MSE Train Loss: 263.1788024902344 | MSE Test Loss: 266.04571533203125\n",
      "Epoch: 14700 | MSE Train Loss: 261.8310852050781 | MSE Test Loss: 264.666015625\n",
      "Epoch: 14800 | MSE Train Loss: 260.4929504394531 | MSE Test Loss: 263.2961120605469\n",
      "Epoch: 14900 | MSE Train Loss: 259.16424560546875 | MSE Test Loss: 261.9356994628906\n",
      "Epoch: 15000 | MSE Train Loss: 257.8449401855469 | MSE Test Loss: 260.5849304199219\n",
      "Epoch: 15100 | MSE Train Loss: 256.53509521484375 | MSE Test Loss: 259.24365234375\n",
      "Epoch: 15200 | MSE Train Loss: 255.23460388183594 | MSE Test Loss: 257.91192626953125\n",
      "Epoch: 15300 | MSE Train Loss: 253.9434051513672 | MSE Test Loss: 256.58966064453125\n",
      "Epoch: 15400 | MSE Train Loss: 252.6613311767578 | MSE Test Loss: 255.27671813964844\n",
      "Epoch: 15500 | MSE Train Loss: 251.3883056640625 | MSE Test Loss: 253.9730224609375\n",
      "Epoch: 15600 | MSE Train Loss: 250.12440490722656 | MSE Test Loss: 252.6785430908203\n",
      "Epoch: 15700 | MSE Train Loss: 248.8695068359375 | MSE Test Loss: 251.39329528808594\n",
      "Epoch: 15800 | MSE Train Loss: 247.62350463867188 | MSE Test Loss: 250.11715698242188\n",
      "Epoch: 15900 | MSE Train Loss: 246.3865509033203 | MSE Test Loss: 248.85012817382812\n",
      "Epoch: 16000 | MSE Train Loss: 245.15835571289062 | MSE Test Loss: 247.59197998046875\n",
      "Epoch: 16100 | MSE Train Loss: 243.93878173828125 | MSE Test Loss: 246.3426971435547\n",
      "Epoch: 16200 | MSE Train Loss: 242.72787475585938 | MSE Test Loss: 245.1022491455078\n",
      "Epoch: 16300 | MSE Train Loss: 241.52557373046875 | MSE Test Loss: 243.87049865722656\n",
      "Epoch: 16400 | MSE Train Loss: 240.33180236816406 | MSE Test Loss: 242.64739990234375\n",
      "Epoch: 16500 | MSE Train Loss: 239.1466064453125 | MSE Test Loss: 241.43304443359375\n",
      "Epoch: 16600 | MSE Train Loss: 237.9698486328125 | MSE Test Loss: 240.22727966308594\n",
      "Epoch: 16700 | MSE Train Loss: 236.8013458251953 | MSE Test Loss: 239.02992248535156\n",
      "Epoch: 16800 | MSE Train Loss: 235.64105224609375 | MSE Test Loss: 237.84092712402344\n",
      "Epoch: 16900 | MSE Train Loss: 234.48919677734375 | MSE Test Loss: 236.6604766845703\n",
      "Epoch: 17000 | MSE Train Loss: 233.34559631347656 | MSE Test Loss: 235.4886016845703\n",
      "Epoch: 17100 | MSE Train Loss: 232.21005249023438 | MSE Test Loss: 234.3248748779297\n",
      "Epoch: 17200 | MSE Train Loss: 231.0824432373047 | MSE Test Loss: 233.16920471191406\n",
      "Epoch: 17300 | MSE Train Loss: 229.96299743652344 | MSE Test Loss: 232.02169799804688\n",
      "Epoch: 17400 | MSE Train Loss: 228.85154724121094 | MSE Test Loss: 230.88250732421875\n",
      "Epoch: 17500 | MSE Train Loss: 227.74789428710938 | MSE Test Loss: 229.751220703125\n",
      "Epoch: 17600 | MSE Train Loss: 226.6519775390625 | MSE Test Loss: 228.62786865234375\n",
      "Epoch: 17700 | MSE Train Loss: 225.56411743164062 | MSE Test Loss: 227.5126495361328\n",
      "Epoch: 17800 | MSE Train Loss: 224.48390197753906 | MSE Test Loss: 226.40524291992188\n",
      "Epoch: 17900 | MSE Train Loss: 223.4113006591797 | MSE Test Loss: 225.30564880371094\n",
      "Epoch: 18000 | MSE Train Loss: 222.34634399414062 | MSE Test Loss: 224.2137451171875\n",
      "Epoch: 18100 | MSE Train Loss: 221.28907775878906 | MSE Test Loss: 223.12966918945312\n",
      "Epoch: 18200 | MSE Train Loss: 220.23915100097656 | MSE Test Loss: 222.05322265625\n",
      "Epoch: 18300 | MSE Train Loss: 219.19674682617188 | MSE Test Loss: 220.98440551757812\n",
      "Epoch: 18400 | MSE Train Loss: 218.16189575195312 | MSE Test Loss: 219.92327880859375\n",
      "Epoch: 18500 | MSE Train Loss: 217.13429260253906 | MSE Test Loss: 218.86944580078125\n",
      "Epoch: 18600 | MSE Train Loss: 216.1138458251953 | MSE Test Loss: 217.82290649414062\n",
      "Epoch: 18700 | MSE Train Loss: 215.1009521484375 | MSE Test Loss: 216.78414916992188\n",
      "Epoch: 18800 | MSE Train Loss: 214.0950927734375 | MSE Test Loss: 215.75259399414062\n",
      "Epoch: 18900 | MSE Train Loss: 213.09637451171875 | MSE Test Loss: 214.728271484375\n",
      "Epoch: 19000 | MSE Train Loss: 212.1049041748047 | MSE Test Loss: 213.71127319335938\n",
      "Epoch: 19100 | MSE Train Loss: 211.1201934814453 | MSE Test Loss: 212.7012481689453\n",
      "Epoch: 19200 | MSE Train Loss: 210.14273071289062 | MSE Test Loss: 211.6986541748047\n",
      "Epoch: 19300 | MSE Train Loss: 209.17221069335938 | MSE Test Loss: 210.70309448242188\n",
      "Epoch: 19400 | MSE Train Loss: 208.2084503173828 | MSE Test Loss: 209.71441650390625\n",
      "Epoch: 19500 | MSE Train Loss: 207.25169372558594 | MSE Test Loss: 208.73287963867188\n",
      "Epoch: 19600 | MSE Train Loss: 206.30160522460938 | MSE Test Loss: 207.7581024169922\n",
      "Epoch: 19700 | MSE Train Loss: 205.3583221435547 | MSE Test Loss: 206.79034423828125\n",
      "Epoch: 19800 | MSE Train Loss: 204.4218292236328 | MSE Test Loss: 205.8295440673828\n",
      "Epoch: 19900 | MSE Train Loss: 203.4917449951172 | MSE Test Loss: 204.87518310546875\n",
      "Epoch: 20000 | MSE Train Loss: 202.56857299804688 | MSE Test Loss: 203.9278564453125\n",
      "Epoch: 20100 | MSE Train Loss: 201.6517791748047 | MSE Test Loss: 202.9871063232422\n",
      "Epoch: 20200 | MSE Train Loss: 200.74159240722656 | MSE Test Loss: 202.05313110351562\n",
      "Epoch: 20300 | MSE Train Loss: 199.83790588378906 | MSE Test Loss: 201.12574768066406\n",
      "Epoch: 20400 | MSE Train Loss: 198.9404754638672 | MSE Test Loss: 200.20469665527344\n",
      "Epoch: 20500 | MSE Train Loss: 198.04957580566406 | MSE Test Loss: 199.2903289794922\n",
      "Epoch: 20600 | MSE Train Loss: 197.16485595703125 | MSE Test Loss: 198.38229370117188\n",
      "Epoch: 20700 | MSE Train Loss: 196.28672790527344 | MSE Test Loss: 197.48101806640625\n",
      "Epoch: 20800 | MSE Train Loss: 195.41455078125 | MSE Test Loss: 196.5857391357422\n",
      "Epoch: 20900 | MSE Train Loss: 194.5487518310547 | MSE Test Loss: 195.69700622558594\n",
      "Epoch: 21000 | MSE Train Loss: 193.6889190673828 | MSE Test Loss: 194.8143310546875\n",
      "Epoch: 21100 | MSE Train Loss: 192.83541870117188 | MSE Test Loss: 193.9381561279297\n",
      "Epoch: 21200 | MSE Train Loss: 191.9878692626953 | MSE Test Loss: 193.06809997558594\n",
      "Epoch: 21300 | MSE Train Loss: 191.1464385986328 | MSE Test Loss: 192.20420837402344\n",
      "Epoch: 21400 | MSE Train Loss: 190.31080627441406 | MSE Test Loss: 191.34617614746094\n",
      "Epoch: 21500 | MSE Train Loss: 189.4812469482422 | MSE Test Loss: 190.49441528320312\n",
      "Epoch: 21600 | MSE Train Loss: 188.6575469970703 | MSE Test Loss: 189.64874267578125\n",
      "Epoch: 21700 | MSE Train Loss: 187.83982849121094 | MSE Test Loss: 188.80905151367188\n",
      "Epoch: 21800 | MSE Train Loss: 187.02769470214844 | MSE Test Loss: 187.97511291503906\n",
      "Epoch: 21900 | MSE Train Loss: 186.2215576171875 | MSE Test Loss: 187.14715576171875\n",
      "Epoch: 22000 | MSE Train Loss: 185.42091369628906 | MSE Test Loss: 186.32493591308594\n",
      "Epoch: 22100 | MSE Train Loss: 184.62625122070312 | MSE Test Loss: 185.50885009765625\n",
      "Epoch: 22200 | MSE Train Loss: 183.83697509765625 | MSE Test Loss: 184.69821166992188\n",
      "Epoch: 22300 | MSE Train Loss: 183.0535430908203 | MSE Test Loss: 183.89352416992188\n",
      "Epoch: 22400 | MSE Train Loss: 182.27540588378906 | MSE Test Loss: 183.0941925048828\n",
      "Epoch: 22500 | MSE Train Loss: 181.5030517578125 | MSE Test Loss: 182.30084228515625\n",
      "Epoch: 22600 | MSE Train Loss: 180.73609924316406 | MSE Test Loss: 181.51303100585938\n",
      "Epoch: 22700 | MSE Train Loss: 179.97459411621094 | MSE Test Loss: 180.73077392578125\n",
      "Epoch: 22800 | MSE Train Loss: 179.2185516357422 | MSE Test Loss: 179.95399475097656\n",
      "Epoch: 22900 | MSE Train Loss: 178.46766662597656 | MSE Test Loss: 179.18251037597656\n",
      "Epoch: 23000 | MSE Train Loss: 177.72242736816406 | MSE Test Loss: 178.4168701171875\n",
      "Epoch: 23100 | MSE Train Loss: 176.98226928710938 | MSE Test Loss: 177.6564178466797\n",
      "Epoch: 23200 | MSE Train Loss: 176.24754333496094 | MSE Test Loss: 176.9014892578125\n",
      "Epoch: 23300 | MSE Train Loss: 175.5179443359375 | MSE Test Loss: 176.15171813964844\n",
      "Epoch: 23400 | MSE Train Loss: 174.79339599609375 | MSE Test Loss: 175.40713500976562\n",
      "Epoch: 23500 | MSE Train Loss: 174.07427978515625 | MSE Test Loss: 174.6681671142578\n",
      "Epoch: 23600 | MSE Train Loss: 173.36004638671875 | MSE Test Loss: 173.93418884277344\n",
      "Epoch: 23700 | MSE Train Loss: 172.6510467529297 | MSE Test Loss: 173.20550537109375\n",
      "Epoch: 23800 | MSE Train Loss: 171.94705200195312 | MSE Test Loss: 172.48187255859375\n",
      "Epoch: 23900 | MSE Train Loss: 171.24786376953125 | MSE Test Loss: 171.76319885253906\n",
      "Epoch: 24000 | MSE Train Loss: 170.553955078125 | MSE Test Loss: 171.04994201660156\n",
      "Epoch: 24100 | MSE Train Loss: 169.8648681640625 | MSE Test Loss: 170.3416290283203\n",
      "Epoch: 24200 | MSE Train Loss: 169.18057250976562 | MSE Test Loss: 169.63821411132812\n",
      "Epoch: 24300 | MSE Train Loss: 168.50132751464844 | MSE Test Loss: 168.93984985351562\n",
      "Epoch: 24400 | MSE Train Loss: 167.8267059326172 | MSE Test Loss: 168.24627685546875\n",
      "Epoch: 24500 | MSE Train Loss: 167.15699768066406 | MSE Test Loss: 167.5576934814453\n",
      "Epoch: 24600 | MSE Train Loss: 166.49209594726562 | MSE Test Loss: 166.87411499023438\n",
      "Epoch: 24700 | MSE Train Loss: 165.83180236816406 | MSE Test Loss: 166.19520568847656\n",
      "Epoch: 24800 | MSE Train Loss: 165.17626953125 | MSE Test Loss: 165.52108764648438\n",
      "Epoch: 24900 | MSE Train Loss: 164.52542114257812 | MSE Test Loss: 164.8518524169922\n",
      "Epoch: 25000 | MSE Train Loss: 163.87913513183594 | MSE Test Loss: 164.18719482421875\n",
      "Epoch: 25100 | MSE Train Loss: 163.23745727539062 | MSE Test Loss: 163.52723693847656\n",
      "Epoch: 25200 | MSE Train Loss: 162.60047912597656 | MSE Test Loss: 162.8721466064453\n",
      "Epoch: 25300 | MSE Train Loss: 161.96788024902344 | MSE Test Loss: 162.2214813232422\n",
      "Epoch: 25400 | MSE Train Loss: 161.3397216796875 | MSE Test Loss: 161.5753936767578\n",
      "Epoch: 25500 | MSE Train Loss: 160.7162322998047 | MSE Test Loss: 160.9340362548828\n",
      "Epoch: 25600 | MSE Train Loss: 160.09706115722656 | MSE Test Loss: 160.29708862304688\n",
      "Epoch: 25700 | MSE Train Loss: 159.48219299316406 | MSE Test Loss: 159.66453552246094\n",
      "Epoch: 25800 | MSE Train Loss: 158.8718719482422 | MSE Test Loss: 159.0366668701172\n",
      "Epoch: 25900 | MSE Train Loss: 158.265869140625 | MSE Test Loss: 158.41322326660156\n",
      "Epoch: 26000 | MSE Train Loss: 157.66409301757812 | MSE Test Loss: 157.79408264160156\n",
      "Epoch: 26100 | MSE Train Loss: 157.0665283203125 | MSE Test Loss: 157.1792449951172\n",
      "Epoch: 26200 | MSE Train Loss: 156.473388671875 | MSE Test Loss: 156.56887817382812\n",
      "Epoch: 26300 | MSE Train Loss: 155.8843994140625 | MSE Test Loss: 155.9627685546875\n",
      "Epoch: 26400 | MSE Train Loss: 155.29954528808594 | MSE Test Loss: 155.36090087890625\n",
      "Epoch: 26500 | MSE Train Loss: 154.7188720703125 | MSE Test Loss: 154.7633819580078\n",
      "Epoch: 26600 | MSE Train Loss: 154.14244079589844 | MSE Test Loss: 154.17010498046875\n",
      "Epoch: 26700 | MSE Train Loss: 153.57000732421875 | MSE Test Loss: 153.58091735839844\n",
      "Epoch: 26800 | MSE Train Loss: 153.0015411376953 | MSE Test Loss: 152.99574279785156\n",
      "Epoch: 26900 | MSE Train Loss: 152.43716430664062 | MSE Test Loss: 152.41485595703125\n",
      "Epoch: 27000 | MSE Train Loss: 151.8769989013672 | MSE Test Loss: 151.83827209472656\n",
      "Epoch: 27100 | MSE Train Loss: 151.3201141357422 | MSE Test Loss: 151.26498413085938\n",
      "Epoch: 27200 | MSE Train Loss: 150.76834106445312 | MSE Test Loss: 150.69692993164062\n",
      "Epoch: 27300 | MSE Train Loss: 150.2197265625 | MSE Test Loss: 150.1320343017578\n",
      "Epoch: 27400 | MSE Train Loss: 149.67507934570312 | MSE Test Loss: 149.5712127685547\n",
      "Epoch: 27500 | MSE Train Loss: 149.13470458984375 | MSE Test Loss: 149.014892578125\n",
      "Epoch: 27600 | MSE Train Loss: 148.5973663330078 | MSE Test Loss: 148.46163940429688\n",
      "Epoch: 27700 | MSE Train Loss: 148.06488037109375 | MSE Test Loss: 147.91329956054688\n",
      "Epoch: 27800 | MSE Train Loss: 147.5355224609375 | MSE Test Loss: 147.3681182861328\n",
      "Epoch: 27900 | MSE Train Loss: 147.00994873046875 | MSE Test Loss: 146.82679748535156\n",
      "Epoch: 28000 | MSE Train Loss: 146.48849487304688 | MSE Test Loss: 146.28973388671875\n",
      "Epoch: 28100 | MSE Train Loss: 145.9698944091797 | MSE Test Loss: 145.75570678710938\n",
      "Epoch: 28200 | MSE Train Loss: 145.4562225341797 | MSE Test Loss: 145.22654724121094\n",
      "Epoch: 28300 | MSE Train Loss: 144.94534301757812 | MSE Test Loss: 144.7003173828125\n",
      "Epoch: 28400 | MSE Train Loss: 144.4383544921875 | MSE Test Loss: 144.17799377441406\n",
      "Epoch: 28500 | MSE Train Loss: 143.93502807617188 | MSE Test Loss: 143.65940856933594\n",
      "Epoch: 28600 | MSE Train Loss: 143.43472290039062 | MSE Test Loss: 143.1439971923828\n",
      "Epoch: 28700 | MSE Train Loss: 142.93894958496094 | MSE Test Loss: 142.6332244873047\n",
      "Epoch: 28800 | MSE Train Loss: 142.44578552246094 | MSE Test Loss: 142.12515258789062\n",
      "Epoch: 28900 | MSE Train Loss: 141.95689392089844 | MSE Test Loss: 141.62132263183594\n",
      "Epoch: 29000 | MSE Train Loss: 141.47096252441406 | MSE Test Loss: 141.12059020996094\n",
      "Epoch: 29100 | MSE Train Loss: 140.98851013183594 | MSE Test Loss: 140.62339782714844\n",
      "Epoch: 29200 | MSE Train Loss: 140.50985717773438 | MSE Test Loss: 140.1300811767578\n",
      "Epoch: 29300 | MSE Train Loss: 140.03384399414062 | MSE Test Loss: 139.6395263671875\n",
      "Epoch: 29400 | MSE Train Loss: 139.56231689453125 | MSE Test Loss: 139.15350341796875\n",
      "Epoch: 29500 | MSE Train Loss: 139.0930938720703 | MSE Test Loss: 138.6698455810547\n",
      "Epoch: 29600 | MSE Train Loss: 138.62806701660156 | MSE Test Loss: 138.1904754638672\n",
      "Epoch: 29700 | MSE Train Loss: 138.165771484375 | MSE Test Loss: 137.71388244628906\n",
      "Epoch: 29800 | MSE Train Loss: 137.70703125 | MSE Test Loss: 137.24093627929688\n",
      "Epoch: 29900 | MSE Train Loss: 137.25149536132812 | MSE Test Loss: 136.7713165283203\n",
      "Epoch: 30000 | MSE Train Loss: 136.79891967773438 | MSE Test Loss: 136.30470275878906\n",
      "Epoch: 30100 | MSE Train Loss: 136.35012817382812 | MSE Test Loss: 135.8419647216797\n",
      "Epoch: 30200 | MSE Train Loss: 135.90367126464844 | MSE Test Loss: 135.3815460205078\n",
      "Epoch: 30300 | MSE Train Loss: 135.46145629882812 | MSE Test Loss: 134.92555236816406\n",
      "Epoch: 30400 | MSE Train Loss: 135.0213165283203 | MSE Test Loss: 134.47169494628906\n",
      "Epoch: 30500 | MSE Train Loss: 134.58534240722656 | MSE Test Loss: 134.02200317382812\n",
      "Epoch: 30600 | MSE Train Loss: 134.15162658691406 | MSE Test Loss: 133.5746307373047\n",
      "Epoch: 30700 | MSE Train Loss: 133.7216033935547 | MSE Test Loss: 133.13108825683594\n",
      "Epoch: 30800 | MSE Train Loss: 133.29420471191406 | MSE Test Loss: 132.6902313232422\n",
      "Epoch: 30900 | MSE Train Loss: 132.87013244628906 | MSE Test Loss: 132.2527313232422\n",
      "Epoch: 31000 | MSE Train Loss: 132.448974609375 | MSE Test Loss: 131.8183135986328\n",
      "Epoch: 31100 | MSE Train Loss: 132.03067016601562 | MSE Test Loss: 131.38671875\n",
      "Epoch: 31200 | MSE Train Loss: 131.61563110351562 | MSE Test Loss: 130.9584503173828\n",
      "Epoch: 31300 | MSE Train Loss: 131.2030792236328 | MSE Test Loss: 130.53273010253906\n",
      "Epoch: 31400 | MSE Train Loss: 130.7940216064453 | MSE Test Loss: 130.11065673828125\n",
      "Epoch: 31500 | MSE Train Loss: 130.38720703125 | MSE Test Loss: 129.69090270996094\n",
      "Epoch: 31600 | MSE Train Loss: 129.98414611816406 | MSE Test Loss: 129.2749481201172\n",
      "Epoch: 31700 | MSE Train Loss: 129.58294677734375 | MSE Test Loss: 128.86087036132812\n",
      "Epoch: 31800 | MSE Train Loss: 129.18565368652344 | MSE Test Loss: 128.4508056640625\n",
      "Epoch: 31900 | MSE Train Loss: 128.79002380371094 | MSE Test Loss: 128.0424041748047\n",
      "Epoch: 32000 | MSE Train Loss: 128.39846801757812 | MSE Test Loss: 127.6381607055664\n",
      "Epoch: 32100 | MSE Train Loss: 128.0084991455078 | MSE Test Loss: 127.23568725585938\n",
      "Epoch: 32200 | MSE Train Loss: 127.62249755859375 | MSE Test Loss: 126.83719635009766\n",
      "Epoch: 32300 | MSE Train Loss: 127.23812866210938 | MSE Test Loss: 126.44039916992188\n",
      "Epoch: 32400 | MSE Train Loss: 126.85752868652344 | MSE Test Loss: 126.04737854003906\n",
      "Epoch: 32500 | MSE Train Loss: 126.47857666015625 | MSE Test Loss: 125.65609741210938\n",
      "Epoch: 32600 | MSE Train Loss: 126.10332489013672 | MSE Test Loss: 125.26850128173828\n",
      "Epoch: 32700 | MSE Train Loss: 125.72969818115234 | MSE Test Loss: 124.88267517089844\n",
      "Epoch: 32800 | MSE Train Loss: 125.35982513427734 | MSE Test Loss: 124.5007095336914\n",
      "Epoch: 32900 | MSE Train Loss: 124.9915771484375 | MSE Test Loss: 124.12043762207031\n",
      "Epoch: 33000 | MSE Train Loss: 124.62692260742188 | MSE Test Loss: 123.7437515258789\n",
      "Epoch: 33100 | MSE Train Loss: 124.26384735107422 | MSE Test Loss: 123.36872863769531\n",
      "Epoch: 33200 | MSE Train Loss: 123.90436553955078 | MSE Test Loss: 122.997314453125\n",
      "Epoch: 33300 | MSE Train Loss: 123.54634857177734 | MSE Test Loss: 122.62744903564453\n",
      "Epoch: 33400 | MSE Train Loss: 123.19200134277344 | MSE Test Loss: 122.26132202148438\n",
      "Epoch: 33500 | MSE Train Loss: 122.83902740478516 | MSE Test Loss: 121.89669799804688\n",
      "Epoch: 33600 | MSE Train Loss: 122.48981475830078 | MSE Test Loss: 121.53585052490234\n",
      "Epoch: 33700 | MSE Train Loss: 122.1418228149414 | MSE Test Loss: 121.17628479003906\n",
      "Epoch: 33800 | MSE Train Loss: 121.79757690429688 | MSE Test Loss: 120.82046508789062\n",
      "Epoch: 33900 | MSE Train Loss: 121.45457458496094 | MSE Test Loss: 120.46597290039062\n",
      "Epoch: 34000 | MSE Train Loss: 121.11502838134766 | MSE Test Loss: 120.11499786376953\n",
      "Epoch: 34100 | MSE Train Loss: 120.77702331542969 | MSE Test Loss: 119.76564025878906\n",
      "Epoch: 34200 | MSE Train Loss: 120.44219970703125 | MSE Test Loss: 119.41954803466797\n",
      "Epoch: 34300 | MSE Train Loss: 120.10911560058594 | MSE Test Loss: 119.07527160644531\n",
      "Epoch: 34400 | MSE Train Loss: 119.77892303466797 | MSE Test Loss: 118.73394012451172\n",
      "Epoch: 34500 | MSE Train Loss: 119.4506607055664 | MSE Test Loss: 118.39452362060547\n",
      "Epoch: 34600 | MSE Train Loss: 119.125 | MSE Test Loss: 118.05777740478516\n",
      "Epoch: 34700 | MSE Train Loss: 118.801513671875 | MSE Test Loss: 117.72325134277344\n",
      "Epoch: 34800 | MSE Train Loss: 118.48027801513672 | MSE Test Loss: 117.39109802246094\n",
      "Epoch: 34900 | MSE Train Loss: 118.1615982055664 | MSE Test Loss: 117.06148529052734\n",
      "Epoch: 35000 | MSE Train Loss: 117.84468841552734 | MSE Test Loss: 116.73373413085938\n",
      "Epoch: 35100 | MSE Train Loss: 117.53067779541016 | MSE Test Loss: 116.40899658203125\n",
      "Epoch: 35200 | MSE Train Loss: 117.21804809570312 | MSE Test Loss: 116.08564758300781\n",
      "Epoch: 35300 | MSE Train Loss: 116.90868377685547 | MSE Test Loss: 115.7656021118164\n",
      "Epoch: 35400 | MSE Train Loss: 116.6004409790039 | MSE Test Loss: 115.44666290283203\n",
      "Epoch: 35500 | MSE Train Loss: 116.29547119140625 | MSE Test Loss: 115.13117218017578\n",
      "Epoch: 35600 | MSE Train Loss: 115.9918212890625 | MSE Test Loss: 114.81698608398438\n",
      "Epoch: 35700 | MSE Train Loss: 115.69090270996094 | MSE Test Loss: 114.505615234375\n",
      "Epoch: 35800 | MSE Train Loss: 115.39176177978516 | MSE Test Loss: 114.19605255126953\n",
      "Epoch: 35900 | MSE Train Loss: 115.0948257446289 | MSE Test Loss: 113.88878631591797\n",
      "Epoch: 36000 | MSE Train Loss: 114.80016326904297 | MSE Test Loss: 113.58380889892578\n",
      "Epoch: 36100 | MSE Train Loss: 114.50712585449219 | MSE Test Loss: 113.28052520751953\n",
      "Epoch: 36200 | MSE Train Loss: 114.21691131591797 | MSE Test Loss: 112.98014068603516\n",
      "Epoch: 36300 | MSE Train Loss: 113.92774963378906 | MSE Test Loss: 112.68082427978516\n",
      "Epoch: 36400 | MSE Train Loss: 113.64187622070312 | MSE Test Loss: 112.38484954833984\n",
      "Epoch: 36500 | MSE Train Loss: 113.3570785522461 | MSE Test Loss: 112.08995056152344\n",
      "Epoch: 36600 | MSE Train Loss: 113.0748519897461 | MSE Test Loss: 111.79772186279297\n",
      "Epoch: 36700 | MSE Train Loss: 112.79437255859375 | MSE Test Loss: 111.50733947753906\n",
      "Epoch: 36800 | MSE Train Loss: 112.51580047607422 | MSE Test Loss: 111.21890258789062\n",
      "Epoch: 36900 | MSE Train Loss: 112.2396011352539 | MSE Test Loss: 110.9328384399414\n",
      "Epoch: 37000 | MSE Train Loss: 111.964599609375 | MSE Test Loss: 110.64810180664062\n",
      "Epoch: 37100 | MSE Train Loss: 111.6926498413086 | MSE Test Loss: 110.36634826660156\n",
      "Epoch: 37200 | MSE Train Loss: 111.42162322998047 | MSE Test Loss: 110.08562469482422\n",
      "Epoch: 37300 | MSE Train Loss: 111.15325164794922 | MSE Test Loss: 109.80760192871094\n",
      "Epoch: 37400 | MSE Train Loss: 110.88639068603516 | MSE Test Loss: 109.53111267089844\n",
      "Epoch: 37500 | MSE Train Loss: 110.62135314941406 | MSE Test Loss: 109.25651550292969\n",
      "Epoch: 37600 | MSE Train Loss: 110.35862731933594 | MSE Test Loss: 108.9842758178711\n",
      "Epoch: 37700 | MSE Train Loss: 110.096923828125 | MSE Test Loss: 108.71318817138672\n",
      "Epoch: 37800 | MSE Train Loss: 109.83830261230469 | MSE Test Loss: 108.44515991210938\n",
      "Epoch: 37900 | MSE Train Loss: 109.5805892944336 | MSE Test Loss: 108.1781005859375\n",
      "Epoch: 38000 | MSE Train Loss: 109.32514953613281 | MSE Test Loss: 107.91333770751953\n",
      "Epoch: 38100 | MSE Train Loss: 109.07144927978516 | MSE Test Loss: 107.65033721923828\n",
      "Epoch: 38200 | MSE Train Loss: 108.81908416748047 | MSE Test Loss: 107.38871002197266\n",
      "Epoch: 38300 | MSE Train Loss: 108.56932830810547 | MSE Test Loss: 107.12975311279297\n",
      "Epoch: 38400 | MSE Train Loss: 108.32050323486328 | MSE Test Loss: 106.8718032836914\n",
      "Epoch: 38500 | MSE Train Loss: 108.07422637939453 | MSE Test Loss: 106.61648559570312\n",
      "Epoch: 38600 | MSE Train Loss: 107.82931518554688 | MSE Test Loss: 106.36260223388672\n",
      "Epoch: 38700 | MSE Train Loss: 107.58595275878906 | MSE Test Loss: 106.1102523803711\n",
      "Epoch: 38800 | MSE Train Loss: 107.34490203857422 | MSE Test Loss: 105.8602523803711\n",
      "Epoch: 38900 | MSE Train Loss: 107.10472869873047 | MSE Test Loss: 105.61113739013672\n",
      "Epoch: 39000 | MSE Train Loss: 106.86713409423828 | MSE Test Loss: 105.36466217041016\n",
      "Epoch: 39100 | MSE Train Loss: 106.6307601928711 | MSE Test Loss: 105.11941528320312\n",
      "Epoch: 39200 | MSE Train Loss: 106.39585876464844 | MSE Test Loss: 104.8757095336914\n",
      "Epoch: 39300 | MSE Train Loss: 106.16329956054688 | MSE Test Loss: 104.63450622558594\n",
      "Epoch: 39400 | MSE Train Loss: 105.93157196044922 | MSE Test Loss: 104.39412689208984\n",
      "Epoch: 39500 | MSE Train Loss: 105.70230102539062 | MSE Test Loss: 104.15625\n",
      "Epoch: 39600 | MSE Train Loss: 105.47427368164062 | MSE Test Loss: 103.91960906982422\n",
      "Epoch: 39700 | MSE Train Loss: 105.24759674072266 | MSE Test Loss: 103.68437194824219\n",
      "Epoch: 39800 | MSE Train Loss: 105.02322387695312 | MSE Test Loss: 103.4514389038086\n",
      "Epoch: 39900 | MSE Train Loss: 104.79964447021484 | MSE Test Loss: 103.21935272216797\n",
      "Epoch: 40000 | MSE Train Loss: 104.5782470703125 | MSE Test Loss: 102.98948669433594\n",
      "Epoch: 40100 | MSE Train Loss: 104.35830688476562 | MSE Test Loss: 102.76118469238281\n",
      "Epoch: 40200 | MSE Train Loss: 104.13933563232422 | MSE Test Loss: 102.53389739990234\n",
      "Epoch: 40300 | MSE Train Loss: 103.92301177978516 | MSE Test Loss: 102.30928039550781\n",
      "Epoch: 40400 | MSE Train Loss: 103.70743560791016 | MSE Test Loss: 102.0854721069336\n",
      "Epoch: 40500 | MSE Train Loss: 103.49359130859375 | MSE Test Loss: 101.8634033203125\n",
      "Epoch: 40600 | MSE Train Loss: 103.28152465820312 | MSE Test Loss: 101.64314270019531\n",
      "Epoch: 40700 | MSE Train Loss: 103.07023620605469 | MSE Test Loss: 101.42367553710938\n",
      "Epoch: 40800 | MSE Train Loss: 102.86131286621094 | MSE Test Loss: 101.20662689208984\n",
      "Epoch: 40900 | MSE Train Loss: 102.65349578857422 | MSE Test Loss: 100.99071502685547\n",
      "Epoch: 41000 | MSE Train Loss: 102.44673919677734 | MSE Test Loss: 100.77594757080078\n",
      "Epoch: 41100 | MSE Train Loss: 102.24237060546875 | MSE Test Loss: 100.5635757446289\n",
      "Epoch: 41200 | MSE Train Loss: 102.03872680664062 | MSE Test Loss: 100.35203552246094\n",
      "Epoch: 41300 | MSE Train Loss: 101.83672332763672 | MSE Test Loss: 100.14218139648438\n",
      "Epoch: 41400 | MSE Train Loss: 101.636474609375 | MSE Test Loss: 99.93405151367188\n",
      "Epoch: 41500 | MSE Train Loss: 101.43696594238281 | MSE Test Loss: 99.7266845703125\n",
      "Epoch: 41600 | MSE Train Loss: 101.23954772949219 | MSE Test Loss: 99.5214614868164\n",
      "Epoch: 41700 | MSE Train Loss: 101.04335021972656 | MSE Test Loss: 99.3174819946289\n",
      "Epoch: 41800 | MSE Train Loss: 100.84786224365234 | MSE Test Loss: 99.11427307128906\n",
      "Epoch: 41900 | MSE Train Loss: 100.65496063232422 | MSE Test Loss: 98.91368865966797\n",
      "Epoch: 42000 | MSE Train Loss: 100.46275329589844 | MSE Test Loss: 98.7138442993164\n",
      "Epoch: 42100 | MSE Train Loss: 100.27161407470703 | MSE Test Loss: 98.51510620117188\n",
      "Epoch: 42200 | MSE Train Loss: 100.08267974853516 | MSE Test Loss: 98.31863403320312\n",
      "Epoch: 42300 | MSE Train Loss: 99.8944091796875 | MSE Test Loss: 98.12286376953125\n",
      "Epoch: 42400 | MSE Train Loss: 99.70759582519531 | MSE Test Loss: 97.92853546142578\n",
      "Epoch: 42500 | MSE Train Loss: 99.52251434326172 | MSE Test Loss: 97.73599243164062\n",
      "Epoch: 42600 | MSE Train Loss: 99.3381118774414 | MSE Test Loss: 97.54412841796875\n",
      "Epoch: 42700 | MSE Train Loss: 99.15544128417969 | MSE Test Loss: 97.35409545898438\n",
      "Epoch: 42800 | MSE Train Loss: 98.97418975830078 | MSE Test Loss: 97.16551971435547\n",
      "Epoch: 42900 | MSE Train Loss: 98.79360961914062 | MSE Test Loss: 96.97759246826172\n",
      "Epoch: 43000 | MSE Train Loss: 98.61495971679688 | MSE Test Loss: 96.79167938232422\n",
      "Epoch: 43100 | MSE Train Loss: 98.4374771118164 | MSE Test Loss: 96.6069107055664\n",
      "Epoch: 43200 | MSE Train Loss: 98.26062774658203 | MSE Test Loss: 96.42284393310547\n",
      "Epoch: 43300 | MSE Train Loss: 98.08589172363281 | MSE Test Loss: 96.2409896850586\n",
      "Epoch: 43400 | MSE Train Loss: 97.91210174560547 | MSE Test Loss: 96.06007385253906\n",
      "Epoch: 43500 | MSE Train Loss: 97.73894500732422 | MSE Test Loss: 95.87979125976562\n",
      "Epoch: 43600 | MSE Train Loss: 97.56802368164062 | MSE Test Loss: 95.70179748535156\n",
      "Epoch: 43700 | MSE Train Loss: 97.39788818359375 | MSE Test Loss: 95.52465057373047\n",
      "Epoch: 43800 | MSE Train Loss: 97.22836303710938 | MSE Test Loss: 95.3481216430664\n",
      "Epoch: 43900 | MSE Train Loss: 97.0611343383789 | MSE Test Loss: 95.17393493652344\n",
      "Epoch: 44000 | MSE Train Loss: 96.89457702636719 | MSE Test Loss: 95.00041961669922\n",
      "Epoch: 44100 | MSE Train Loss: 96.7286148071289 | MSE Test Loss: 94.82752227783203\n",
      "Epoch: 44200 | MSE Train Loss: 96.5649185180664 | MSE Test Loss: 94.65693664550781\n",
      "Epoch: 44300 | MSE Train Loss: 96.4018783569336 | MSE Test Loss: 94.48709106445312\n",
      "Epoch: 44400 | MSE Train Loss: 96.23943328857422 | MSE Test Loss: 94.3178482055664\n",
      "Epoch: 44500 | MSE Train Loss: 96.07921600341797 | MSE Test Loss: 94.15087127685547\n",
      "Epoch: 44600 | MSE Train Loss: 95.91966247558594 | MSE Test Loss: 93.9845962524414\n",
      "Epoch: 44700 | MSE Train Loss: 95.76067352294922 | MSE Test Loss: 93.8189468383789\n",
      "Epoch: 44800 | MSE Train Loss: 95.60384368896484 | MSE Test Loss: 93.65544128417969\n",
      "Epoch: 44900 | MSE Train Loss: 95.44770050048828 | MSE Test Loss: 93.4926528930664\n",
      "Epoch: 45000 | MSE Train Loss: 95.2921142578125 | MSE Test Loss: 93.33043670654297\n",
      "Epoch: 45100 | MSE Train Loss: 95.13848876953125 | MSE Test Loss: 93.17022705078125\n",
      "Epoch: 45200 | MSE Train Loss: 94.98568725585938 | MSE Test Loss: 93.01084899902344\n",
      "Epoch: 45300 | MSE Train Loss: 94.83342742919922 | MSE Test Loss: 92.85205078125\n",
      "Epoch: 45400 | MSE Train Loss: 94.68291473388672 | MSE Test Loss: 92.69505310058594\n",
      "Epoch: 45500 | MSE Train Loss: 94.53343200683594 | MSE Test Loss: 92.53914642333984\n",
      "Epoch: 45600 | MSE Train Loss: 94.38449096679688 | MSE Test Loss: 92.38380432128906\n",
      "Epoch: 45700 | MSE Train Loss: 94.23705291748047 | MSE Test Loss: 92.2300033569336\n",
      "Epoch: 45800 | MSE Train Loss: 94.09083557128906 | MSE Test Loss: 92.07745361328125\n",
      "Epoch: 45900 | MSE Train Loss: 93.94512176513672 | MSE Test Loss: 91.9254150390625\n",
      "Epoch: 46000 | MSE Train Loss: 93.80059051513672 | MSE Test Loss: 91.77458953857422\n",
      "Epoch: 46100 | MSE Train Loss: 93.6575698852539 | MSE Test Loss: 91.62528991699219\n",
      "Epoch: 46200 | MSE Train Loss: 93.5150375366211 | MSE Test Loss: 91.47647857666016\n",
      "Epoch: 46300 | MSE Train Loss: 93.37327575683594 | MSE Test Loss: 91.3284912109375\n",
      "Epoch: 46400 | MSE Train Loss: 93.23338317871094 | MSE Test Loss: 91.18236541748047\n",
      "Epoch: 46500 | MSE Train Loss: 93.0939712524414 | MSE Test Loss: 91.03678131103516\n",
      "Epoch: 46600 | MSE Train Loss: 92.95508575439453 | MSE Test Loss: 90.89178466796875\n",
      "Epoch: 46700 | MSE Train Loss: 92.81815338134766 | MSE Test Loss: 90.74882507324219\n",
      "Epoch: 46800 | MSE Train Loss: 92.68187713623047 | MSE Test Loss: 90.60650634765625\n",
      "Epoch: 46900 | MSE Train Loss: 92.54607391357422 | MSE Test Loss: 90.4646987915039\n",
      "Epoch: 47000 | MSE Train Loss: 92.41173553466797 | MSE Test Loss: 90.32437896728516\n",
      "Epoch: 47100 | MSE Train Loss: 92.27848052978516 | MSE Test Loss: 90.18512725830078\n",
      "Epoch: 47200 | MSE Train Loss: 92.14569854736328 | MSE Test Loss: 90.04637145996094\n",
      "Epoch: 47300 | MSE Train Loss: 92.01380920410156 | MSE Test Loss: 89.90858459472656\n",
      "Epoch: 47400 | MSE Train Loss: 91.88355255126953 | MSE Test Loss: 89.7723617553711\n",
      "Epoch: 47500 | MSE Train Loss: 91.75372314453125 | MSE Test Loss: 89.63664245605469\n",
      "Epoch: 47600 | MSE Train Loss: 91.62435913085938 | MSE Test Loss: 89.50141143798828\n",
      "Epoch: 47700 | MSE Train Loss: 91.49684143066406 | MSE Test Loss: 89.36807250976562\n",
      "Epoch: 47800 | MSE Train Loss: 91.36996459960938 | MSE Test Loss: 89.2354736328125\n",
      "Epoch: 47900 | MSE Train Loss: 91.24356079101562 | MSE Test Loss: 89.10336303710938\n",
      "Epoch: 48000 | MSE Train Loss: 91.11833953857422 | MSE Test Loss: 88.9724349975586\n",
      "Epoch: 48100 | MSE Train Loss: 90.99434661865234 | MSE Test Loss: 88.84275817871094\n",
      "Epoch: 48200 | MSE Train Loss: 90.87079620361328 | MSE Test Loss: 88.7135238647461\n",
      "Epoch: 48300 | MSE Train Loss: 90.74772644042969 | MSE Test Loss: 88.5848159790039\n",
      "Epoch: 48400 | MSE Train Loss: 90.62655639648438 | MSE Test Loss: 88.45799255371094\n",
      "Epoch: 48500 | MSE Train Loss: 90.50582122802734 | MSE Test Loss: 88.33164978027344\n",
      "Epoch: 48600 | MSE Train Loss: 90.38551330566406 | MSE Test Loss: 88.20574951171875\n",
      "Epoch: 48700 | MSE Train Loss: 90.26638793945312 | MSE Test Loss: 88.08104705810547\n",
      "Epoch: 48800 | MSE Train Loss: 90.14842224121094 | MSE Test Loss: 87.9575424194336\n",
      "Epoch: 48900 | MSE Train Loss: 90.03089904785156 | MSE Test Loss: 87.83454895019531\n",
      "Epoch: 49000 | MSE Train Loss: 89.91378784179688 | MSE Test Loss: 87.71195983886719\n",
      "Epoch: 49100 | MSE Train Loss: 89.79851531982422 | MSE Test Loss: 87.59130096435547\n",
      "Epoch: 49200 | MSE Train Loss: 89.6837158203125 | MSE Test Loss: 87.47113800048828\n",
      "Epoch: 49300 | MSE Train Loss: 89.56932830810547 | MSE Test Loss: 87.35136413574219\n",
      "Epoch: 49400 | MSE Train Loss: 89.45590209960938 | MSE Test Loss: 87.23258209228516\n",
      "Epoch: 49500 | MSE Train Loss: 89.34376525878906 | MSE Test Loss: 87.11509704589844\n",
      "Epoch: 49600 | MSE Train Loss: 89.23204040527344 | MSE Test Loss: 86.998046875\n",
      "Epoch: 49700 | MSE Train Loss: 89.12069702148438 | MSE Test Loss: 86.88140869140625\n",
      "Epoch: 49800 | MSE Train Loss: 89.01081085205078 | MSE Test Loss: 86.76622772216797\n",
      "Epoch: 49900 | MSE Train Loss: 88.90170288085938 | MSE Test Loss: 86.65184020996094\n",
      "Epoch: 50000 | MSE Train Loss: 88.79297637939453 | MSE Test Loss: 86.53790283203125\n",
      "Epoch: 50100 | MSE Train Loss: 88.68472290039062 | MSE Test Loss: 86.42446899414062\n",
      "Epoch: 50200 | MSE Train Loss: 88.57820129394531 | MSE Test Loss: 86.31275939941406\n",
      "Epoch: 50300 | MSE Train Loss: 88.47203826904297 | MSE Test Loss: 86.20145416259766\n",
      "Epoch: 50400 | MSE Train Loss: 88.36627197265625 | MSE Test Loss: 86.09060668945312\n",
      "Epoch: 50500 | MSE Train Loss: 88.26131439208984 | MSE Test Loss: 85.98057556152344\n",
      "Epoch: 50600 | MSE Train Loss: 88.15769958496094 | MSE Test Loss: 85.8719253540039\n",
      "Epoch: 50700 | MSE Train Loss: 88.0544662475586 | MSE Test Loss: 85.76362609863281\n",
      "Epoch: 50800 | MSE Train Loss: 87.95158386230469 | MSE Test Loss: 85.65572357177734\n",
      "Epoch: 50900 | MSE Train Loss: 87.84981536865234 | MSE Test Loss: 85.54893493652344\n",
      "Epoch: 51000 | MSE Train Loss: 87.7490234375 | MSE Test Loss: 85.44315338134766\n",
      "Epoch: 51100 | MSE Train Loss: 87.64862060546875 | MSE Test Loss: 85.33776092529297\n",
      "Epoch: 51200 | MSE Train Loss: 87.5485610961914 | MSE Test Loss: 85.23275756835938\n",
      "Epoch: 51300 | MSE Train Loss: 87.4498062133789 | MSE Test Loss: 85.1291275024414\n",
      "Epoch: 51400 | MSE Train Loss: 87.3518295288086 | MSE Test Loss: 85.02626037597656\n",
      "Epoch: 51500 | MSE Train Loss: 87.25420379638672 | MSE Test Loss: 84.92375183105469\n",
      "Epoch: 51600 | MSE Train Loss: 87.15691375732422 | MSE Test Loss: 84.8216323852539\n",
      "Epoch: 51700 | MSE Train Loss: 87.06102752685547 | MSE Test Loss: 84.72087860107422\n",
      "Epoch: 51800 | MSE Train Loss: 86.96578979492188 | MSE Test Loss: 84.62088775634766\n",
      "Epoch: 51900 | MSE Train Loss: 86.87088012695312 | MSE Test Loss: 84.52123260498047\n",
      "Epoch: 52000 | MSE Train Loss: 86.77632141113281 | MSE Test Loss: 84.42190551757812\n",
      "Epoch: 52100 | MSE Train Loss: 86.68319702148438 | MSE Test Loss: 84.32406616210938\n",
      "Epoch: 52200 | MSE Train Loss: 86.59062194824219 | MSE Test Loss: 84.22677612304688\n",
      "Epoch: 52300 | MSE Train Loss: 86.49835968017578 | MSE Test Loss: 84.12982177734375\n",
      "Epoch: 52400 | MSE Train Loss: 86.40644836425781 | MSE Test Loss: 84.033203125\n",
      "Epoch: 52500 | MSE Train Loss: 86.3159408569336 | MSE Test Loss: 83.93807220458984\n",
      "Epoch: 52600 | MSE Train Loss: 86.22599029541016 | MSE Test Loss: 83.8435287475586\n",
      "Epoch: 52700 | MSE Train Loss: 86.13636016845703 | MSE Test Loss: 83.74928283691406\n",
      "Epoch: 52800 | MSE Train Loss: 86.04705810546875 | MSE Test Loss: 83.65538787841797\n",
      "Epoch: 52900 | MSE Train Loss: 85.95903778076172 | MSE Test Loss: 83.56278991699219\n",
      "Epoch: 53000 | MSE Train Loss: 85.87162780761719 | MSE Test Loss: 83.47084045410156\n",
      "Epoch: 53100 | MSE Train Loss: 85.78455352783203 | MSE Test Loss: 83.37925720214844\n",
      "Epoch: 53200 | MSE Train Loss: 85.6977767944336 | MSE Test Loss: 83.28797149658203\n",
      "Epoch: 53300 | MSE Train Loss: 85.61212158203125 | MSE Test Loss: 83.19784545898438\n",
      "Epoch: 53400 | MSE Train Loss: 85.5272216796875 | MSE Test Loss: 83.10845947265625\n",
      "Epoch: 53500 | MSE Train Loss: 85.44261169433594 | MSE Test Loss: 83.0194091796875\n",
      "Epoch: 53600 | MSE Train Loss: 85.35832214355469 | MSE Test Loss: 82.93067169189453\n",
      "Epoch: 53700 | MSE Train Loss: 85.2748794555664 | MSE Test Loss: 82.84280395507812\n",
      "Epoch: 53800 | MSE Train Loss: 85.19243621826172 | MSE Test Loss: 82.7560043334961\n",
      "Epoch: 53900 | MSE Train Loss: 85.11029815673828 | MSE Test Loss: 82.66954040527344\n",
      "Epoch: 54000 | MSE Train Loss: 85.0284423828125 | MSE Test Loss: 82.58338165283203\n",
      "Epoch: 54100 | MSE Train Loss: 84.94717407226562 | MSE Test Loss: 82.49778747558594\n",
      "Epoch: 54200 | MSE Train Loss: 84.86712646484375 | MSE Test Loss: 82.4134292602539\n",
      "Epoch: 54300 | MSE Train Loss: 84.78736877441406 | MSE Test Loss: 82.32938385009766\n",
      "Epoch: 54400 | MSE Train Loss: 84.7078857421875 | MSE Test Loss: 82.24565124511719\n",
      "Epoch: 54500 | MSE Train Loss: 84.62870025634766 | MSE Test Loss: 82.16222381591797\n",
      "Epoch: 54600 | MSE Train Loss: 84.5508804321289 | MSE Test Loss: 82.08019256591797\n",
      "Epoch: 54700 | MSE Train Loss: 84.47346496582031 | MSE Test Loss: 81.99856567382812\n",
      "Epoch: 54800 | MSE Train Loss: 84.39630889892578 | MSE Test Loss: 81.91724395751953\n",
      "Epoch: 54900 | MSE Train Loss: 84.3194351196289 | MSE Test Loss: 81.83618927001953\n",
      "Epoch: 55000 | MSE Train Loss: 84.24346160888672 | MSE Test Loss: 81.75601959228516\n",
      "Epoch: 55100 | MSE Train Loss: 84.1683120727539 | MSE Test Loss: 81.67672729492188\n",
      "Epoch: 55200 | MSE Train Loss: 84.09345245361328 | MSE Test Loss: 81.59776306152344\n",
      "Epoch: 55300 | MSE Train Loss: 84.01885986328125 | MSE Test Loss: 81.51912689208984\n",
      "Epoch: 55400 | MSE Train Loss: 83.94466400146484 | MSE Test Loss: 81.44087219238281\n",
      "Epoch: 55500 | MSE Train Loss: 83.87176513671875 | MSE Test Loss: 81.36394500732422\n",
      "Epoch: 55600 | MSE Train Loss: 83.79912567138672 | MSE Test Loss: 81.28728485107422\n",
      "Epoch: 55700 | MSE Train Loss: 83.72676086425781 | MSE Test Loss: 81.21090698242188\n",
      "Epoch: 55800 | MSE Train Loss: 83.6546630859375 | MSE Test Loss: 81.1347885131836\n",
      "Epoch: 55900 | MSE Train Loss: 83.58343505859375 | MSE Test Loss: 81.05957794189453\n",
      "Epoch: 56000 | MSE Train Loss: 83.51298522949219 | MSE Test Loss: 80.98513793945312\n",
      "Epoch: 56100 | MSE Train Loss: 83.44279479980469 | MSE Test Loss: 80.9110336303711\n",
      "Epoch: 56200 | MSE Train Loss: 83.37285614013672 | MSE Test Loss: 80.83716583251953\n",
      "Epoch: 56300 | MSE Train Loss: 83.30314636230469 | MSE Test Loss: 80.76355743408203\n",
      "Epoch: 56400 | MSE Train Loss: 83.23479461669922 | MSE Test Loss: 80.6912841796875\n",
      "Epoch: 56500 | MSE Train Loss: 83.16670989990234 | MSE Test Loss: 80.61931610107422\n",
      "Epoch: 56600 | MSE Train Loss: 83.098876953125 | MSE Test Loss: 80.54761505126953\n",
      "Epoch: 56700 | MSE Train Loss: 83.03128814697266 | MSE Test Loss: 80.47618103027344\n",
      "Epoch: 56800 | MSE Train Loss: 82.96427154541016 | MSE Test Loss: 80.40536499023438\n",
      "Epoch: 56900 | MSE Train Loss: 82.89830017089844 | MSE Test Loss: 80.3355712890625\n",
      "Epoch: 57000 | MSE Train Loss: 82.83256530761719 | MSE Test Loss: 80.2660903930664\n",
      "Epoch: 57100 | MSE Train Loss: 82.76705932617188 | MSE Test Loss: 80.19681549072266\n",
      "Epoch: 57200 | MSE Train Loss: 82.7017822265625 | MSE Test Loss: 80.12781524658203\n",
      "Epoch: 57300 | MSE Train Loss: 82.63732147216797 | MSE Test Loss: 80.05960845947266\n",
      "Epoch: 57400 | MSE Train Loss: 82.5736083984375 | MSE Test Loss: 79.99217224121094\n",
      "Epoch: 57500 | MSE Train Loss: 82.5101089477539 | MSE Test Loss: 79.92495727539062\n",
      "Epoch: 57600 | MSE Train Loss: 82.44685363769531 | MSE Test Loss: 79.85800170898438\n",
      "Epoch: 57700 | MSE Train Loss: 82.38379669189453 | MSE Test Loss: 79.791259765625\n",
      "Epoch: 57800 | MSE Train Loss: 82.32173919677734 | MSE Test Loss: 79.72551727294922\n",
      "Epoch: 57900 | MSE Train Loss: 82.26022338867188 | MSE Test Loss: 79.6603775024414\n",
      "Epoch: 58000 | MSE Train Loss: 82.19892120361328 | MSE Test Loss: 79.595458984375\n",
      "Epoch: 58100 | MSE Train Loss: 82.1378402709961 | MSE Test Loss: 79.53076934814453\n",
      "Epoch: 58200 | MSE Train Loss: 82.07698059082031 | MSE Test Loss: 79.46629333496094\n",
      "Epoch: 58300 | MSE Train Loss: 82.01717376708984 | MSE Test Loss: 79.40290832519531\n",
      "Epoch: 58400 | MSE Train Loss: 81.95780944824219 | MSE Test Loss: 79.34001159667969\n",
      "Epoch: 58500 | MSE Train Loss: 81.8986587524414 | MSE Test Loss: 79.27732849121094\n",
      "Epoch: 58600 | MSE Train Loss: 81.8397216796875 | MSE Test Loss: 79.2148666381836\n",
      "Epoch: 58700 | MSE Train Loss: 81.78099060058594 | MSE Test Loss: 79.15261840820312\n",
      "Epoch: 58800 | MSE Train Loss: 81.7232894897461 | MSE Test Loss: 79.09144592285156\n",
      "Epoch: 58900 | MSE Train Loss: 81.666015625 | MSE Test Loss: 79.03070068359375\n",
      "Epoch: 59000 | MSE Train Loss: 81.60894775390625 | MSE Test Loss: 78.97017669677734\n",
      "Epoch: 59100 | MSE Train Loss: 81.55207824707031 | MSE Test Loss: 78.90985870361328\n",
      "Epoch: 59200 | MSE Train Loss: 81.49542236328125 | MSE Test Loss: 78.84974670410156\n",
      "Epoch: 59300 | MSE Train Loss: 81.43965148925781 | MSE Test Loss: 78.7905502319336\n",
      "Epoch: 59400 | MSE Train Loss: 81.38441467285156 | MSE Test Loss: 78.73188018798828\n",
      "Epoch: 59500 | MSE Train Loss: 81.32935333251953 | MSE Test Loss: 78.67342376708984\n",
      "Epoch: 59600 | MSE Train Loss: 81.27450561523438 | MSE Test Loss: 78.6152114868164\n",
      "Epoch: 59700 | MSE Train Loss: 81.2198486328125 | MSE Test Loss: 78.55718994140625\n",
      "Epoch: 59800 | MSE Train Loss: 81.16590118408203 | MSE Test Loss: 78.49986267089844\n",
      "Epoch: 59900 | MSE Train Loss: 81.11264038085938 | MSE Test Loss: 78.44329833984375\n",
      "Epoch: 60000 | MSE Train Loss: 81.05957794189453 | MSE Test Loss: 78.38694763183594\n",
      "Epoch: 60100 | MSE Train Loss: 81.00669860839844 | MSE Test Loss: 78.33079528808594\n",
      "Epoch: 60200 | MSE Train Loss: 80.95402526855469 | MSE Test Loss: 78.27484130859375\n",
      "Epoch: 60300 | MSE Train Loss: 80.90174102783203 | MSE Test Loss: 78.21927642822266\n",
      "Epoch: 60400 | MSE Train Loss: 80.85040283203125 | MSE Test Loss: 78.16466522216797\n",
      "Epoch: 60500 | MSE Train Loss: 80.79924774169922 | MSE Test Loss: 78.11026763916016\n",
      "Epoch: 60600 | MSE Train Loss: 80.748291015625 | MSE Test Loss: 78.05611419677734\n",
      "Epoch: 60700 | MSE Train Loss: 80.697509765625 | MSE Test Loss: 78.00212860107422\n",
      "Epoch: 60800 | MSE Train Loss: 80.64691162109375 | MSE Test Loss: 77.94833374023438\n",
      "Epoch: 60900 | MSE Train Loss: 80.5972900390625 | MSE Test Loss: 77.8955078125\n",
      "Epoch: 61000 | MSE Train Loss: 80.54801177978516 | MSE Test Loss: 77.84306335449219\n",
      "Epoch: 61100 | MSE Train Loss: 80.4989013671875 | MSE Test Loss: 77.79077911376953\n",
      "Epoch: 61200 | MSE Train Loss: 80.4499740600586 | MSE Test Loss: 77.73870849609375\n",
      "Epoch: 61300 | MSE Train Loss: 80.4012222290039 | MSE Test Loss: 77.6867904663086\n",
      "Epoch: 61400 | MSE Train Loss: 80.3529281616211 | MSE Test Loss: 77.6353530883789\n",
      "Epoch: 61500 | MSE Train Loss: 80.30546569824219 | MSE Test Loss: 77.58474731445312\n",
      "Epoch: 61600 | MSE Train Loss: 80.2581787109375 | MSE Test Loss: 77.53437805175781\n",
      "Epoch: 61700 | MSE Train Loss: 80.2110595703125 | MSE Test Loss: 77.4842529296875\n",
      "Epoch: 61800 | MSE Train Loss: 80.16412353515625 | MSE Test Loss: 77.43428039550781\n",
      "Epoch: 61900 | MSE Train Loss: 80.11734771728516 | MSE Test Loss: 77.38446807861328\n",
      "Epoch: 62000 | MSE Train Loss: 80.0714111328125 | MSE Test Loss: 77.33550262451172\n",
      "Epoch: 62100 | MSE Train Loss: 80.02588653564453 | MSE Test Loss: 77.2869644165039\n",
      "Epoch: 62200 | MSE Train Loss: 79.98053741455078 | MSE Test Loss: 77.23859405517578\n",
      "Epoch: 62300 | MSE Train Loss: 79.93534088134766 | MSE Test Loss: 77.19039916992188\n",
      "Epoch: 62400 | MSE Train Loss: 79.89029693603516 | MSE Test Loss: 77.14237976074219\n",
      "Epoch: 62500 | MSE Train Loss: 79.84542846679688 | MSE Test Loss: 77.09452056884766\n",
      "Epoch: 62600 | MSE Train Loss: 79.80159759521484 | MSE Test Loss: 77.04772186279297\n",
      "Epoch: 62700 | MSE Train Loss: 79.75796508789062 | MSE Test Loss: 77.00115203857422\n",
      "Epoch: 62800 | MSE Train Loss: 79.71446228027344 | MSE Test Loss: 76.9547348022461\n",
      "Epoch: 62900 | MSE Train Loss: 79.67113494873047 | MSE Test Loss: 76.90847778320312\n",
      "Epoch: 63000 | MSE Train Loss: 79.62794494628906 | MSE Test Loss: 76.86237335205078\n",
      "Epoch: 63100 | MSE Train Loss: 79.58505249023438 | MSE Test Loss: 76.81658172607422\n",
      "Epoch: 63200 | MSE Train Loss: 79.54306030273438 | MSE Test Loss: 76.77169036865234\n",
      "Epoch: 63300 | MSE Train Loss: 79.501220703125 | MSE Test Loss: 76.72695922851562\n",
      "Epoch: 63400 | MSE Train Loss: 79.45952606201172 | MSE Test Loss: 76.68238830566406\n",
      "Epoch: 63500 | MSE Train Loss: 79.41798400878906 | MSE Test Loss: 76.63800048828125\n",
      "Epoch: 63600 | MSE Train Loss: 79.3766098022461 | MSE Test Loss: 76.59381103515625\n",
      "Epoch: 63700 | MSE Train Loss: 79.33553314208984 | MSE Test Loss: 76.54991149902344\n",
      "Epoch: 63800 | MSE Train Loss: 79.2953109741211 | MSE Test Loss: 76.50691223144531\n",
      "Epoch: 63900 | MSE Train Loss: 79.2552490234375 | MSE Test Loss: 76.46407318115234\n",
      "Epoch: 64000 | MSE Train Loss: 79.2153091430664 | MSE Test Loss: 76.42137908935547\n",
      "Epoch: 64100 | MSE Train Loss: 79.17552185058594 | MSE Test Loss: 76.37882995605469\n",
      "Epoch: 64200 | MSE Train Loss: 79.1358871459961 | MSE Test Loss: 76.33642578125\n",
      "Epoch: 64300 | MSE Train Loss: 79.09647369384766 | MSE Test Loss: 76.29426574707031\n",
      "Epoch: 64400 | MSE Train Loss: 79.05795288085938 | MSE Test Loss: 76.25299835205078\n",
      "Epoch: 64500 | MSE Train Loss: 79.01958465576172 | MSE Test Loss: 76.2118911743164\n",
      "Epoch: 64600 | MSE Train Loss: 78.98133850097656 | MSE Test Loss: 76.1709213256836\n",
      "Epoch: 64700 | MSE Train Loss: 78.94322967529297 | MSE Test Loss: 76.1301040649414\n",
      "Epoch: 64800 | MSE Train Loss: 78.90526580810547 | MSE Test Loss: 76.08943939208984\n",
      "Epoch: 64900 | MSE Train Loss: 78.867431640625 | MSE Test Loss: 76.04893493652344\n",
      "Epoch: 65000 | MSE Train Loss: 78.83045959472656 | MSE Test Loss: 76.00929260253906\n",
      "Epoch: 65100 | MSE Train Loss: 78.79373931884766 | MSE Test Loss: 75.96990966796875\n",
      "Epoch: 65200 | MSE Train Loss: 78.75712585449219 | MSE Test Loss: 75.93064880371094\n",
      "Epoch: 65300 | MSE Train Loss: 78.72065734863281 | MSE Test Loss: 75.89154052734375\n",
      "Epoch: 65400 | MSE Train Loss: 78.68431091308594 | MSE Test Loss: 75.85255432128906\n",
      "Epoch: 65500 | MSE Train Loss: 78.64810943603516 | MSE Test Loss: 75.81372833251953\n",
      "Epoch: 65600 | MSE Train Loss: 78.61243438720703 | MSE Test Loss: 75.77545928955078\n",
      "Epoch: 65700 | MSE Train Loss: 78.57730102539062 | MSE Test Loss: 75.73775482177734\n",
      "Epoch: 65800 | MSE Train Loss: 78.54229736328125 | MSE Test Loss: 75.70018768310547\n",
      "Epoch: 65900 | MSE Train Loss: 78.50741577148438 | MSE Test Loss: 75.66275024414062\n",
      "Epoch: 66000 | MSE Train Loss: 78.47266387939453 | MSE Test Loss: 75.6254653930664\n",
      "Epoch: 66100 | MSE Train Loss: 78.43803405761719 | MSE Test Loss: 75.58831024169922\n",
      "Epoch: 66200 | MSE Train Loss: 78.40352630615234 | MSE Test Loss: 75.55128479003906\n",
      "Epoch: 66300 | MSE Train Loss: 78.36992645263672 | MSE Test Loss: 75.51515197753906\n",
      "Epoch: 66400 | MSE Train Loss: 78.33646392822266 | MSE Test Loss: 75.47917175292969\n",
      "Epoch: 66500 | MSE Train Loss: 78.30311584472656 | MSE Test Loss: 75.44335174560547\n",
      "Epoch: 66600 | MSE Train Loss: 78.26988220214844 | MSE Test Loss: 75.4076156616211\n",
      "Epoch: 66700 | MSE Train Loss: 78.23676300048828 | MSE Test Loss: 75.37202453613281\n",
      "Epoch: 66800 | MSE Train Loss: 78.20377349853516 | MSE Test Loss: 75.3365478515625\n",
      "Epoch: 66900 | MSE Train Loss: 78.17113494873047 | MSE Test Loss: 75.30142974853516\n",
      "Epoch: 67000 | MSE Train Loss: 78.13915252685547 | MSE Test Loss: 75.2669677734375\n",
      "Epoch: 67100 | MSE Train Loss: 78.1072769165039 | MSE Test Loss: 75.23265075683594\n",
      "Epoch: 67200 | MSE Train Loss: 78.07551574707031 | MSE Test Loss: 75.198486328125\n",
      "Epoch: 67300 | MSE Train Loss: 78.04386138916016 | MSE Test Loss: 75.16443634033203\n",
      "Epoch: 67400 | MSE Train Loss: 78.01234436035156 | MSE Test Loss: 75.13048553466797\n",
      "Epoch: 67500 | MSE Train Loss: 77.98092651367188 | MSE Test Loss: 75.0966567993164\n",
      "Epoch: 67600 | MSE Train Loss: 77.9499740600586 | MSE Test Loss: 75.06330871582031\n",
      "Epoch: 67700 | MSE Train Loss: 77.91954040527344 | MSE Test Loss: 75.03050994873047\n",
      "Epoch: 67800 | MSE Train Loss: 77.88922119140625 | MSE Test Loss: 74.99786376953125\n",
      "Epoch: 67900 | MSE Train Loss: 77.8590087890625 | MSE Test Loss: 74.96532440185547\n",
      "Epoch: 68000 | MSE Train Loss: 77.82891082763672 | MSE Test Loss: 74.93286895751953\n",
      "Epoch: 68100 | MSE Train Loss: 77.79891204833984 | MSE Test Loss: 74.9005355834961\n",
      "Epoch: 68200 | MSE Train Loss: 77.76902770996094 | MSE Test Loss: 74.86832427978516\n",
      "Epoch: 68300 | MSE Train Loss: 77.73959350585938 | MSE Test Loss: 74.83655548095703\n",
      "Epoch: 68400 | MSE Train Loss: 77.71065521240234 | MSE Test Loss: 74.80529022216797\n",
      "Epoch: 68500 | MSE Train Loss: 77.68181610107422 | MSE Test Loss: 74.77420043945312\n",
      "Epoch: 68600 | MSE Train Loss: 77.653076171875 | MSE Test Loss: 74.74320983886719\n",
      "Epoch: 68700 | MSE Train Loss: 77.62446594238281 | MSE Test Loss: 74.7123031616211\n",
      "Epoch: 68800 | MSE Train Loss: 77.59593963623047 | MSE Test Loss: 74.68152618408203\n",
      "Epoch: 68900 | MSE Train Loss: 77.56751251220703 | MSE Test Loss: 74.65083312988281\n",
      "Epoch: 69000 | MSE Train Loss: 77.53939819335938 | MSE Test Loss: 74.6204833984375\n",
      "Epoch: 69100 | MSE Train Loss: 77.51189422607422 | MSE Test Loss: 74.59071350097656\n",
      "Epoch: 69200 | MSE Train Loss: 77.48448944091797 | MSE Test Loss: 74.56106567382812\n",
      "Epoch: 69300 | MSE Train Loss: 77.45718383789062 | MSE Test Loss: 74.53153991699219\n",
      "Epoch: 69400 | MSE Train Loss: 77.42996215820312 | MSE Test Loss: 74.5020980834961\n",
      "Epoch: 69500 | MSE Train Loss: 77.40284729003906 | MSE Test Loss: 74.47276306152344\n",
      "Epoch: 69600 | MSE Train Loss: 77.37582397460938 | MSE Test Loss: 74.44353485107422\n",
      "Epoch: 69700 | MSE Train Loss: 77.34891510009766 | MSE Test Loss: 74.41439056396484\n",
      "Epoch: 69800 | MSE Train Loss: 77.32270050048828 | MSE Test Loss: 74.385986328125\n",
      "Epoch: 69900 | MSE Train Loss: 77.29666900634766 | MSE Test Loss: 74.35780334472656\n",
      "Epoch: 70000 | MSE Train Loss: 77.27073669433594 | MSE Test Loss: 74.32975006103516\n",
      "Epoch: 70100 | MSE Train Loss: 77.24490356445312 | MSE Test Loss: 74.30180358886719\n",
      "Epoch: 70200 | MSE Train Loss: 77.21915435791016 | MSE Test Loss: 74.27395629882812\n",
      "Epoch: 70300 | MSE Train Loss: 77.1935043334961 | MSE Test Loss: 74.2461929321289\n",
      "Epoch: 70400 | MSE Train Loss: 77.16796112060547 | MSE Test Loss: 74.21852111816406\n",
      "Epoch: 70500 | MSE Train Loss: 77.14273071289062 | MSE Test Loss: 74.1911849975586\n",
      "Epoch: 70600 | MSE Train Loss: 77.11803436279297 | MSE Test Loss: 74.16438293457031\n",
      "Epoch: 70700 | MSE Train Loss: 77.09342193603516 | MSE Test Loss: 74.13768005371094\n",
      "Epoch: 70800 | MSE Train Loss: 77.06890106201172 | MSE Test Loss: 74.11107635498047\n",
      "Epoch: 70900 | MSE Train Loss: 77.04446411132812 | MSE Test Loss: 74.0845718383789\n",
      "Epoch: 71000 | MSE Train Loss: 77.0201187133789 | MSE Test Loss: 74.05815887451172\n",
      "Epoch: 71100 | MSE Train Loss: 76.995849609375 | MSE Test Loss: 74.03182220458984\n",
      "Epoch: 71200 | MSE Train Loss: 76.97167205810547 | MSE Test Loss: 74.00556945800781\n",
      "Epoch: 71300 | MSE Train Loss: 76.94795989990234 | MSE Test Loss: 73.97979736328125\n",
      "Epoch: 71400 | MSE Train Loss: 76.92461395263672 | MSE Test Loss: 73.95445251464844\n",
      "Epoch: 71500 | MSE Train Loss: 76.90135955810547 | MSE Test Loss: 73.92918395996094\n",
      "Epoch: 71600 | MSE Train Loss: 76.8781967163086 | MSE Test Loss: 73.90399932861328\n",
      "Epoch: 71700 | MSE Train Loss: 76.8551025390625 | MSE Test Loss: 73.87889862060547\n",
      "Epoch: 71800 | MSE Train Loss: 76.83209228515625 | MSE Test Loss: 73.85389709472656\n",
      "Epoch: 71900 | MSE Train Loss: 76.80917358398438 | MSE Test Loss: 73.82897186279297\n",
      "Epoch: 72000 | MSE Train Loss: 76.78632354736328 | MSE Test Loss: 73.80413055419922\n",
      "Epoch: 72100 | MSE Train Loss: 76.763916015625 | MSE Test Loss: 73.77970886230469\n",
      "Epoch: 72200 | MSE Train Loss: 76.74188995361328 | MSE Test Loss: 73.75568389892578\n",
      "Epoch: 72300 | MSE Train Loss: 76.71990966796875 | MSE Test Loss: 73.73176574707031\n",
      "Epoch: 72400 | MSE Train Loss: 76.69802856445312 | MSE Test Loss: 73.70791625976562\n",
      "Epoch: 72500 | MSE Train Loss: 76.67623901367188 | MSE Test Loss: 73.68415069580078\n",
      "Epoch: 72600 | MSE Train Loss: 76.6545181274414 | MSE Test Loss: 73.66053009033203\n",
      "Epoch: 72700 | MSE Train Loss: 76.63288879394531 | MSE Test Loss: 73.63700103759766\n",
      "Epoch: 72800 | MSE Train Loss: 76.611328125 | MSE Test Loss: 73.61354064941406\n",
      "Epoch: 72900 | MSE Train Loss: 76.59003448486328 | MSE Test Loss: 73.59033966064453\n",
      "Epoch: 73000 | MSE Train Loss: 76.56925201416016 | MSE Test Loss: 73.56766510009766\n",
      "Epoch: 73100 | MSE Train Loss: 76.54854583740234 | MSE Test Loss: 73.54509735107422\n",
      "Epoch: 73200 | MSE Train Loss: 76.52791595458984 | MSE Test Loss: 73.52259063720703\n",
      "Epoch: 73300 | MSE Train Loss: 76.50736236572266 | MSE Test Loss: 73.50016784667969\n",
      "Epoch: 73400 | MSE Train Loss: 76.48687744140625 | MSE Test Loss: 73.47782135009766\n",
      "Epoch: 73500 | MSE Train Loss: 76.46646118164062 | MSE Test Loss: 73.45555114746094\n",
      "Epoch: 73600 | MSE Train Loss: 76.44612121582031 | MSE Test Loss: 73.433349609375\n",
      "Epoch: 73700 | MSE Train Loss: 76.42586517333984 | MSE Test Loss: 73.41122436523438\n",
      "Epoch: 73800 | MSE Train Loss: 76.40615844726562 | MSE Test Loss: 73.38966369628906\n",
      "Epoch: 73900 | MSE Train Loss: 76.38663482666016 | MSE Test Loss: 73.3683090209961\n",
      "Epoch: 74000 | MSE Train Loss: 76.36719512939453 | MSE Test Loss: 73.3470230102539\n",
      "Epoch: 74100 | MSE Train Loss: 76.34780883789062 | MSE Test Loss: 73.32583618164062\n",
      "Epoch: 74200 | MSE Train Loss: 76.32849884033203 | MSE Test Loss: 73.30473327636719\n",
      "Epoch: 74300 | MSE Train Loss: 76.30926513671875 | MSE Test Loss: 73.28369903564453\n",
      "Epoch: 74400 | MSE Train Loss: 76.29010009765625 | MSE Test Loss: 73.26273345947266\n",
      "Epoch: 74500 | MSE Train Loss: 76.27098846435547 | MSE Test Loss: 73.24183654785156\n",
      "Epoch: 74600 | MSE Train Loss: 76.2519760131836 | MSE Test Loss: 73.22103881835938\n",
      "Epoch: 74700 | MSE Train Loss: 76.23360443115234 | MSE Test Loss: 73.20086669921875\n",
      "Epoch: 74800 | MSE Train Loss: 76.21529388427734 | MSE Test Loss: 73.18077850341797\n",
      "Epoch: 74900 | MSE Train Loss: 76.19705200195312 | MSE Test Loss: 73.16077423095703\n",
      "Epoch: 75000 | MSE Train Loss: 76.1788558959961 | MSE Test Loss: 73.1408462524414\n",
      "Epoch: 75100 | MSE Train Loss: 76.16075134277344 | MSE Test Loss: 73.1209716796875\n",
      "Epoch: 75200 | MSE Train Loss: 76.14269256591797 | MSE Test Loss: 73.10116577148438\n",
      "Epoch: 75300 | MSE Train Loss: 76.12471008300781 | MSE Test Loss: 73.08142852783203\n",
      "Epoch: 75400 | MSE Train Loss: 76.1067886352539 | MSE Test Loss: 73.06178283691406\n",
      "Epoch: 75500 | MSE Train Loss: 76.08892822265625 | MSE Test Loss: 73.042236328125\n",
      "Epoch: 75600 | MSE Train Loss: 76.07171630859375 | MSE Test Loss: 73.02330017089844\n",
      "Epoch: 75700 | MSE Train Loss: 76.0545654296875 | MSE Test Loss: 73.00444030761719\n",
      "Epoch: 75800 | MSE Train Loss: 76.03746795654297 | MSE Test Loss: 72.98567199707031\n",
      "Epoch: 75900 | MSE Train Loss: 76.02043914794922 | MSE Test Loss: 72.96698760986328\n",
      "Epoch: 76000 | MSE Train Loss: 76.00346374511719 | MSE Test Loss: 72.94835662841797\n",
      "Epoch: 76100 | MSE Train Loss: 75.9865493774414 | MSE Test Loss: 72.92977905273438\n",
      "Epoch: 76200 | MSE Train Loss: 75.9697036743164 | MSE Test Loss: 72.9112777709961\n",
      "Epoch: 76300 | MSE Train Loss: 75.9529037475586 | MSE Test Loss: 72.89283752441406\n",
      "Epoch: 76400 | MSE Train Loss: 75.93617248535156 | MSE Test Loss: 72.87444305419922\n",
      "Epoch: 76500 | MSE Train Loss: 75.91988372802734 | MSE Test Loss: 72.85649108886719\n",
      "Epoch: 76600 | MSE Train Loss: 75.90381622314453 | MSE Test Loss: 72.83879089355469\n",
      "Epoch: 76700 | MSE Train Loss: 75.88780975341797 | MSE Test Loss: 72.82115936279297\n",
      "Epoch: 76800 | MSE Train Loss: 75.87186431884766 | MSE Test Loss: 72.80359649658203\n",
      "Epoch: 76900 | MSE Train Loss: 75.85597229003906 | MSE Test Loss: 72.78607940673828\n",
      "Epoch: 77000 | MSE Train Loss: 75.84014129638672 | MSE Test Loss: 72.76863098144531\n",
      "Epoch: 77100 | MSE Train Loss: 75.82435607910156 | MSE Test Loss: 72.75123596191406\n",
      "Epoch: 77200 | MSE Train Loss: 75.80863952636719 | MSE Test Loss: 72.73390197753906\n",
      "Epoch: 77300 | MSE Train Loss: 75.79296875 | MSE Test Loss: 72.71662139892578\n",
      "Epoch: 77400 | MSE Train Loss: 75.77735900878906 | MSE Test Loss: 72.69939422607422\n",
      "Epoch: 77500 | MSE Train Loss: 75.76232147216797 | MSE Test Loss: 72.68275451660156\n",
      "Epoch: 77600 | MSE Train Loss: 75.74736022949219 | MSE Test Loss: 72.6662368774414\n",
      "Epoch: 77700 | MSE Train Loss: 75.73245239257812 | MSE Test Loss: 72.64977264404297\n",
      "Epoch: 77800 | MSE Train Loss: 75.71759033203125 | MSE Test Loss: 72.63336181640625\n",
      "Epoch: 77900 | MSE Train Loss: 75.7027816772461 | MSE Test Loss: 72.61700439453125\n",
      "Epoch: 78000 | MSE Train Loss: 75.68801879882812 | MSE Test Loss: 72.60070037841797\n",
      "Epoch: 78100 | MSE Train Loss: 75.67332458496094 | MSE Test Loss: 72.58443450927734\n",
      "Epoch: 78200 | MSE Train Loss: 75.6586685180664 | MSE Test Loss: 72.5682373046875\n",
      "Epoch: 78300 | MSE Train Loss: 75.64407348632812 | MSE Test Loss: 72.55208587646484\n",
      "Epoch: 78400 | MSE Train Loss: 75.62952423095703 | MSE Test Loss: 72.53600311279297\n",
      "Epoch: 78500 | MSE Train Loss: 75.61553955078125 | MSE Test Loss: 72.52051544189453\n",
      "Epoch: 78600 | MSE Train Loss: 75.60161590576172 | MSE Test Loss: 72.50509643554688\n",
      "Epoch: 78700 | MSE Train Loss: 75.58775329589844 | MSE Test Loss: 72.48975372314453\n",
      "Epoch: 78800 | MSE Train Loss: 75.57392120361328 | MSE Test Loss: 72.47443389892578\n",
      "Epoch: 78900 | MSE Train Loss: 75.56015014648438 | MSE Test Loss: 72.45917510986328\n",
      "Epoch: 79000 | MSE Train Loss: 75.54642486572266 | MSE Test Loss: 72.4439697265625\n",
      "Epoch: 79100 | MSE Train Loss: 75.5327377319336 | MSE Test Loss: 72.42881774902344\n",
      "Epoch: 79200 | MSE Train Loss: 75.51911163330078 | MSE Test Loss: 72.41371154785156\n",
      "Epoch: 79300 | MSE Train Loss: 75.50554656982422 | MSE Test Loss: 72.3986587524414\n",
      "Epoch: 79400 | MSE Train Loss: 75.49200439453125 | MSE Test Loss: 72.38365173339844\n",
      "Epoch: 79500 | MSE Train Loss: 75.47883605957031 | MSE Test Loss: 72.3689956665039\n",
      "Epoch: 79600 | MSE Train Loss: 75.46588897705078 | MSE Test Loss: 72.35462951660156\n",
      "Epoch: 79700 | MSE Train Loss: 75.4530029296875 | MSE Test Loss: 72.34031677246094\n",
      "Epoch: 79800 | MSE Train Loss: 75.44014739990234 | MSE Test Loss: 72.32606506347656\n",
      "Epoch: 79900 | MSE Train Loss: 75.42735290527344 | MSE Test Loss: 72.31183624267578\n",
      "Epoch: 80000 | MSE Train Loss: 75.41459655761719 | MSE Test Loss: 72.29766082763672\n",
      "Epoch: 80100 | MSE Train Loss: 75.4018783569336 | MSE Test Loss: 72.2835464477539\n",
      "Epoch: 80200 | MSE Train Loss: 75.38922119140625 | MSE Test Loss: 72.26946258544922\n",
      "Epoch: 80300 | MSE Train Loss: 75.3766098022461 | MSE Test Loss: 72.25542449951172\n",
      "Epoch: 80400 | MSE Train Loss: 75.3640365600586 | MSE Test Loss: 72.24144744873047\n",
      "Epoch: 80500 | MSE Train Loss: 75.35150909423828 | MSE Test Loss: 72.22750091552734\n",
      "Epoch: 80600 | MSE Train Loss: 75.33942413330078 | MSE Test Loss: 72.21400451660156\n",
      "Epoch: 80700 | MSE Train Loss: 75.32746124267578 | MSE Test Loss: 72.20064544677734\n",
      "Epoch: 80800 | MSE Train Loss: 75.31554412841797 | MSE Test Loss: 72.18733215332031\n",
      "Epoch: 80900 | MSE Train Loss: 75.30366516113281 | MSE Test Loss: 72.174072265625\n",
      "Epoch: 81000 | MSE Train Loss: 75.29181671142578 | MSE Test Loss: 72.16085052490234\n",
      "Epoch: 81100 | MSE Train Loss: 75.28002166748047 | MSE Test Loss: 72.1476821899414\n",
      "Epoch: 81200 | MSE Train Loss: 75.26827239990234 | MSE Test Loss: 72.13455200195312\n",
      "Epoch: 81300 | MSE Train Loss: 75.25655364990234 | MSE Test Loss: 72.1214599609375\n",
      "Epoch: 81400 | MSE Train Loss: 75.244873046875 | MSE Test Loss: 72.10841369628906\n",
      "Epoch: 81500 | MSE Train Loss: 75.23326110839844 | MSE Test Loss: 72.09542846679688\n",
      "Epoch: 81600 | MSE Train Loss: 75.22166442871094 | MSE Test Loss: 72.08248901367188\n",
      "Epoch: 81700 | MSE Train Loss: 75.21038818359375 | MSE Test Loss: 72.06986236572266\n",
      "Epoch: 81800 | MSE Train Loss: 75.1993408203125 | MSE Test Loss: 72.05747985839844\n",
      "Epoch: 81900 | MSE Train Loss: 75.1883316040039 | MSE Test Loss: 72.045166015625\n",
      "Epoch: 82000 | MSE Train Loss: 75.17738342285156 | MSE Test Loss: 72.03292846679688\n",
      "Epoch: 82100 | MSE Train Loss: 75.16645050048828 | MSE Test Loss: 72.02072143554688\n",
      "Epoch: 82200 | MSE Train Loss: 75.15557861328125 | MSE Test Loss: 72.00855255126953\n",
      "Epoch: 82300 | MSE Train Loss: 75.14472198486328 | MSE Test Loss: 71.99642944335938\n",
      "Epoch: 82400 | MSE Train Loss: 75.13392639160156 | MSE Test Loss: 71.9843521118164\n",
      "Epoch: 82500 | MSE Train Loss: 75.12315368652344 | MSE Test Loss: 71.9722900390625\n",
      "Epoch: 82600 | MSE Train Loss: 75.1124267578125 | MSE Test Loss: 71.96028900146484\n",
      "Epoch: 82700 | MSE Train Loss: 75.10173797607422 | MSE Test Loss: 71.94831085205078\n",
      "Epoch: 82800 | MSE Train Loss: 75.0910873413086 | MSE Test Loss: 71.9363784790039\n",
      "Epoch: 82900 | MSE Train Loss: 75.08090209960938 | MSE Test Loss: 71.92489624023438\n",
      "Epoch: 83000 | MSE Train Loss: 75.07076263427734 | MSE Test Loss: 71.91349029541016\n",
      "Epoch: 83100 | MSE Train Loss: 75.06066131591797 | MSE Test Loss: 71.90213775634766\n",
      "Epoch: 83200 | MSE Train Loss: 75.05059051513672 | MSE Test Loss: 71.89082336425781\n",
      "Epoch: 83300 | MSE Train Loss: 75.04056549072266 | MSE Test Loss: 71.87954711914062\n",
      "Epoch: 83400 | MSE Train Loss: 75.03056335449219 | MSE Test Loss: 71.86829376220703\n",
      "Epoch: 83500 | MSE Train Loss: 75.02059936523438 | MSE Test Loss: 71.85710144042969\n",
      "Epoch: 83600 | MSE Train Loss: 75.01067352294922 | MSE Test Loss: 71.84595489501953\n",
      "Epoch: 83700 | MSE Train Loss: 75.00078582763672 | MSE Test Loss: 71.83484649658203\n",
      "Epoch: 83800 | MSE Train Loss: 74.9909439086914 | MSE Test Loss: 71.8237533569336\n",
      "Epoch: 83900 | MSE Train Loss: 74.98113250732422 | MSE Test Loss: 71.8127212524414\n",
      "Epoch: 84000 | MSE Train Loss: 74.97134399414062 | MSE Test Loss: 71.80169677734375\n",
      "Epoch: 84100 | MSE Train Loss: 74.96192169189453 | MSE Test Loss: 71.79106140136719\n",
      "Epoch: 84200 | MSE Train Loss: 74.95262145996094 | MSE Test Loss: 71.78056335449219\n",
      "Epoch: 84300 | MSE Train Loss: 74.943359375 | MSE Test Loss: 71.77011108398438\n",
      "Epoch: 84400 | MSE Train Loss: 74.93413543701172 | MSE Test Loss: 71.75968933105469\n",
      "Epoch: 84500 | MSE Train Loss: 74.92493438720703 | MSE Test Loss: 71.74932098388672\n",
      "Epoch: 84600 | MSE Train Loss: 74.915771484375 | MSE Test Loss: 71.73896789550781\n",
      "Epoch: 84700 | MSE Train Loss: 74.90664672851562 | MSE Test Loss: 71.72865295410156\n",
      "Epoch: 84800 | MSE Train Loss: 74.89754486083984 | MSE Test Loss: 71.71837615966797\n",
      "Epoch: 84900 | MSE Train Loss: 74.88848114013672 | MSE Test Loss: 71.70812225341797\n",
      "Epoch: 85000 | MSE Train Loss: 74.87944793701172 | MSE Test Loss: 71.69792175292969\n",
      "Epoch: 85100 | MSE Train Loss: 74.87045288085938 | MSE Test Loss: 71.68773651123047\n",
      "Epoch: 85200 | MSE Train Loss: 74.86148834228516 | MSE Test Loss: 71.67758178710938\n",
      "Epoch: 85300 | MSE Train Loss: 74.85254669189453 | MSE Test Loss: 71.66747283935547\n",
      "Epoch: 85400 | MSE Train Loss: 74.84404754638672 | MSE Test Loss: 71.65778350830078\n",
      "Epoch: 85500 | MSE Train Loss: 74.8355712890625 | MSE Test Loss: 71.64813995361328\n",
      "Epoch: 85600 | MSE Train Loss: 74.82713317871094 | MSE Test Loss: 71.63854217529297\n",
      "Epoch: 85700 | MSE Train Loss: 74.8187255859375 | MSE Test Loss: 71.62899017333984\n",
      "Epoch: 85800 | MSE Train Loss: 74.81034088134766 | MSE Test Loss: 71.6194839477539\n",
      "Epoch: 85900 | MSE Train Loss: 74.80198669433594 | MSE Test Loss: 71.61001586914062\n",
      "Epoch: 86000 | MSE Train Loss: 74.7936782836914 | MSE Test Loss: 71.60060119628906\n",
      "Epoch: 86100 | MSE Train Loss: 74.78538513183594 | MSE Test Loss: 71.59121704101562\n",
      "Epoch: 86200 | MSE Train Loss: 74.7771224975586 | MSE Test Loss: 71.58185577392578\n",
      "Epoch: 86300 | MSE Train Loss: 74.7688980102539 | MSE Test Loss: 71.5725326538086\n",
      "Epoch: 86400 | MSE Train Loss: 74.76071166992188 | MSE Test Loss: 71.563232421875\n",
      "Epoch: 86500 | MSE Train Loss: 74.7525405883789 | MSE Test Loss: 71.55397033691406\n",
      "Epoch: 86600 | MSE Train Loss: 74.74440002441406 | MSE Test Loss: 71.54473114013672\n",
      "Epoch: 86700 | MSE Train Loss: 74.7365493774414 | MSE Test Loss: 71.5357894897461\n",
      "Epoch: 86800 | MSE Train Loss: 74.7288589477539 | MSE Test Loss: 71.52701568603516\n",
      "Epoch: 86900 | MSE Train Loss: 74.72118377685547 | MSE Test Loss: 71.51827239990234\n",
      "Epoch: 87000 | MSE Train Loss: 74.71353912353516 | MSE Test Loss: 71.50955963134766\n",
      "Epoch: 87100 | MSE Train Loss: 74.70592498779297 | MSE Test Loss: 71.50088500976562\n",
      "Epoch: 87200 | MSE Train Loss: 74.69833374023438 | MSE Test Loss: 71.49224090576172\n",
      "Epoch: 87300 | MSE Train Loss: 74.6907730102539 | MSE Test Loss: 71.4836196899414\n",
      "Epoch: 87400 | MSE Train Loss: 74.68323516845703 | MSE Test Loss: 71.47502136230469\n",
      "Epoch: 87500 | MSE Train Loss: 74.67572784423828 | MSE Test Loss: 71.4664535522461\n",
      "Epoch: 87600 | MSE Train Loss: 74.66825103759766 | MSE Test Loss: 71.45791625976562\n",
      "Epoch: 87700 | MSE Train Loss: 74.6607894897461 | MSE Test Loss: 71.44940948486328\n",
      "Epoch: 87800 | MSE Train Loss: 74.65336608886719 | MSE Test Loss: 71.44091796875\n",
      "Epoch: 87900 | MSE Train Loss: 74.64596557617188 | MSE Test Loss: 71.43246459960938\n",
      "Epoch: 88000 | MSE Train Loss: 74.6385726928711 | MSE Test Loss: 71.42403411865234\n",
      "Epoch: 88100 | MSE Train Loss: 74.63147735595703 | MSE Test Loss: 71.41586303710938\n",
      "Epoch: 88200 | MSE Train Loss: 74.62451934814453 | MSE Test Loss: 71.4078598022461\n",
      "Epoch: 88300 | MSE Train Loss: 74.61758422851562 | MSE Test Loss: 71.39993286132812\n",
      "Epoch: 88400 | MSE Train Loss: 74.61067199707031 | MSE Test Loss: 71.39201354980469\n",
      "Epoch: 88500 | MSE Train Loss: 74.60379028320312 | MSE Test Loss: 71.38414001464844\n",
      "Epoch: 88600 | MSE Train Loss: 74.596923828125 | MSE Test Loss: 71.37626647949219\n",
      "Epoch: 88700 | MSE Train Loss: 74.59010314941406 | MSE Test Loss: 71.36842346191406\n",
      "Epoch: 88800 | MSE Train Loss: 74.5832748413086 | MSE Test Loss: 71.3606185913086\n",
      "Epoch: 88900 | MSE Train Loss: 74.57649230957031 | MSE Test Loss: 71.35282897949219\n",
      "Epoch: 89000 | MSE Train Loss: 74.5697250366211 | MSE Test Loss: 71.3450698852539\n",
      "Epoch: 89100 | MSE Train Loss: 74.56298828125 | MSE Test Loss: 71.33732604980469\n",
      "Epoch: 89200 | MSE Train Loss: 74.55628967285156 | MSE Test Loss: 71.3296127319336\n",
      "Epoch: 89300 | MSE Train Loss: 74.5495834350586 | MSE Test Loss: 71.3219223022461\n",
      "Epoch: 89400 | MSE Train Loss: 74.54291534423828 | MSE Test Loss: 71.31425476074219\n",
      "Epoch: 89500 | MSE Train Loss: 74.5362777709961 | MSE Test Loss: 71.30662536621094\n",
      "Epoch: 89600 | MSE Train Loss: 74.52997589111328 | MSE Test Loss: 71.29931640625\n",
      "Epoch: 89700 | MSE Train Loss: 74.52371978759766 | MSE Test Loss: 71.29207611083984\n",
      "Epoch: 89800 | MSE Train Loss: 74.51749420166016 | MSE Test Loss: 71.28488159179688\n",
      "Epoch: 89900 | MSE Train Loss: 74.51128387451172 | MSE Test Loss: 71.27771759033203\n",
      "Epoch: 90000 | MSE Train Loss: 74.50509643554688 | MSE Test Loss: 71.27056121826172\n",
      "Epoch: 90100 | MSE Train Loss: 74.49893188476562 | MSE Test Loss: 71.26343536376953\n",
      "Epoch: 90200 | MSE Train Loss: 74.49279022216797 | MSE Test Loss: 71.25633239746094\n",
      "Epoch: 90300 | MSE Train Loss: 74.4866714477539 | MSE Test Loss: 71.24925994873047\n",
      "Epoch: 90400 | MSE Train Loss: 74.48057556152344 | MSE Test Loss: 71.24219512939453\n",
      "Epoch: 90500 | MSE Train Loss: 74.47450256347656 | MSE Test Loss: 71.23516845703125\n",
      "Epoch: 90600 | MSE Train Loss: 74.46845245361328 | MSE Test Loss: 71.22818756103516\n",
      "Epoch: 90700 | MSE Train Loss: 74.46241760253906 | MSE Test Loss: 71.22123718261719\n",
      "Epoch: 90800 | MSE Train Loss: 74.45641326904297 | MSE Test Loss: 71.21429443359375\n",
      "Epoch: 90900 | MSE Train Loss: 74.45043182373047 | MSE Test Loss: 71.20736694335938\n",
      "Epoch: 91000 | MSE Train Loss: 74.44446563720703 | MSE Test Loss: 71.20047760009766\n",
      "Epoch: 91100 | MSE Train Loss: 74.43861389160156 | MSE Test Loss: 71.19367980957031\n",
      "Epoch: 91200 | MSE Train Loss: 74.43302154541016 | MSE Test Loss: 71.18720245361328\n",
      "Epoch: 91300 | MSE Train Loss: 74.42745208740234 | MSE Test Loss: 71.18074035644531\n",
      "Epoch: 91400 | MSE Train Loss: 74.4218978881836 | MSE Test Loss: 71.17430877685547\n",
      "Epoch: 91500 | MSE Train Loss: 74.4163589477539 | MSE Test Loss: 71.16789245605469\n",
      "Epoch: 91600 | MSE Train Loss: 74.41085052490234 | MSE Test Loss: 71.1615219116211\n",
      "Epoch: 91700 | MSE Train Loss: 74.40536499023438 | MSE Test Loss: 71.15514373779297\n",
      "Epoch: 91800 | MSE Train Loss: 74.39988708496094 | MSE Test Loss: 71.1487808227539\n",
      "Epoch: 91900 | MSE Train Loss: 74.39443969726562 | MSE Test Loss: 71.14244842529297\n",
      "Epoch: 92000 | MSE Train Loss: 74.38899993896484 | MSE Test Loss: 71.13614654541016\n",
      "Epoch: 92100 | MSE Train Loss: 74.38357543945312 | MSE Test Loss: 71.12984466552734\n",
      "Epoch: 92200 | MSE Train Loss: 74.37818908691406 | MSE Test Loss: 71.12356567382812\n",
      "Epoch: 92300 | MSE Train Loss: 74.372802734375 | MSE Test Loss: 71.11732482910156\n",
      "Epoch: 92400 | MSE Train Loss: 74.36744689941406 | MSE Test Loss: 71.11109161376953\n",
      "Epoch: 92500 | MSE Train Loss: 74.36211395263672 | MSE Test Loss: 71.10486602783203\n",
      "Epoch: 92600 | MSE Train Loss: 74.3567886352539 | MSE Test Loss: 71.09867095947266\n",
      "Epoch: 92700 | MSE Train Loss: 74.35148620605469 | MSE Test Loss: 71.0925064086914\n",
      "Epoch: 92800 | MSE Train Loss: 74.34649658203125 | MSE Test Loss: 71.08662414550781\n",
      "Epoch: 92900 | MSE Train Loss: 74.34153747558594 | MSE Test Loss: 71.080810546875\n",
      "Epoch: 93000 | MSE Train Loss: 74.33660125732422 | MSE Test Loss: 71.07501983642578\n",
      "Epoch: 93100 | MSE Train Loss: 74.33167266845703 | MSE Test Loss: 71.06925964355469\n",
      "Epoch: 93200 | MSE Train Loss: 74.32676696777344 | MSE Test Loss: 71.06352233886719\n",
      "Epoch: 93300 | MSE Train Loss: 74.32188415527344 | MSE Test Loss: 71.05778503417969\n",
      "Epoch: 93400 | MSE Train Loss: 74.31700897216797 | MSE Test Loss: 71.05207824707031\n",
      "Epoch: 93500 | MSE Train Loss: 74.31216430664062 | MSE Test Loss: 71.04638671875\n",
      "Epoch: 93600 | MSE Train Loss: 74.30733489990234 | MSE Test Loss: 71.04071807861328\n",
      "Epoch: 93700 | MSE Train Loss: 74.30249786376953 | MSE Test Loss: 71.03506469726562\n",
      "Epoch: 93800 | MSE Train Loss: 74.29771423339844 | MSE Test Loss: 71.02942657470703\n",
      "Epoch: 93900 | MSE Train Loss: 74.29292297363281 | MSE Test Loss: 71.0238037109375\n",
      "Epoch: 94000 | MSE Train Loss: 74.28816223144531 | MSE Test Loss: 71.01820373535156\n",
      "Epoch: 94100 | MSE Train Loss: 74.28340911865234 | MSE Test Loss: 71.01262664794922\n",
      "Epoch: 94200 | MSE Train Loss: 74.27867889404297 | MSE Test Loss: 71.00707244873047\n",
      "Epoch: 94300 | MSE Train Loss: 74.27397155761719 | MSE Test Loss: 71.00153350830078\n",
      "Epoch: 94400 | MSE Train Loss: 74.26927185058594 | MSE Test Loss: 70.99600982666016\n",
      "Epoch: 94500 | MSE Train Loss: 74.26467895507812 | MSE Test Loss: 70.99059295654297\n",
      "Epoch: 94600 | MSE Train Loss: 74.26030731201172 | MSE Test Loss: 70.98542022705078\n",
      "Epoch: 94700 | MSE Train Loss: 74.25595092773438 | MSE Test Loss: 70.98027801513672\n",
      "Epoch: 94800 | MSE Train Loss: 74.2516098022461 | MSE Test Loss: 70.97514343261719\n",
      "Epoch: 94900 | MSE Train Loss: 74.24727630615234 | MSE Test Loss: 70.97003936767578\n",
      "Epoch: 95000 | MSE Train Loss: 74.24296569824219 | MSE Test Loss: 70.96495056152344\n",
      "Epoch: 95100 | MSE Train Loss: 74.2386703491211 | MSE Test Loss: 70.95987701416016\n",
      "Epoch: 95200 | MSE Train Loss: 74.23439025878906 | MSE Test Loss: 70.9548110961914\n",
      "Epoch: 95300 | MSE Train Loss: 74.23011779785156 | MSE Test Loss: 70.94975280761719\n",
      "Epoch: 95400 | MSE Train Loss: 74.22587585449219 | MSE Test Loss: 70.9447250366211\n",
      "Epoch: 95500 | MSE Train Loss: 74.22162628173828 | MSE Test Loss: 70.939697265625\n",
      "Epoch: 95600 | MSE Train Loss: 74.21741485595703 | MSE Test Loss: 70.93470764160156\n",
      "Epoch: 95700 | MSE Train Loss: 74.21320343017578 | MSE Test Loss: 70.92971801757812\n",
      "Epoch: 95800 | MSE Train Loss: 74.20901489257812 | MSE Test Loss: 70.92474365234375\n",
      "Epoch: 95900 | MSE Train Loss: 74.204833984375 | MSE Test Loss: 70.9197998046875\n",
      "Epoch: 96000 | MSE Train Loss: 74.20067596435547 | MSE Test Loss: 70.91485595703125\n",
      "Epoch: 96100 | MSE Train Loss: 74.19654083251953 | MSE Test Loss: 70.9099349975586\n",
      "Epoch: 96200 | MSE Train Loss: 74.19239807128906 | MSE Test Loss: 70.9050521850586\n",
      "Epoch: 96300 | MSE Train Loss: 74.18828582763672 | MSE Test Loss: 70.90017700195312\n",
      "Epoch: 96400 | MSE Train Loss: 74.18436431884766 | MSE Test Loss: 70.89550018310547\n",
      "Epoch: 96500 | MSE Train Loss: 74.1805419921875 | MSE Test Loss: 70.89093780517578\n",
      "Epoch: 96600 | MSE Train Loss: 74.17675018310547 | MSE Test Loss: 70.88642120361328\n",
      "Epoch: 96700 | MSE Train Loss: 74.17296600341797 | MSE Test Loss: 70.88191223144531\n",
      "Epoch: 96800 | MSE Train Loss: 74.169189453125 | MSE Test Loss: 70.87741088867188\n",
      "Epoch: 96900 | MSE Train Loss: 74.16543579101562 | MSE Test Loss: 70.87293243408203\n",
      "Epoch: 97000 | MSE Train Loss: 74.16168975830078 | MSE Test Loss: 70.86846923828125\n",
      "Epoch: 97100 | MSE Train Loss: 74.15795135498047 | MSE Test Loss: 70.864013671875\n",
      "Epoch: 97200 | MSE Train Loss: 74.15422821044922 | MSE Test Loss: 70.85957336425781\n",
      "Epoch: 97300 | MSE Train Loss: 74.1505355834961 | MSE Test Loss: 70.85515594482422\n",
      "Epoch: 97400 | MSE Train Loss: 74.14683532714844 | MSE Test Loss: 70.85073852539062\n",
      "Epoch: 97500 | MSE Train Loss: 74.1431655883789 | MSE Test Loss: 70.8463363647461\n",
      "Epoch: 97600 | MSE Train Loss: 74.13948822021484 | MSE Test Loss: 70.84197235107422\n",
      "Epoch: 97700 | MSE Train Loss: 74.13583374023438 | MSE Test Loss: 70.83761596679688\n",
      "Epoch: 97800 | MSE Train Loss: 74.1322021484375 | MSE Test Loss: 70.83325958251953\n",
      "Epoch: 97900 | MSE Train Loss: 74.12857818603516 | MSE Test Loss: 70.82893371582031\n",
      "Epoch: 98000 | MSE Train Loss: 74.12496185302734 | MSE Test Loss: 70.8246078491211\n",
      "Epoch: 98100 | MSE Train Loss: 74.12135314941406 | MSE Test Loss: 70.82030487060547\n",
      "Epoch: 98200 | MSE Train Loss: 74.1177749633789 | MSE Test Loss: 70.81600952148438\n",
      "Epoch: 98300 | MSE Train Loss: 74.11419677734375 | MSE Test Loss: 70.81172943115234\n",
      "Epoch: 98400 | MSE Train Loss: 74.11080169677734 | MSE Test Loss: 70.8076400756836\n",
      "Epoch: 98500 | MSE Train Loss: 74.10749816894531 | MSE Test Loss: 70.80366516113281\n",
      "Epoch: 98600 | MSE Train Loss: 74.10420989990234 | MSE Test Loss: 70.79971313476562\n",
      "Epoch: 98700 | MSE Train Loss: 74.1009521484375 | MSE Test Loss: 70.79576110839844\n",
      "Epoch: 98800 | MSE Train Loss: 74.0976791381836 | MSE Test Loss: 70.79183197021484\n",
      "Epoch: 98900 | MSE Train Loss: 74.09442138671875 | MSE Test Loss: 70.78792572021484\n",
      "Epoch: 99000 | MSE Train Loss: 74.0911865234375 | MSE Test Loss: 70.7840347290039\n",
      "Epoch: 99100 | MSE Train Loss: 74.08795928955078 | MSE Test Loss: 70.78013610839844\n",
      "Epoch: 99200 | MSE Train Loss: 74.08474731445312 | MSE Test Loss: 70.7762680053711\n",
      "Epoch: 99300 | MSE Train Loss: 74.08154296875 | MSE Test Loss: 70.77238464355469\n",
      "Epoch: 99400 | MSE Train Loss: 74.0783462524414 | MSE Test Loss: 70.76853942871094\n",
      "Epoch: 99500 | MSE Train Loss: 74.07516479492188 | MSE Test Loss: 70.76468658447266\n",
      "Epoch: 99600 | MSE Train Loss: 74.07199096679688 | MSE Test Loss: 70.76085662841797\n",
      "Epoch: 99700 | MSE Train Loss: 74.06884002685547 | MSE Test Loss: 70.75703430175781\n",
      "Epoch: 99800 | MSE Train Loss: 74.06568908691406 | MSE Test Loss: 70.75322723388672\n",
      "Epoch: 99900 | MSE Train Loss: 74.06254577636719 | MSE Test Loss: 70.74942779541016\n",
      "Epoch: 100000 | MSE Train Loss: 74.0594253540039 | MSE Test Loss: 70.74565887451172\n",
      "Epoch: 100100 | MSE Train Loss: 74.05631256103516 | MSE Test Loss: 70.74187469482422\n",
      "Epoch: 100200 | MSE Train Loss: 74.05320739746094 | MSE Test Loss: 70.73811340332031\n",
      "Epoch: 100300 | MSE Train Loss: 74.05010986328125 | MSE Test Loss: 70.73435974121094\n",
      "Epoch: 100400 | MSE Train Loss: 74.04702758789062 | MSE Test Loss: 70.7306137084961\n",
      "Epoch: 100500 | MSE Train Loss: 74.04400634765625 | MSE Test Loss: 70.72692108154297\n",
      "Epoch: 100600 | MSE Train Loss: 74.04117584228516 | MSE Test Loss: 70.72343444824219\n",
      "Epoch: 100700 | MSE Train Loss: 74.03836059570312 | MSE Test Loss: 70.719970703125\n",
      "Epoch: 100800 | MSE Train Loss: 74.03555297851562 | MSE Test Loss: 70.71652221679688\n",
      "Epoch: 100900 | MSE Train Loss: 74.03276062011719 | MSE Test Loss: 70.71309661865234\n",
      "Epoch: 101000 | MSE Train Loss: 74.02997589111328 | MSE Test Loss: 70.70968627929688\n",
      "Epoch: 101100 | MSE Train Loss: 74.02719116210938 | MSE Test Loss: 70.70628356933594\n",
      "Epoch: 101200 | MSE Train Loss: 74.02442169189453 | MSE Test Loss: 70.70288848876953\n",
      "Epoch: 101300 | MSE Train Loss: 74.02165985107422 | MSE Test Loss: 70.69951629638672\n",
      "Epoch: 101400 | MSE Train Loss: 74.0189208984375 | MSE Test Loss: 70.69613647460938\n",
      "Epoch: 101500 | MSE Train Loss: 74.01618194580078 | MSE Test Loss: 70.69279479980469\n",
      "Epoch: 101600 | MSE Train Loss: 74.0134506225586 | MSE Test Loss: 70.68946075439453\n",
      "Epoch: 101700 | MSE Train Loss: 74.01073455810547 | MSE Test Loss: 70.68612670898438\n",
      "Epoch: 101800 | MSE Train Loss: 74.00802612304688 | MSE Test Loss: 70.68280792236328\n",
      "Epoch: 101900 | MSE Train Loss: 74.00532531738281 | MSE Test Loss: 70.67948913574219\n",
      "Epoch: 102000 | MSE Train Loss: 74.00264739990234 | MSE Test Loss: 70.67617797851562\n",
      "Epoch: 102100 | MSE Train Loss: 73.99996185302734 | MSE Test Loss: 70.67289733886719\n",
      "Epoch: 102200 | MSE Train Loss: 73.99729919433594 | MSE Test Loss: 70.66960906982422\n",
      "Epoch: 102300 | MSE Train Loss: 73.99462890625 | MSE Test Loss: 70.66633605957031\n",
      "Epoch: 102400 | MSE Train Loss: 73.99198913574219 | MSE Test Loss: 70.66307830810547\n",
      "Epoch: 102500 | MSE Train Loss: 73.98933410644531 | MSE Test Loss: 70.65982818603516\n",
      "Epoch: 102600 | MSE Train Loss: 73.98670959472656 | MSE Test Loss: 70.65657806396484\n",
      "Epoch: 102700 | MSE Train Loss: 73.98408508300781 | MSE Test Loss: 70.65335083007812\n",
      "Epoch: 102800 | MSE Train Loss: 73.98147583007812 | MSE Test Loss: 70.65011596679688\n",
      "Epoch: 102900 | MSE Train Loss: 73.97901153564453 | MSE Test Loss: 70.64703369140625\n",
      "Epoch: 103000 | MSE Train Loss: 73.97662353515625 | MSE Test Loss: 70.64407348632812\n",
      "Epoch: 103100 | MSE Train Loss: 73.9742660522461 | MSE Test Loss: 70.64116668701172\n",
      "Epoch: 103200 | MSE Train Loss: 73.9719009399414 | MSE Test Loss: 70.63825988769531\n",
      "Epoch: 103300 | MSE Train Loss: 73.96955871582031 | MSE Test Loss: 70.6353530883789\n",
      "Epoch: 103400 | MSE Train Loss: 73.96720886230469 | MSE Test Loss: 70.63246154785156\n",
      "Epoch: 103500 | MSE Train Loss: 73.96488189697266 | MSE Test Loss: 70.62957763671875\n",
      "Epoch: 103600 | MSE Train Loss: 73.9625473022461 | MSE Test Loss: 70.62670135498047\n",
      "Epoch: 103700 | MSE Train Loss: 73.9602279663086 | MSE Test Loss: 70.62384033203125\n",
      "Epoch: 103800 | MSE Train Loss: 73.95792388916016 | MSE Test Loss: 70.62097930908203\n",
      "Epoch: 103900 | MSE Train Loss: 73.95561981201172 | MSE Test Loss: 70.61812591552734\n",
      "Epoch: 104000 | MSE Train Loss: 73.95332336425781 | MSE Test Loss: 70.61528015136719\n",
      "Epoch: 104100 | MSE Train Loss: 73.95104217529297 | MSE Test Loss: 70.6124496459961\n",
      "Epoch: 104200 | MSE Train Loss: 73.94876098632812 | MSE Test Loss: 70.60962677001953\n",
      "Epoch: 104300 | MSE Train Loss: 73.94650268554688 | MSE Test Loss: 70.60679626464844\n",
      "Epoch: 104400 | MSE Train Loss: 73.9442367553711 | MSE Test Loss: 70.60399627685547\n",
      "Epoch: 104500 | MSE Train Loss: 73.94198608398438 | MSE Test Loss: 70.60118865966797\n",
      "Epoch: 104600 | MSE Train Loss: 73.93972778320312 | MSE Test Loss: 70.59839630126953\n",
      "Epoch: 104700 | MSE Train Loss: 73.9375 | MSE Test Loss: 70.5956039428711\n",
      "Epoch: 104800 | MSE Train Loss: 73.93527221679688 | MSE Test Loss: 70.59283447265625\n",
      "Epoch: 104900 | MSE Train Loss: 73.93305206298828 | MSE Test Loss: 70.5900650024414\n",
      "Epoch: 105000 | MSE Train Loss: 73.93084716796875 | MSE Test Loss: 70.5873031616211\n",
      "Epoch: 105100 | MSE Train Loss: 73.92863464355469 | MSE Test Loss: 70.58454895019531\n",
      "Epoch: 105200 | MSE Train Loss: 73.92643737792969 | MSE Test Loss: 70.5818099975586\n",
      "Epoch: 105300 | MSE Train Loss: 73.92424774169922 | MSE Test Loss: 70.57907104492188\n",
      "Epoch: 105400 | MSE Train Loss: 73.92213439941406 | MSE Test Loss: 70.57638549804688\n",
      "Epoch: 105500 | MSE Train Loss: 73.92015838623047 | MSE Test Loss: 70.57386016845703\n",
      "Epoch: 105600 | MSE Train Loss: 73.91817474365234 | MSE Test Loss: 70.57136535644531\n",
      "Epoch: 105700 | MSE Train Loss: 73.91622161865234 | MSE Test Loss: 70.56888580322266\n",
      "Epoch: 105800 | MSE Train Loss: 73.91426086425781 | MSE Test Loss: 70.56640625\n",
      "Epoch: 105900 | MSE Train Loss: 73.91231536865234 | MSE Test Loss: 70.56396484375\n",
      "Epoch: 106000 | MSE Train Loss: 73.91036987304688 | MSE Test Loss: 70.56153869628906\n",
      "Epoch: 106100 | MSE Train Loss: 73.90843200683594 | MSE Test Loss: 70.55909729003906\n",
      "Epoch: 106200 | MSE Train Loss: 73.90650939941406 | MSE Test Loss: 70.55667877197266\n",
      "Epoch: 106300 | MSE Train Loss: 73.90458679199219 | MSE Test Loss: 70.55425262451172\n",
      "Epoch: 106400 | MSE Train Loss: 73.90267181396484 | MSE Test Loss: 70.55184936523438\n",
      "Epoch: 106500 | MSE Train Loss: 73.90076446533203 | MSE Test Loss: 70.54944610595703\n",
      "Epoch: 106600 | MSE Train Loss: 73.89886474609375 | MSE Test Loss: 70.54703521728516\n",
      "Epoch: 106700 | MSE Train Loss: 73.89696502685547 | MSE Test Loss: 70.54464721679688\n",
      "Epoch: 106800 | MSE Train Loss: 73.89508819580078 | MSE Test Loss: 70.5422592163086\n",
      "Epoch: 106900 | MSE Train Loss: 73.89320373535156 | MSE Test Loss: 70.53988647460938\n",
      "Epoch: 107000 | MSE Train Loss: 73.89132690429688 | MSE Test Loss: 70.53751373291016\n",
      "Epoch: 107100 | MSE Train Loss: 73.88945007324219 | MSE Test Loss: 70.53514862060547\n",
      "Epoch: 107200 | MSE Train Loss: 73.88760375976562 | MSE Test Loss: 70.53279113769531\n",
      "Epoch: 107300 | MSE Train Loss: 73.88574981689453 | MSE Test Loss: 70.53043365478516\n",
      "Epoch: 107400 | MSE Train Loss: 73.88390350341797 | MSE Test Loss: 70.52809143066406\n",
      "Epoch: 107500 | MSE Train Loss: 73.88206481933594 | MSE Test Loss: 70.52574920654297\n",
      "Epoch: 107600 | MSE Train Loss: 73.8802261352539 | MSE Test Loss: 70.5234146118164\n",
      "Epoch: 107700 | MSE Train Loss: 73.8783950805664 | MSE Test Loss: 70.52108764648438\n",
      "Epoch: 107800 | MSE Train Loss: 73.87657165527344 | MSE Test Loss: 70.51878356933594\n",
      "Epoch: 107900 | MSE Train Loss: 73.87476348876953 | MSE Test Loss: 70.51647186279297\n",
      "Epoch: 108000 | MSE Train Loss: 73.8729476928711 | MSE Test Loss: 70.51416778564453\n",
      "Epoch: 108100 | MSE Train Loss: 73.87114715576172 | MSE Test Loss: 70.51187133789062\n",
      "Epoch: 108200 | MSE Train Loss: 73.86941528320312 | MSE Test Loss: 70.50962829589844\n",
      "Epoch: 108300 | MSE Train Loss: 73.8677978515625 | MSE Test Loss: 70.50753021240234\n",
      "Epoch: 108400 | MSE Train Loss: 73.86620330810547 | MSE Test Loss: 70.50546264648438\n",
      "Epoch: 108500 | MSE Train Loss: 73.86459350585938 | MSE Test Loss: 70.50340270996094\n",
      "Epoch: 108600 | MSE Train Loss: 73.86300659179688 | MSE Test Loss: 70.50135040283203\n",
      "Epoch: 108700 | MSE Train Loss: 73.86141967773438 | MSE Test Loss: 70.4992904663086\n",
      "Epoch: 108800 | MSE Train Loss: 73.8598403930664 | MSE Test Loss: 70.49726104736328\n",
      "Epoch: 108900 | MSE Train Loss: 73.85826110839844 | MSE Test Loss: 70.49520874023438\n",
      "Epoch: 109000 | MSE Train Loss: 73.85668182373047 | MSE Test Loss: 70.4931869506836\n",
      "Epoch: 109100 | MSE Train Loss: 73.8551254272461 | MSE Test Loss: 70.49116516113281\n",
      "Epoch: 109200 | MSE Train Loss: 73.85356140136719 | MSE Test Loss: 70.4891586303711\n",
      "Epoch: 109300 | MSE Train Loss: 73.85199737548828 | MSE Test Loss: 70.48714447021484\n",
      "Epoch: 109400 | MSE Train Loss: 73.8504638671875 | MSE Test Loss: 70.4851303100586\n",
      "Epoch: 109500 | MSE Train Loss: 73.84891510009766 | MSE Test Loss: 70.48313903808594\n",
      "Epoch: 109600 | MSE Train Loss: 73.84737396240234 | MSE Test Loss: 70.48114013671875\n",
      "Epoch: 109700 | MSE Train Loss: 73.84584045410156 | MSE Test Loss: 70.47914123535156\n",
      "Epoch: 109800 | MSE Train Loss: 73.84431457519531 | MSE Test Loss: 70.47715759277344\n",
      "Epoch: 109900 | MSE Train Loss: 73.84278869628906 | MSE Test Loss: 70.47518920898438\n",
      "Epoch: 110000 | MSE Train Loss: 73.84127807617188 | MSE Test Loss: 70.47320556640625\n",
      "Epoch: 110100 | MSE Train Loss: 73.83975982666016 | MSE Test Loss: 70.47123718261719\n",
      "Epoch: 110200 | MSE Train Loss: 73.83824920654297 | MSE Test Loss: 70.46927642822266\n",
      "Epoch: 110300 | MSE Train Loss: 73.83674621582031 | MSE Test Loss: 70.46731567382812\n",
      "Epoch: 110400 | MSE Train Loss: 73.83526611328125 | MSE Test Loss: 70.46536254882812\n",
      "Epoch: 110500 | MSE Train Loss: 73.83377075195312 | MSE Test Loss: 70.46340942382812\n",
      "Epoch: 110600 | MSE Train Loss: 73.83228302001953 | MSE Test Loss: 70.46147155761719\n",
      "Epoch: 110700 | MSE Train Loss: 73.83080291748047 | MSE Test Loss: 70.45953369140625\n",
      "Epoch: 110800 | MSE Train Loss: 73.8293228149414 | MSE Test Loss: 70.45761108398438\n",
      "Epoch: 110900 | MSE Train Loss: 73.82786560058594 | MSE Test Loss: 70.45569610595703\n",
      "Epoch: 111000 | MSE Train Loss: 73.8263931274414 | MSE Test Loss: 70.45376586914062\n",
      "Epoch: 111100 | MSE Train Loss: 73.82494354248047 | MSE Test Loss: 70.45186614990234\n",
      "Epoch: 111200 | MSE Train Loss: 73.823486328125 | MSE Test Loss: 70.449951171875\n",
      "Epoch: 111300 | MSE Train Loss: 73.82203674316406 | MSE Test Loss: 70.44805908203125\n",
      "Epoch: 111400 | MSE Train Loss: 73.82073974609375 | MSE Test Loss: 70.44629669189453\n",
      "Epoch: 111500 | MSE Train Loss: 73.81946563720703 | MSE Test Loss: 70.4446029663086\n",
      "Epoch: 111600 | MSE Train Loss: 73.81818389892578 | MSE Test Loss: 70.44290924072266\n",
      "Epoch: 111700 | MSE Train Loss: 73.8169174194336 | MSE Test Loss: 70.44123840332031\n",
      "Epoch: 111800 | MSE Train Loss: 73.81566619873047 | MSE Test Loss: 70.43956756591797\n",
      "Epoch: 111900 | MSE Train Loss: 73.81439971923828 | MSE Test Loss: 70.43793487548828\n",
      "Epoch: 112000 | MSE Train Loss: 73.81314086914062 | MSE Test Loss: 70.43627166748047\n",
      "Epoch: 112100 | MSE Train Loss: 73.8118896484375 | MSE Test Loss: 70.43463897705078\n",
      "Epoch: 112200 | MSE Train Loss: 73.81065368652344 | MSE Test Loss: 70.43299865722656\n",
      "Epoch: 112300 | MSE Train Loss: 73.80941009521484 | MSE Test Loss: 70.43136596679688\n",
      "Epoch: 112400 | MSE Train Loss: 73.80816650390625 | MSE Test Loss: 70.42973327636719\n",
      "Epoch: 112500 | MSE Train Loss: 73.80693817138672 | MSE Test Loss: 70.42811584472656\n",
      "Epoch: 112600 | MSE Train Loss: 73.80570983886719 | MSE Test Loss: 70.4264907836914\n",
      "Epoch: 112700 | MSE Train Loss: 73.80448913574219 | MSE Test Loss: 70.42488098144531\n",
      "Epoch: 112800 | MSE Train Loss: 73.80326080322266 | MSE Test Loss: 70.42327117919922\n",
      "Epoch: 112900 | MSE Train Loss: 73.80204772949219 | MSE Test Loss: 70.42166137695312\n",
      "Epoch: 113000 | MSE Train Loss: 73.80083465576172 | MSE Test Loss: 70.42005920410156\n",
      "Epoch: 113100 | MSE Train Loss: 73.79964447021484 | MSE Test Loss: 70.41845703125\n",
      "Epoch: 113200 | MSE Train Loss: 73.79843139648438 | MSE Test Loss: 70.41686248779297\n",
      "Epoch: 113300 | MSE Train Loss: 73.7972412109375 | MSE Test Loss: 70.41527557373047\n",
      "Epoch: 113400 | MSE Train Loss: 73.79603576660156 | MSE Test Loss: 70.4136962890625\n",
      "Epoch: 113500 | MSE Train Loss: 73.79485321044922 | MSE Test Loss: 70.412109375\n",
      "Epoch: 113600 | MSE Train Loss: 73.79366302490234 | MSE Test Loss: 70.41053771972656\n",
      "Epoch: 113700 | MSE Train Loss: 73.79248809814453 | MSE Test Loss: 70.40896606445312\n",
      "Epoch: 113800 | MSE Train Loss: 73.79130554199219 | MSE Test Loss: 70.40740203857422\n",
      "Epoch: 113900 | MSE Train Loss: 73.79014587402344 | MSE Test Loss: 70.40583801269531\n",
      "Epoch: 114000 | MSE Train Loss: 73.78897857666016 | MSE Test Loss: 70.40426635742188\n",
      "Epoch: 114100 | MSE Train Loss: 73.78780364990234 | MSE Test Loss: 70.40271759033203\n",
      "Epoch: 114200 | MSE Train Loss: 73.7866439819336 | MSE Test Loss: 70.40116882324219\n",
      "Epoch: 114300 | MSE Train Loss: 73.78548431396484 | MSE Test Loss: 70.39962768554688\n",
      "Epoch: 114400 | MSE Train Loss: 73.78433990478516 | MSE Test Loss: 70.39807891845703\n",
      "Epoch: 114500 | MSE Train Loss: 73.78318786621094 | MSE Test Loss: 70.39654541015625\n",
      "Epoch: 114600 | MSE Train Loss: 73.78205108642578 | MSE Test Loss: 70.39501953125\n",
      "Epoch: 114700 | MSE Train Loss: 73.78092193603516 | MSE Test Loss: 70.39347839355469\n",
      "Epoch: 114800 | MSE Train Loss: 73.77977752685547 | MSE Test Loss: 70.39196014404297\n",
      "Epoch: 114900 | MSE Train Loss: 73.77880096435547 | MSE Test Loss: 70.39060974121094\n",
      "Epoch: 115000 | MSE Train Loss: 73.7778091430664 | MSE Test Loss: 70.3892593383789\n",
      "Epoch: 115100 | MSE Train Loss: 73.77684020996094 | MSE Test Loss: 70.38792419433594\n",
      "Epoch: 115200 | MSE Train Loss: 73.77586364746094 | MSE Test Loss: 70.38658905029297\n",
      "Epoch: 115300 | MSE Train Loss: 73.77487182617188 | MSE Test Loss: 70.38526916503906\n",
      "Epoch: 115400 | MSE Train Loss: 73.7739028930664 | MSE Test Loss: 70.38395690917969\n",
      "Epoch: 115500 | MSE Train Loss: 73.77293395996094 | MSE Test Loss: 70.38262939453125\n",
      "Epoch: 115600 | MSE Train Loss: 73.77196502685547 | MSE Test Loss: 70.38131713867188\n",
      "Epoch: 115700 | MSE Train Loss: 73.77101135253906 | MSE Test Loss: 70.38002014160156\n",
      "Epoch: 115800 | MSE Train Loss: 73.77005004882812 | MSE Test Loss: 70.37871551513672\n",
      "Epoch: 115900 | MSE Train Loss: 73.76909637451172 | MSE Test Loss: 70.37741088867188\n",
      "Epoch: 116000 | MSE Train Loss: 73.76815032958984 | MSE Test Loss: 70.3760986328125\n",
      "Epoch: 116100 | MSE Train Loss: 73.76719665527344 | MSE Test Loss: 70.37480926513672\n",
      "Epoch: 116200 | MSE Train Loss: 73.7662582397461 | MSE Test Loss: 70.37352752685547\n",
      "Epoch: 116300 | MSE Train Loss: 73.76529693603516 | MSE Test Loss: 70.37223815917969\n",
      "Epoch: 116400 | MSE Train Loss: 73.76436614990234 | MSE Test Loss: 70.3709487915039\n",
      "Epoch: 116500 | MSE Train Loss: 73.763427734375 | MSE Test Loss: 70.36965942382812\n",
      "Epoch: 116600 | MSE Train Loss: 73.76249694824219 | MSE Test Loss: 70.3683853149414\n",
      "Epoch: 116700 | MSE Train Loss: 73.7615737915039 | MSE Test Loss: 70.36710357666016\n",
      "Epoch: 116800 | MSE Train Loss: 73.76065063476562 | MSE Test Loss: 70.36582946777344\n",
      "Epoch: 116900 | MSE Train Loss: 73.75972747802734 | MSE Test Loss: 70.36457824707031\n",
      "Epoch: 117000 | MSE Train Loss: 73.75880432128906 | MSE Test Loss: 70.36331176757812\n",
      "Epoch: 117100 | MSE Train Loss: 73.75788879394531 | MSE Test Loss: 70.3620376586914\n",
      "Epoch: 117200 | MSE Train Loss: 73.75697326660156 | MSE Test Loss: 70.36079406738281\n",
      "Epoch: 117300 | MSE Train Loss: 73.75606536865234 | MSE Test Loss: 70.35955047607422\n",
      "Epoch: 117400 | MSE Train Loss: 73.75515747070312 | MSE Test Loss: 70.3582992553711\n",
      "Epoch: 117500 | MSE Train Loss: 73.75426483154297 | MSE Test Loss: 70.3570556640625\n",
      "Epoch: 117600 | MSE Train Loss: 73.75335693359375 | MSE Test Loss: 70.35581970214844\n",
      "Epoch: 117700 | MSE Train Loss: 73.7524642944336 | MSE Test Loss: 70.35458374023438\n",
      "Epoch: 117800 | MSE Train Loss: 73.7515640258789 | MSE Test Loss: 70.35334777832031\n",
      "Epoch: 117900 | MSE Train Loss: 73.75067901611328 | MSE Test Loss: 70.35211181640625\n",
      "Epoch: 118000 | MSE Train Loss: 73.74979400634766 | MSE Test Loss: 70.35089111328125\n",
      "Epoch: 118100 | MSE Train Loss: 73.74890899658203 | MSE Test Loss: 70.34967041015625\n",
      "Epoch: 118200 | MSE Train Loss: 73.7480239868164 | MSE Test Loss: 70.34844970703125\n",
      "Epoch: 118300 | MSE Train Loss: 73.74715423583984 | MSE Test Loss: 70.34723663330078\n",
      "Epoch: 118400 | MSE Train Loss: 73.74628448486328 | MSE Test Loss: 70.34601593017578\n",
      "Epoch: 118500 | MSE Train Loss: 73.74541473388672 | MSE Test Loss: 70.34481048583984\n",
      "Epoch: 118600 | MSE Train Loss: 73.74454498291016 | MSE Test Loss: 70.34359741210938\n",
      "Epoch: 118700 | MSE Train Loss: 73.74368286132812 | MSE Test Loss: 70.34239959716797\n",
      "Epoch: 118800 | MSE Train Loss: 73.74283599853516 | MSE Test Loss: 70.3412094116211\n",
      "Epoch: 118900 | MSE Train Loss: 73.74210357666016 | MSE Test Loss: 70.34017181396484\n",
      "Epoch: 119000 | MSE Train Loss: 73.74137115478516 | MSE Test Loss: 70.33912658691406\n",
      "Epoch: 119100 | MSE Train Loss: 73.74063873291016 | MSE Test Loss: 70.33809661865234\n",
      "Epoch: 119200 | MSE Train Loss: 73.73991394042969 | MSE Test Loss: 70.3370590209961\n",
      "Epoch: 119300 | MSE Train Loss: 73.73918151855469 | MSE Test Loss: 70.33604431152344\n",
      "Epoch: 119400 | MSE Train Loss: 73.73844909667969 | MSE Test Loss: 70.33501434326172\n",
      "Epoch: 119500 | MSE Train Loss: 73.73773956298828 | MSE Test Loss: 70.33399963378906\n",
      "Epoch: 119600 | MSE Train Loss: 73.73702239990234 | MSE Test Loss: 70.3329849243164\n",
      "Epoch: 119700 | MSE Train Loss: 73.73629760742188 | MSE Test Loss: 70.33197784423828\n",
      "Epoch: 119800 | MSE Train Loss: 73.735595703125 | MSE Test Loss: 70.3309555053711\n",
      "Epoch: 119900 | MSE Train Loss: 73.7348861694336 | MSE Test Loss: 70.32994842529297\n",
      "Epoch: 120000 | MSE Train Loss: 73.73417663574219 | MSE Test Loss: 70.32894134521484\n",
      "Epoch: 120100 | MSE Train Loss: 73.73345947265625 | MSE Test Loss: 70.32793426513672\n",
      "Epoch: 120200 | MSE Train Loss: 73.73274993896484 | MSE Test Loss: 70.32694244384766\n",
      "Epoch: 120300 | MSE Train Loss: 73.73206329345703 | MSE Test Loss: 70.32594299316406\n",
      "Epoch: 120400 | MSE Train Loss: 73.73136138916016 | MSE Test Loss: 70.324951171875\n",
      "Epoch: 120500 | MSE Train Loss: 73.73065948486328 | MSE Test Loss: 70.3239517211914\n",
      "Epoch: 120600 | MSE Train Loss: 73.72996520996094 | MSE Test Loss: 70.32295989990234\n",
      "Epoch: 120700 | MSE Train Loss: 73.72927856445312 | MSE Test Loss: 70.32196807861328\n",
      "Epoch: 120800 | MSE Train Loss: 73.72859191894531 | MSE Test Loss: 70.32098388671875\n",
      "Epoch: 120900 | MSE Train Loss: 73.72789764404297 | MSE Test Loss: 70.31999969482422\n",
      "Epoch: 121000 | MSE Train Loss: 73.72721862792969 | MSE Test Loss: 70.31902313232422\n",
      "Epoch: 121100 | MSE Train Loss: 73.7265396118164 | MSE Test Loss: 70.31803894042969\n",
      "Epoch: 121200 | MSE Train Loss: 73.72586059570312 | MSE Test Loss: 70.31707000732422\n",
      "Epoch: 121300 | MSE Train Loss: 73.72518157958984 | MSE Test Loss: 70.31608581542969\n",
      "Epoch: 121400 | MSE Train Loss: 73.72450256347656 | MSE Test Loss: 70.31510925292969\n",
      "Epoch: 121500 | MSE Train Loss: 73.72383880615234 | MSE Test Loss: 70.31414794921875\n",
      "Epoch: 121600 | MSE Train Loss: 73.7231674194336 | MSE Test Loss: 70.31318664550781\n",
      "Epoch: 121700 | MSE Train Loss: 73.72250366210938 | MSE Test Loss: 70.31221771240234\n",
      "Epoch: 121800 | MSE Train Loss: 73.72183227539062 | MSE Test Loss: 70.31126403808594\n",
      "Epoch: 121900 | MSE Train Loss: 73.7211685180664 | MSE Test Loss: 70.310302734375\n",
      "Epoch: 122000 | MSE Train Loss: 73.72051239013672 | MSE Test Loss: 70.30934143066406\n",
      "Epoch: 122100 | MSE Train Loss: 73.7198486328125 | MSE Test Loss: 70.30839538574219\n",
      "Epoch: 122200 | MSE Train Loss: 73.71920013427734 | MSE Test Loss: 70.30743408203125\n",
      "Epoch: 122300 | MSE Train Loss: 73.71855163574219 | MSE Test Loss: 70.3064956665039\n",
      "Epoch: 122400 | MSE Train Loss: 73.71788787841797 | MSE Test Loss: 70.30554962158203\n",
      "Epoch: 122500 | MSE Train Loss: 73.71724700927734 | MSE Test Loss: 70.30460357666016\n",
      "Epoch: 122600 | MSE Train Loss: 73.71659851074219 | MSE Test Loss: 70.30364990234375\n",
      "Epoch: 122700 | MSE Train Loss: 73.71595764160156 | MSE Test Loss: 70.30271911621094\n",
      "Epoch: 122800 | MSE Train Loss: 73.71532440185547 | MSE Test Loss: 70.30177307128906\n",
      "Epoch: 122900 | MSE Train Loss: 73.71469116210938 | MSE Test Loss: 70.30083465576172\n",
      "Epoch: 123000 | MSE Train Loss: 73.71405029296875 | MSE Test Loss: 70.29991912841797\n",
      "Epoch: 123100 | MSE Train Loss: 73.71340942382812 | MSE Test Loss: 70.29898834228516\n",
      "Epoch: 123200 | MSE Train Loss: 73.71278381347656 | MSE Test Loss: 70.29806518554688\n",
      "Epoch: 123300 | MSE Train Loss: 73.71215057373047 | MSE Test Loss: 70.29713439941406\n",
      "Epoch: 123400 | MSE Train Loss: 73.7115249633789 | MSE Test Loss: 70.29621887207031\n",
      "Epoch: 123500 | MSE Train Loss: 73.7109375 | MSE Test Loss: 70.2953109741211\n",
      "Epoch: 123600 | MSE Train Loss: 73.71041107177734 | MSE Test Loss: 70.29450988769531\n",
      "Epoch: 123700 | MSE Train Loss: 73.70989990234375 | MSE Test Loss: 70.29370880126953\n",
      "Epoch: 123800 | MSE Train Loss: 73.70938110351562 | MSE Test Loss: 70.29293060302734\n",
      "Epoch: 123900 | MSE Train Loss: 73.70886993408203 | MSE Test Loss: 70.29216003417969\n",
      "Epoch: 124000 | MSE Train Loss: 73.7083511352539 | MSE Test Loss: 70.2913818359375\n",
      "Epoch: 124100 | MSE Train Loss: 73.70783996582031 | MSE Test Loss: 70.29060363769531\n",
      "Epoch: 124200 | MSE Train Loss: 73.70733642578125 | MSE Test Loss: 70.28984069824219\n",
      "Epoch: 124300 | MSE Train Loss: 73.70682525634766 | MSE Test Loss: 70.28907775878906\n",
      "Epoch: 124400 | MSE Train Loss: 73.7063217163086 | MSE Test Loss: 70.28829193115234\n",
      "Epoch: 124500 | MSE Train Loss: 73.70581817626953 | MSE Test Loss: 70.28752899169922\n",
      "Epoch: 124600 | MSE Train Loss: 73.70531463623047 | MSE Test Loss: 70.2867660522461\n",
      "Epoch: 124700 | MSE Train Loss: 73.7048110961914 | MSE Test Loss: 70.28600311279297\n",
      "Epoch: 124800 | MSE Train Loss: 73.70431518554688 | MSE Test Loss: 70.28526306152344\n",
      "Epoch: 124900 | MSE Train Loss: 73.70382690429688 | MSE Test Loss: 70.2845230102539\n",
      "Epoch: 125000 | MSE Train Loss: 73.70332336425781 | MSE Test Loss: 70.2837905883789\n",
      "Epoch: 125100 | MSE Train Loss: 73.70283508300781 | MSE Test Loss: 70.28303527832031\n",
      "Epoch: 125200 | MSE Train Loss: 73.70234680175781 | MSE Test Loss: 70.28230285644531\n",
      "Epoch: 125300 | MSE Train Loss: 73.70185089111328 | MSE Test Loss: 70.28157043457031\n",
      "Epoch: 125400 | MSE Train Loss: 73.70137023925781 | MSE Test Loss: 70.28083801269531\n",
      "Epoch: 125500 | MSE Train Loss: 73.70088195800781 | MSE Test Loss: 70.28011322021484\n",
      "Epoch: 125600 | MSE Train Loss: 73.70040130615234 | MSE Test Loss: 70.27938842773438\n",
      "Epoch: 125700 | MSE Train Loss: 73.69991302490234 | MSE Test Loss: 70.2786636352539\n",
      "Epoch: 125800 | MSE Train Loss: 73.6994400024414 | MSE Test Loss: 70.27794647216797\n",
      "Epoch: 125900 | MSE Train Loss: 73.69895935058594 | MSE Test Loss: 70.2772216796875\n",
      "Epoch: 126000 | MSE Train Loss: 73.69847869873047 | MSE Test Loss: 70.2765121459961\n",
      "Epoch: 126100 | MSE Train Loss: 73.697998046875 | MSE Test Loss: 70.27578735351562\n",
      "Epoch: 126200 | MSE Train Loss: 73.6975326538086 | MSE Test Loss: 70.27507781982422\n",
      "Epoch: 126300 | MSE Train Loss: 73.69705963134766 | MSE Test Loss: 70.27436828613281\n",
      "Epoch: 126400 | MSE Train Loss: 73.69658660888672 | MSE Test Loss: 70.27365112304688\n",
      "Epoch: 126500 | MSE Train Loss: 73.69612121582031 | MSE Test Loss: 70.27294921875\n",
      "Epoch: 126600 | MSE Train Loss: 73.6956558227539 | MSE Test Loss: 70.27223205566406\n",
      "Epoch: 126700 | MSE Train Loss: 73.6951904296875 | MSE Test Loss: 70.27153778076172\n",
      "Epoch: 126800 | MSE Train Loss: 73.6947250366211 | MSE Test Loss: 70.27082824707031\n",
      "Epoch: 126900 | MSE Train Loss: 73.69425964355469 | MSE Test Loss: 70.27013397216797\n",
      "Epoch: 127000 | MSE Train Loss: 73.69380187988281 | MSE Test Loss: 70.26942443847656\n",
      "Epoch: 127100 | MSE Train Loss: 73.6933364868164 | MSE Test Loss: 70.26873016357422\n",
      "Epoch: 127200 | MSE Train Loss: 73.69288635253906 | MSE Test Loss: 70.26802825927734\n",
      "Epoch: 127300 | MSE Train Loss: 73.69242858886719 | MSE Test Loss: 70.26734161376953\n",
      "Epoch: 127400 | MSE Train Loss: 73.69197845458984 | MSE Test Loss: 70.26663970947266\n",
      "Epoch: 127500 | MSE Train Loss: 73.6915283203125 | MSE Test Loss: 70.26594543457031\n",
      "Epoch: 127600 | MSE Train Loss: 73.69107818603516 | MSE Test Loss: 70.2652587890625\n",
      "Epoch: 127700 | MSE Train Loss: 73.69062805175781 | MSE Test Loss: 70.26457214355469\n",
      "Epoch: 127800 | MSE Train Loss: 73.69017791748047 | MSE Test Loss: 70.26387786865234\n",
      "Epoch: 127900 | MSE Train Loss: 73.68973541259766 | MSE Test Loss: 70.26319122314453\n",
      "Epoch: 128000 | MSE Train Loss: 73.68928527832031 | MSE Test Loss: 70.26251220703125\n",
      "Epoch: 128100 | MSE Train Loss: 73.6888427734375 | MSE Test Loss: 70.26182556152344\n",
      "Epoch: 128200 | MSE Train Loss: 73.68841552734375 | MSE Test Loss: 70.26114654541016\n",
      "Epoch: 128300 | MSE Train Loss: 73.6879653930664 | MSE Test Loss: 70.26045989990234\n",
      "Epoch: 128400 | MSE Train Loss: 73.68753051757812 | MSE Test Loss: 70.25979614257812\n",
      "Epoch: 128500 | MSE Train Loss: 73.68710327148438 | MSE Test Loss: 70.25910949707031\n",
      "Epoch: 128600 | MSE Train Loss: 73.6866683959961 | MSE Test Loss: 70.2584457397461\n",
      "Epoch: 128700 | MSE Train Loss: 73.68622589111328 | MSE Test Loss: 70.25776672363281\n",
      "Epoch: 128800 | MSE Train Loss: 73.68579864501953 | MSE Test Loss: 70.2571029663086\n",
      "Epoch: 128900 | MSE Train Loss: 73.68537139892578 | MSE Test Loss: 70.25643920898438\n",
      "Epoch: 129000 | MSE Train Loss: 73.68494415283203 | MSE Test Loss: 70.2557601928711\n",
      "Epoch: 129100 | MSE Train Loss: 73.68452453613281 | MSE Test Loss: 70.2551040649414\n",
      "Epoch: 129200 | MSE Train Loss: 73.6841812133789 | MSE Test Loss: 70.25456237792969\n",
      "Epoch: 129300 | MSE Train Loss: 73.68384552001953 | MSE Test Loss: 70.2540283203125\n",
      "Epoch: 129400 | MSE Train Loss: 73.68350982666016 | MSE Test Loss: 70.25350189208984\n",
      "Epoch: 129500 | MSE Train Loss: 73.68315887451172 | MSE Test Loss: 70.25296783447266\n",
      "Epoch: 129600 | MSE Train Loss: 73.6828384399414 | MSE Test Loss: 70.25244140625\n",
      "Epoch: 129700 | MSE Train Loss: 73.68250274658203 | MSE Test Loss: 70.25191497802734\n",
      "Epoch: 129800 | MSE Train Loss: 73.68217468261719 | MSE Test Loss: 70.25139617919922\n",
      "Epoch: 129900 | MSE Train Loss: 73.68183898925781 | MSE Test Loss: 70.25086975097656\n",
      "Epoch: 130000 | MSE Train Loss: 73.68151092529297 | MSE Test Loss: 70.25035095214844\n",
      "Epoch: 130100 | MSE Train Loss: 73.68118286132812 | MSE Test Loss: 70.24981689453125\n",
      "Epoch: 130200 | MSE Train Loss: 73.68084716796875 | MSE Test Loss: 70.24930572509766\n",
      "Epoch: 130300 | MSE Train Loss: 73.68052673339844 | MSE Test Loss: 70.24878692626953\n",
      "Epoch: 130400 | MSE Train Loss: 73.6801986694336 | MSE Test Loss: 70.2482681274414\n",
      "Epoch: 130500 | MSE Train Loss: 73.67987823486328 | MSE Test Loss: 70.24774932861328\n",
      "Epoch: 130600 | MSE Train Loss: 73.67955017089844 | MSE Test Loss: 70.24723052978516\n",
      "Epoch: 130700 | MSE Train Loss: 73.67922973632812 | MSE Test Loss: 70.2467269897461\n",
      "Epoch: 130800 | MSE Train Loss: 73.67890930175781 | MSE Test Loss: 70.2462158203125\n",
      "Epoch: 130900 | MSE Train Loss: 73.6785888671875 | MSE Test Loss: 70.24569702148438\n",
      "Epoch: 131000 | MSE Train Loss: 73.67826080322266 | MSE Test Loss: 70.24517822265625\n",
      "Epoch: 131100 | MSE Train Loss: 73.67794036865234 | MSE Test Loss: 70.24467468261719\n",
      "Epoch: 131200 | MSE Train Loss: 73.67762756347656 | MSE Test Loss: 70.24417114257812\n",
      "Epoch: 131300 | MSE Train Loss: 73.67731475830078 | MSE Test Loss: 70.24365997314453\n",
      "Epoch: 131400 | MSE Train Loss: 73.67699432373047 | MSE Test Loss: 70.24314880371094\n",
      "Epoch: 131500 | MSE Train Loss: 73.67668151855469 | MSE Test Loss: 70.24264526367188\n",
      "Epoch: 131600 | MSE Train Loss: 73.67637634277344 | MSE Test Loss: 70.24213409423828\n",
      "Epoch: 131700 | MSE Train Loss: 73.67606353759766 | MSE Test Loss: 70.24163055419922\n",
      "Epoch: 131800 | MSE Train Loss: 73.67575073242188 | MSE Test Loss: 70.24113464355469\n",
      "Epoch: 131900 | MSE Train Loss: 73.6754379272461 | MSE Test Loss: 70.24063110351562\n",
      "Epoch: 132000 | MSE Train Loss: 73.67513275146484 | MSE Test Loss: 70.24012756347656\n",
      "Epoch: 132100 | MSE Train Loss: 73.6748275756836 | MSE Test Loss: 70.23963165283203\n",
      "Epoch: 132200 | MSE Train Loss: 73.67451477050781 | MSE Test Loss: 70.23912811279297\n",
      "Epoch: 132300 | MSE Train Loss: 73.67420196533203 | MSE Test Loss: 70.23863983154297\n",
      "Epoch: 132400 | MSE Train Loss: 73.67390441894531 | MSE Test Loss: 70.2381362915039\n",
      "Epoch: 132500 | MSE Train Loss: 73.67359924316406 | MSE Test Loss: 70.2376480102539\n",
      "Epoch: 132600 | MSE Train Loss: 73.67329406738281 | MSE Test Loss: 70.23715209960938\n",
      "Epoch: 132700 | MSE Train Loss: 73.6729965209961 | MSE Test Loss: 70.23666381835938\n",
      "Epoch: 132800 | MSE Train Loss: 73.67269897460938 | MSE Test Loss: 70.23617553710938\n",
      "Epoch: 132900 | MSE Train Loss: 73.67240142822266 | MSE Test Loss: 70.23567962646484\n",
      "Epoch: 133000 | MSE Train Loss: 73.6720962524414 | MSE Test Loss: 70.23518371582031\n",
      "Epoch: 133100 | MSE Train Loss: 73.67179870605469 | MSE Test Loss: 70.23470306396484\n",
      "Epoch: 133200 | MSE Train Loss: 73.6715087890625 | MSE Test Loss: 70.23421478271484\n",
      "Epoch: 133300 | MSE Train Loss: 73.67121124267578 | MSE Test Loss: 70.23372650146484\n",
      "Epoch: 133400 | MSE Train Loss: 73.67091369628906 | MSE Test Loss: 70.23323822021484\n",
      "Epoch: 133500 | MSE Train Loss: 73.6706314086914 | MSE Test Loss: 70.2327651977539\n",
      "Epoch: 133600 | MSE Train Loss: 73.67033386230469 | MSE Test Loss: 70.2322769165039\n",
      "Epoch: 133700 | MSE Train Loss: 73.67005157470703 | MSE Test Loss: 70.23179626464844\n",
      "Epoch: 133800 | MSE Train Loss: 73.66974639892578 | MSE Test Loss: 70.23131561279297\n",
      "Epoch: 133900 | MSE Train Loss: 73.66947174072266 | MSE Test Loss: 70.2308349609375\n",
      "Epoch: 134000 | MSE Train Loss: 73.66917419433594 | MSE Test Loss: 70.23035430908203\n",
      "Epoch: 134100 | MSE Train Loss: 73.66889953613281 | MSE Test Loss: 70.2298812866211\n",
      "Epoch: 134200 | MSE Train Loss: 73.66860961914062 | MSE Test Loss: 70.22940826416016\n",
      "Epoch: 134300 | MSE Train Loss: 73.6683349609375 | MSE Test Loss: 70.22892761230469\n",
      "Epoch: 134400 | MSE Train Loss: 73.66805267333984 | MSE Test Loss: 70.22844696044922\n",
      "Epoch: 134500 | MSE Train Loss: 73.66776275634766 | MSE Test Loss: 70.22797393798828\n",
      "Epoch: 134600 | MSE Train Loss: 73.66748809814453 | MSE Test Loss: 70.22750854492188\n",
      "Epoch: 134700 | MSE Train Loss: 73.66719818115234 | MSE Test Loss: 70.22704315185547\n",
      "Epoch: 134800 | MSE Train Loss: 73.66691589355469 | MSE Test Loss: 70.2265625\n",
      "Epoch: 134900 | MSE Train Loss: 73.6666488647461 | MSE Test Loss: 70.2260971069336\n",
      "Epoch: 135000 | MSE Train Loss: 73.66636657714844 | MSE Test Loss: 70.22563934326172\n",
      "Epoch: 135100 | MSE Train Loss: 73.66608428955078 | MSE Test Loss: 70.22515869140625\n",
      "Epoch: 135200 | MSE Train Loss: 73.66581726074219 | MSE Test Loss: 70.22469329833984\n",
      "Epoch: 135300 | MSE Train Loss: 73.6655502319336 | MSE Test Loss: 70.2242431640625\n",
      "Epoch: 135400 | MSE Train Loss: 73.66527557373047 | MSE Test Loss: 70.2237777709961\n",
      "Epoch: 135500 | MSE Train Loss: 73.66500091552734 | MSE Test Loss: 70.22330474853516\n",
      "Epoch: 135600 | MSE Train Loss: 73.66472625732422 | MSE Test Loss: 70.22284698486328\n",
      "Epoch: 135700 | MSE Train Loss: 73.66445922851562 | MSE Test Loss: 70.2223892211914\n",
      "Epoch: 135800 | MSE Train Loss: 73.66419982910156 | MSE Test Loss: 70.22193908691406\n",
      "Epoch: 135900 | MSE Train Loss: 73.66392517089844 | MSE Test Loss: 70.22147369384766\n",
      "Epoch: 136000 | MSE Train Loss: 73.66366577148438 | MSE Test Loss: 70.22101593017578\n",
      "Epoch: 136100 | MSE Train Loss: 73.66343688964844 | MSE Test Loss: 70.22059631347656\n",
      "Epoch: 136200 | MSE Train Loss: 73.66323852539062 | MSE Test Loss: 70.22022247314453\n",
      "Epoch: 136300 | MSE Train Loss: 73.66304016113281 | MSE Test Loss: 70.21985626220703\n",
      "Epoch: 136400 | MSE Train Loss: 73.66284942626953 | MSE Test Loss: 70.21949768066406\n",
      "Epoch: 136500 | MSE Train Loss: 73.66263580322266 | MSE Test Loss: 70.21914672851562\n",
      "Epoch: 136600 | MSE Train Loss: 73.6624526977539 | MSE Test Loss: 70.21879577636719\n",
      "Epoch: 136700 | MSE Train Loss: 73.66226196289062 | MSE Test Loss: 70.21845245361328\n",
      "Epoch: 136800 | MSE Train Loss: 73.66206359863281 | MSE Test Loss: 70.21809387207031\n",
      "Epoch: 136900 | MSE Train Loss: 73.661865234375 | MSE Test Loss: 70.21773529052734\n",
      "Epoch: 137000 | MSE Train Loss: 73.66168212890625 | MSE Test Loss: 70.21739196777344\n",
      "Epoch: 137100 | MSE Train Loss: 73.66148376464844 | MSE Test Loss: 70.21704864501953\n",
      "Epoch: 137200 | MSE Train Loss: 73.66129302978516 | MSE Test Loss: 70.21669006347656\n",
      "Epoch: 137300 | MSE Train Loss: 73.66110229492188 | MSE Test Loss: 70.21634674072266\n",
      "Epoch: 137400 | MSE Train Loss: 73.6609115600586 | MSE Test Loss: 70.21600341796875\n",
      "Epoch: 137500 | MSE Train Loss: 73.66072845458984 | MSE Test Loss: 70.21565246582031\n",
      "Epoch: 137600 | MSE Train Loss: 73.66053009033203 | MSE Test Loss: 70.21533203125\n",
      "Epoch: 137700 | MSE Train Loss: 73.66033935546875 | MSE Test Loss: 70.21499633789062\n",
      "Epoch: 137800 | MSE Train Loss: 73.66014862060547 | MSE Test Loss: 70.21466064453125\n",
      "Epoch: 137900 | MSE Train Loss: 73.65995788574219 | MSE Test Loss: 70.21434020996094\n",
      "Epoch: 138000 | MSE Train Loss: 73.6597671508789 | MSE Test Loss: 70.21399688720703\n",
      "Epoch: 138100 | MSE Train Loss: 73.65959167480469 | MSE Test Loss: 70.21366119384766\n",
      "Epoch: 138200 | MSE Train Loss: 73.6594009399414 | MSE Test Loss: 70.21332550048828\n",
      "Epoch: 138300 | MSE Train Loss: 73.65921020507812 | MSE Test Loss: 70.21299743652344\n",
      "Epoch: 138400 | MSE Train Loss: 73.6590347290039 | MSE Test Loss: 70.2126693725586\n",
      "Epoch: 138500 | MSE Train Loss: 73.65885162353516 | MSE Test Loss: 70.21233367919922\n",
      "Epoch: 138600 | MSE Train Loss: 73.65865325927734 | MSE Test Loss: 70.2120132446289\n",
      "Epoch: 138700 | MSE Train Loss: 73.65847778320312 | MSE Test Loss: 70.21167755126953\n",
      "Epoch: 138800 | MSE Train Loss: 73.6583023071289 | MSE Test Loss: 70.21134948730469\n",
      "Epoch: 138900 | MSE Train Loss: 73.65811157226562 | MSE Test Loss: 70.21101379394531\n",
      "Epoch: 139000 | MSE Train Loss: 73.6579360961914 | MSE Test Loss: 70.210693359375\n",
      "Epoch: 139100 | MSE Train Loss: 73.65775299072266 | MSE Test Loss: 70.21036529541016\n",
      "Epoch: 139200 | MSE Train Loss: 73.65757751464844 | MSE Test Loss: 70.21004486083984\n",
      "Epoch: 139300 | MSE Train Loss: 73.65739440917969 | MSE Test Loss: 70.20970916748047\n",
      "Epoch: 139400 | MSE Train Loss: 73.65721130371094 | MSE Test Loss: 70.20939636230469\n",
      "Epoch: 139500 | MSE Train Loss: 73.65703582763672 | MSE Test Loss: 70.20906829833984\n",
      "Epoch: 139600 | MSE Train Loss: 73.6568603515625 | MSE Test Loss: 70.208740234375\n",
      "Epoch: 139700 | MSE Train Loss: 73.65668487548828 | MSE Test Loss: 70.20841979980469\n",
      "Epoch: 139800 | MSE Train Loss: 73.65650939941406 | MSE Test Loss: 70.20809173583984\n",
      "Epoch: 139900 | MSE Train Loss: 73.65633392333984 | MSE Test Loss: 70.20777893066406\n",
      "Epoch: 140000 | MSE Train Loss: 73.6561508178711 | MSE Test Loss: 70.20745086669922\n",
      "Epoch: 140100 | MSE Train Loss: 73.65599060058594 | MSE Test Loss: 70.20713806152344\n",
      "Epoch: 140200 | MSE Train Loss: 73.65581512451172 | MSE Test Loss: 70.20681762695312\n",
      "Epoch: 140300 | MSE Train Loss: 73.6556396484375 | MSE Test Loss: 70.20649719238281\n",
      "Epoch: 140400 | MSE Train Loss: 73.65546417236328 | MSE Test Loss: 70.2061767578125\n",
      "Epoch: 140500 | MSE Train Loss: 73.6552734375 | MSE Test Loss: 70.20587158203125\n",
      "Epoch: 140600 | MSE Train Loss: 73.65512084960938 | MSE Test Loss: 70.20555114746094\n",
      "Epoch: 140700 | MSE Train Loss: 73.65495300292969 | MSE Test Loss: 70.20521545410156\n",
      "Epoch: 140800 | MSE Train Loss: 73.65477752685547 | MSE Test Loss: 70.20491027832031\n",
      "Epoch: 140900 | MSE Train Loss: 73.65460968017578 | MSE Test Loss: 70.20459747314453\n",
      "Epoch: 141000 | MSE Train Loss: 73.65443420410156 | MSE Test Loss: 70.20428466796875\n",
      "Epoch: 141100 | MSE Train Loss: 73.6542739868164 | MSE Test Loss: 70.20396423339844\n",
      "Epoch: 141200 | MSE Train Loss: 73.65409088134766 | MSE Test Loss: 70.20365142822266\n",
      "Epoch: 141300 | MSE Train Loss: 73.65393829345703 | MSE Test Loss: 70.20333862304688\n",
      "Epoch: 141400 | MSE Train Loss: 73.65377044677734 | MSE Test Loss: 70.2030258178711\n",
      "Epoch: 141500 | MSE Train Loss: 73.65360260009766 | MSE Test Loss: 70.20271301269531\n",
      "Epoch: 141600 | MSE Train Loss: 73.65343475341797 | MSE Test Loss: 70.20240020751953\n",
      "Epoch: 141700 | MSE Train Loss: 73.65327453613281 | MSE Test Loss: 70.20208740234375\n",
      "Epoch: 141800 | MSE Train Loss: 73.6530990600586 | MSE Test Loss: 70.20177459716797\n",
      "Epoch: 141900 | MSE Train Loss: 73.65293884277344 | MSE Test Loss: 70.20147705078125\n",
      "Epoch: 142000 | MSE Train Loss: 73.65277862548828 | MSE Test Loss: 70.20116424560547\n",
      "Epoch: 142100 | MSE Train Loss: 73.65262603759766 | MSE Test Loss: 70.20085144042969\n",
      "Epoch: 142200 | MSE Train Loss: 73.65245056152344 | MSE Test Loss: 70.20054626464844\n",
      "Epoch: 142300 | MSE Train Loss: 73.65229797363281 | MSE Test Loss: 70.20024108886719\n",
      "Epoch: 142400 | MSE Train Loss: 73.65213775634766 | MSE Test Loss: 70.1999282836914\n",
      "Epoch: 142500 | MSE Train Loss: 73.6519775390625 | MSE Test Loss: 70.19962310791016\n",
      "Epoch: 142600 | MSE Train Loss: 73.65181732177734 | MSE Test Loss: 70.19932556152344\n",
      "Epoch: 142700 | MSE Train Loss: 73.65166473388672 | MSE Test Loss: 70.19902038574219\n",
      "Epoch: 142800 | MSE Train Loss: 73.65149688720703 | MSE Test Loss: 70.19871520996094\n",
      "Epoch: 142900 | MSE Train Loss: 73.65133666992188 | MSE Test Loss: 70.19841003417969\n",
      "Epoch: 143000 | MSE Train Loss: 73.65118408203125 | MSE Test Loss: 70.19811248779297\n",
      "Epoch: 143100 | MSE Train Loss: 73.65103149414062 | MSE Test Loss: 70.19781494140625\n",
      "Epoch: 143200 | MSE Train Loss: 73.65087127685547 | MSE Test Loss: 70.197509765625\n",
      "Epoch: 143300 | MSE Train Loss: 73.65071105957031 | MSE Test Loss: 70.19721221923828\n",
      "Epoch: 143400 | MSE Train Loss: 73.65056610107422 | MSE Test Loss: 70.19690704345703\n",
      "Epoch: 143500 | MSE Train Loss: 73.6504135131836 | MSE Test Loss: 70.19660949707031\n",
      "Epoch: 143600 | MSE Train Loss: 73.65026092529297 | MSE Test Loss: 70.19629669189453\n",
      "Epoch: 143700 | MSE Train Loss: 73.65010070800781 | MSE Test Loss: 70.19601440429688\n",
      "Epoch: 143800 | MSE Train Loss: 73.64996337890625 | MSE Test Loss: 70.19571685791016\n",
      "Epoch: 143900 | MSE Train Loss: 73.6498031616211 | MSE Test Loss: 70.1954116821289\n",
      "Epoch: 144000 | MSE Train Loss: 73.64964294433594 | MSE Test Loss: 70.19512176513672\n",
      "Epoch: 144100 | MSE Train Loss: 73.64949035644531 | MSE Test Loss: 70.19481658935547\n",
      "Epoch: 144200 | MSE Train Loss: 73.64935302734375 | MSE Test Loss: 70.19452667236328\n",
      "Epoch: 144300 | MSE Train Loss: 73.64920043945312 | MSE Test Loss: 70.19422912597656\n",
      "Epoch: 144400 | MSE Train Loss: 73.6490478515625 | MSE Test Loss: 70.1939468383789\n",
      "Epoch: 144500 | MSE Train Loss: 73.6489028930664 | MSE Test Loss: 70.19363403320312\n",
      "Epoch: 144600 | MSE Train Loss: 73.64875030517578 | MSE Test Loss: 70.19335174560547\n",
      "Epoch: 144700 | MSE Train Loss: 73.64860534667969 | MSE Test Loss: 70.19306945800781\n",
      "Epoch: 144800 | MSE Train Loss: 73.64845275878906 | MSE Test Loss: 70.1927719116211\n",
      "Epoch: 144900 | MSE Train Loss: 73.6483154296875 | MSE Test Loss: 70.19247436523438\n",
      "Epoch: 145000 | MSE Train Loss: 73.6481704711914 | MSE Test Loss: 70.19218444824219\n",
      "Epoch: 145100 | MSE Train Loss: 73.64802551269531 | MSE Test Loss: 70.19190216064453\n",
      "Epoch: 145200 | MSE Train Loss: 73.64788818359375 | MSE Test Loss: 70.19160461425781\n",
      "Epoch: 145300 | MSE Train Loss: 73.64774322509766 | MSE Test Loss: 70.19132232666016\n",
      "Epoch: 145400 | MSE Train Loss: 73.64759826660156 | MSE Test Loss: 70.19103240966797\n",
      "Epoch: 145500 | MSE Train Loss: 73.64746856689453 | MSE Test Loss: 70.19075012207031\n",
      "Epoch: 145600 | MSE Train Loss: 73.64737701416016 | MSE Test Loss: 70.19053649902344\n",
      "Epoch: 145700 | MSE Train Loss: 73.64727783203125 | MSE Test Loss: 70.19033813476562\n",
      "Epoch: 145800 | MSE Train Loss: 73.64718627929688 | MSE Test Loss: 70.19013977050781\n",
      "Epoch: 145900 | MSE Train Loss: 73.6470947265625 | MSE Test Loss: 70.18993377685547\n",
      "Epoch: 146000 | MSE Train Loss: 73.64700317382812 | MSE Test Loss: 70.18973541259766\n",
      "Epoch: 146100 | MSE Train Loss: 73.64690399169922 | MSE Test Loss: 70.18954467773438\n",
      "Epoch: 146200 | MSE Train Loss: 73.64681243896484 | MSE Test Loss: 70.18934631347656\n",
      "Epoch: 146300 | MSE Train Loss: 73.64672088623047 | MSE Test Loss: 70.18914794921875\n",
      "Epoch: 146400 | MSE Train Loss: 73.64662170410156 | MSE Test Loss: 70.18894958496094\n",
      "Epoch: 146500 | MSE Train Loss: 73.64653778076172 | MSE Test Loss: 70.18875122070312\n",
      "Epoch: 146600 | MSE Train Loss: 73.64644622802734 | MSE Test Loss: 70.18856048583984\n",
      "Epoch: 146700 | MSE Train Loss: 73.64634704589844 | MSE Test Loss: 70.18836212158203\n",
      "Epoch: 146800 | MSE Train Loss: 73.6462631225586 | MSE Test Loss: 70.18817138671875\n",
      "Epoch: 146900 | MSE Train Loss: 73.64617156982422 | MSE Test Loss: 70.18797302246094\n",
      "Epoch: 147000 | MSE Train Loss: 73.64607238769531 | MSE Test Loss: 70.18778228759766\n",
      "Epoch: 147100 | MSE Train Loss: 73.64598846435547 | MSE Test Loss: 70.18758392333984\n",
      "Epoch: 147200 | MSE Train Loss: 73.6458969116211 | MSE Test Loss: 70.18738555908203\n",
      "Epoch: 147300 | MSE Train Loss: 73.64582061767578 | MSE Test Loss: 70.18720245361328\n",
      "Epoch: 147400 | MSE Train Loss: 73.64572143554688 | MSE Test Loss: 70.1870346069336\n",
      "Epoch: 147500 | MSE Train Loss: 73.64563751220703 | MSE Test Loss: 70.18684387207031\n",
      "Epoch: 147600 | MSE Train Loss: 73.64555358886719 | MSE Test Loss: 70.18665313720703\n",
      "Epoch: 147700 | MSE Train Loss: 73.64544677734375 | MSE Test Loss: 70.18647766113281\n",
      "Epoch: 147800 | MSE Train Loss: 73.6453628540039 | MSE Test Loss: 70.18629455566406\n",
      "Epoch: 147900 | MSE Train Loss: 73.64527130126953 | MSE Test Loss: 70.18611145019531\n",
      "Epoch: 148000 | MSE Train Loss: 73.64518737792969 | MSE Test Loss: 70.18592834472656\n",
      "Epoch: 148100 | MSE Train Loss: 73.64510345458984 | MSE Test Loss: 70.18574523925781\n",
      "Epoch: 148200 | MSE Train Loss: 73.64501953125 | MSE Test Loss: 70.1855697631836\n",
      "Epoch: 148300 | MSE Train Loss: 73.64492797851562 | MSE Test Loss: 70.18538665771484\n",
      "Epoch: 148400 | MSE Train Loss: 73.64483642578125 | MSE Test Loss: 70.1852035522461\n",
      "Epoch: 148500 | MSE Train Loss: 73.6447525024414 | MSE Test Loss: 70.18502044677734\n",
      "Epoch: 148600 | MSE Train Loss: 73.64466857910156 | MSE Test Loss: 70.18484497070312\n",
      "Epoch: 148700 | MSE Train Loss: 73.64457702636719 | MSE Test Loss: 70.18466186523438\n",
      "Epoch: 148800 | MSE Train Loss: 73.64450073242188 | MSE Test Loss: 70.18447875976562\n",
      "Epoch: 148900 | MSE Train Loss: 73.6444091796875 | MSE Test Loss: 70.1843032836914\n",
      "Epoch: 149000 | MSE Train Loss: 73.64432525634766 | MSE Test Loss: 70.18412780761719\n",
      "Epoch: 149100 | MSE Train Loss: 73.64424133300781 | MSE Test Loss: 70.1839370727539\n",
      "Epoch: 149200 | MSE Train Loss: 73.64414978027344 | MSE Test Loss: 70.18376159667969\n",
      "Epoch: 149300 | MSE Train Loss: 73.64407348632812 | MSE Test Loss: 70.18357849121094\n",
      "Epoch: 149400 | MSE Train Loss: 73.64398956298828 | MSE Test Loss: 70.18340301513672\n",
      "Epoch: 149500 | MSE Train Loss: 73.6438980102539 | MSE Test Loss: 70.1832275390625\n",
      "Epoch: 149600 | MSE Train Loss: 73.6438217163086 | MSE Test Loss: 70.18304443359375\n",
      "Epoch: 149700 | MSE Train Loss: 73.64374542236328 | MSE Test Loss: 70.182861328125\n",
      "Epoch: 149800 | MSE Train Loss: 73.64366149902344 | MSE Test Loss: 70.18268585205078\n",
      "Epoch: 149900 | MSE Train Loss: 73.6435775756836 | MSE Test Loss: 70.18251037597656\n",
      "Epoch: 150000 | MSE Train Loss: 73.64349365234375 | MSE Test Loss: 70.18233489990234\n",
      "Epoch: 150100 | MSE Train Loss: 73.6434097290039 | MSE Test Loss: 70.1821517944336\n",
      "Epoch: 150200 | MSE Train Loss: 73.6433334350586 | MSE Test Loss: 70.1819839477539\n",
      "Epoch: 150300 | MSE Train Loss: 73.64323425292969 | MSE Test Loss: 70.18181610107422\n",
      "Epoch: 150400 | MSE Train Loss: 73.6431655883789 | MSE Test Loss: 70.181640625\n",
      "Epoch: 150500 | MSE Train Loss: 73.6430892944336 | MSE Test Loss: 70.18145751953125\n",
      "Epoch: 150600 | MSE Train Loss: 73.64300537109375 | MSE Test Loss: 70.18128204345703\n",
      "Epoch: 150700 | MSE Train Loss: 73.6429214477539 | MSE Test Loss: 70.18110656738281\n",
      "Epoch: 150800 | MSE Train Loss: 73.6428451538086 | MSE Test Loss: 70.1809310913086\n",
      "Epoch: 150900 | MSE Train Loss: 73.64276123046875 | MSE Test Loss: 70.18074798583984\n",
      "Epoch: 151000 | MSE Train Loss: 73.64268493652344 | MSE Test Loss: 70.18058776855469\n",
      "Epoch: 151100 | MSE Train Loss: 73.64260864257812 | MSE Test Loss: 70.18040466308594\n",
      "Epoch: 151200 | MSE Train Loss: 73.64252471923828 | MSE Test Loss: 70.18023681640625\n",
      "Epoch: 151300 | MSE Train Loss: 73.64244842529297 | MSE Test Loss: 70.1800537109375\n",
      "Epoch: 151400 | MSE Train Loss: 73.64236450195312 | MSE Test Loss: 70.17988586425781\n",
      "Epoch: 151500 | MSE Train Loss: 73.64228820800781 | MSE Test Loss: 70.17970275878906\n",
      "Epoch: 151600 | MSE Train Loss: 73.6422119140625 | MSE Test Loss: 70.17953491210938\n",
      "Epoch: 151700 | MSE Train Loss: 73.64213562011719 | MSE Test Loss: 70.17936706542969\n",
      "Epoch: 151800 | MSE Train Loss: 73.64205932617188 | MSE Test Loss: 70.17919921875\n",
      "Epoch: 151900 | MSE Train Loss: 73.64197540283203 | MSE Test Loss: 70.17902374267578\n",
      "Epoch: 152000 | MSE Train Loss: 73.64189910888672 | MSE Test Loss: 70.1788558959961\n",
      "Epoch: 152100 | MSE Train Loss: 73.64183044433594 | MSE Test Loss: 70.17867279052734\n",
      "Epoch: 152200 | MSE Train Loss: 73.6417465209961 | MSE Test Loss: 70.17851257324219\n",
      "Epoch: 152300 | MSE Train Loss: 73.64167785644531 | MSE Test Loss: 70.1783447265625\n",
      "Epoch: 152400 | MSE Train Loss: 73.6416015625 | MSE Test Loss: 70.17816925048828\n",
      "Epoch: 152500 | MSE Train Loss: 73.64151763916016 | MSE Test Loss: 70.17799377441406\n",
      "Epoch: 152600 | MSE Train Loss: 73.64144134521484 | MSE Test Loss: 70.17784118652344\n",
      "Epoch: 152700 | MSE Train Loss: 73.64137268066406 | MSE Test Loss: 70.17765808105469\n",
      "Epoch: 152800 | MSE Train Loss: 73.64129638671875 | MSE Test Loss: 70.177490234375\n",
      "Epoch: 152900 | MSE Train Loss: 73.64122772216797 | MSE Test Loss: 70.17732238769531\n",
      "Epoch: 153000 | MSE Train Loss: 73.64115142822266 | MSE Test Loss: 70.1771469116211\n",
      "Epoch: 153100 | MSE Train Loss: 73.64107513427734 | MSE Test Loss: 70.17698669433594\n",
      "Epoch: 153200 | MSE Train Loss: 73.64099884033203 | MSE Test Loss: 70.17681121826172\n",
      "Epoch: 153300 | MSE Train Loss: 73.64092254638672 | MSE Test Loss: 70.1766357421875\n",
      "Epoch: 153400 | MSE Train Loss: 73.64085388183594 | MSE Test Loss: 70.17648315429688\n",
      "Epoch: 153500 | MSE Train Loss: 73.64077758789062 | MSE Test Loss: 70.17631530761719\n",
      "Epoch: 153600 | MSE Train Loss: 73.64070892333984 | MSE Test Loss: 70.1761474609375\n",
      "Epoch: 153700 | MSE Train Loss: 73.64064025878906 | MSE Test Loss: 70.17597961425781\n",
      "Epoch: 153800 | MSE Train Loss: 73.64057159423828 | MSE Test Loss: 70.17581176757812\n",
      "Epoch: 153900 | MSE Train Loss: 73.64048767089844 | MSE Test Loss: 70.17564392089844\n",
      "Epoch: 154000 | MSE Train Loss: 73.64042663574219 | MSE Test Loss: 70.17548370361328\n",
      "Epoch: 154100 | MSE Train Loss: 73.64035034179688 | MSE Test Loss: 70.1753158569336\n",
      "Epoch: 154200 | MSE Train Loss: 73.64027404785156 | MSE Test Loss: 70.17514038085938\n",
      "Epoch: 154300 | MSE Train Loss: 73.64019775390625 | MSE Test Loss: 70.17498779296875\n",
      "Epoch: 154400 | MSE Train Loss: 73.64013671875 | MSE Test Loss: 70.1748275756836\n",
      "Epoch: 154500 | MSE Train Loss: 73.64006805419922 | MSE Test Loss: 70.17465209960938\n",
      "Epoch: 154600 | MSE Train Loss: 73.63999938964844 | MSE Test Loss: 70.17448425292969\n",
      "Epoch: 154700 | MSE Train Loss: 73.63992309570312 | MSE Test Loss: 70.17431640625\n",
      "Epoch: 154800 | MSE Train Loss: 73.63985443115234 | MSE Test Loss: 70.17416381835938\n",
      "Epoch: 154900 | MSE Train Loss: 73.63978576660156 | MSE Test Loss: 70.17400360107422\n",
      "Epoch: 155000 | MSE Train Loss: 73.63970947265625 | MSE Test Loss: 70.17383575439453\n",
      "Epoch: 155100 | MSE Train Loss: 73.6396484375 | MSE Test Loss: 70.17367553710938\n",
      "Epoch: 155200 | MSE Train Loss: 73.63958740234375 | MSE Test Loss: 70.17350769042969\n",
      "Epoch: 155300 | MSE Train Loss: 73.63951873779297 | MSE Test Loss: 70.17334747314453\n",
      "Epoch: 155400 | MSE Train Loss: 73.63945007324219 | MSE Test Loss: 70.17318725585938\n",
      "Epoch: 155500 | MSE Train Loss: 73.63937377929688 | MSE Test Loss: 70.17301940917969\n",
      "Epoch: 155600 | MSE Train Loss: 73.63931274414062 | MSE Test Loss: 70.1728515625\n",
      "Epoch: 155700 | MSE Train Loss: 73.63923645019531 | MSE Test Loss: 70.17269134521484\n",
      "Epoch: 155800 | MSE Train Loss: 73.63917541503906 | MSE Test Loss: 70.17254638671875\n",
      "Epoch: 155900 | MSE Train Loss: 73.63911437988281 | MSE Test Loss: 70.17237854003906\n",
      "Epoch: 156000 | MSE Train Loss: 73.63904571533203 | MSE Test Loss: 70.17221069335938\n",
      "Epoch: 156100 | MSE Train Loss: 73.63898468017578 | MSE Test Loss: 70.17205810546875\n",
      "Epoch: 156200 | MSE Train Loss: 73.638916015625 | MSE Test Loss: 70.17189025878906\n",
      "Epoch: 156300 | MSE Train Loss: 73.63884735107422 | MSE Test Loss: 70.17173767089844\n",
      "Epoch: 156400 | MSE Train Loss: 73.63878631591797 | MSE Test Loss: 70.17157745361328\n",
      "Epoch: 156500 | MSE Train Loss: 73.63871002197266 | MSE Test Loss: 70.17141723632812\n",
      "Epoch: 156600 | MSE Train Loss: 73.6386489868164 | MSE Test Loss: 70.17125701904297\n",
      "Epoch: 156700 | MSE Train Loss: 73.63858795166016 | MSE Test Loss: 70.17109680175781\n",
      "Epoch: 156800 | MSE Train Loss: 73.6385269165039 | MSE Test Loss: 70.17093658447266\n",
      "Epoch: 156900 | MSE Train Loss: 73.63846588134766 | MSE Test Loss: 70.17078399658203\n",
      "Epoch: 157000 | MSE Train Loss: 73.63839721679688 | MSE Test Loss: 70.17062377929688\n",
      "Epoch: 157100 | MSE Train Loss: 73.63833618164062 | MSE Test Loss: 70.17047119140625\n",
      "Epoch: 157200 | MSE Train Loss: 73.63827514648438 | MSE Test Loss: 70.17030334472656\n",
      "Epoch: 157300 | MSE Train Loss: 73.63821411132812 | MSE Test Loss: 70.17015075683594\n",
      "Epoch: 157400 | MSE Train Loss: 73.63814544677734 | MSE Test Loss: 70.16999053955078\n",
      "Epoch: 157500 | MSE Train Loss: 73.63807678222656 | MSE Test Loss: 70.16983795166016\n",
      "Epoch: 157600 | MSE Train Loss: 73.63802337646484 | MSE Test Loss: 70.169677734375\n",
      "Epoch: 157700 | MSE Train Loss: 73.6379623413086 | MSE Test Loss: 70.16952514648438\n",
      "Epoch: 157800 | MSE Train Loss: 73.63790130615234 | MSE Test Loss: 70.16936492919922\n",
      "Epoch: 157900 | MSE Train Loss: 73.6378402709961 | MSE Test Loss: 70.1692123413086\n",
      "Epoch: 158000 | MSE Train Loss: 73.63777160644531 | MSE Test Loss: 70.16905975341797\n",
      "Epoch: 158100 | MSE Train Loss: 73.63771057128906 | MSE Test Loss: 70.16890716552734\n",
      "Epoch: 158200 | MSE Train Loss: 73.63766479492188 | MSE Test Loss: 70.16874694824219\n",
      "Epoch: 158300 | MSE Train Loss: 73.6375961303711 | MSE Test Loss: 70.1686019897461\n",
      "Epoch: 158400 | MSE Train Loss: 73.63753509521484 | MSE Test Loss: 70.16844177246094\n",
      "Epoch: 158500 | MSE Train Loss: 73.6374740600586 | MSE Test Loss: 70.16828918457031\n",
      "Epoch: 158600 | MSE Train Loss: 73.63741302490234 | MSE Test Loss: 70.16812896728516\n",
      "Epoch: 158700 | MSE Train Loss: 73.63735961914062 | MSE Test Loss: 70.16797637939453\n",
      "Epoch: 158800 | MSE Train Loss: 73.63729858398438 | MSE Test Loss: 70.1678237915039\n",
      "Epoch: 158900 | MSE Train Loss: 73.63723754882812 | MSE Test Loss: 70.16767120361328\n",
      "Epoch: 159000 | MSE Train Loss: 73.6371841430664 | MSE Test Loss: 70.16752624511719\n",
      "Epoch: 159100 | MSE Train Loss: 73.63712310791016 | MSE Test Loss: 70.16737365722656\n",
      "Epoch: 159200 | MSE Train Loss: 73.63707733154297 | MSE Test Loss: 70.16722106933594\n",
      "Epoch: 159300 | MSE Train Loss: 73.63701629638672 | MSE Test Loss: 70.16706085205078\n",
      "Epoch: 159400 | MSE Train Loss: 73.636962890625 | MSE Test Loss: 70.16691589355469\n",
      "Epoch: 159500 | MSE Train Loss: 73.63690948486328 | MSE Test Loss: 70.16677856445312\n",
      "Epoch: 159600 | MSE Train Loss: 73.63688659667969 | MSE Test Loss: 70.16668701171875\n",
      "Epoch: 159700 | MSE Train Loss: 73.63685607910156 | MSE Test Loss: 70.16658782958984\n",
      "Epoch: 159800 | MSE Train Loss: 73.63683319091797 | MSE Test Loss: 70.16649627685547\n",
      "Epoch: 159900 | MSE Train Loss: 73.63680267333984 | MSE Test Loss: 70.1664047241211\n",
      "Epoch: 160000 | MSE Train Loss: 73.63677215576172 | MSE Test Loss: 70.16632080078125\n",
      "Epoch: 160100 | MSE Train Loss: 73.6367416381836 | MSE Test Loss: 70.1662368774414\n",
      "Epoch: 160200 | MSE Train Loss: 73.63671112060547 | MSE Test Loss: 70.16615295410156\n",
      "Epoch: 160300 | MSE Train Loss: 73.6366958618164 | MSE Test Loss: 70.16605377197266\n",
      "Epoch: 160400 | MSE Train Loss: 73.63666534423828 | MSE Test Loss: 70.16596984863281\n",
      "Epoch: 160500 | MSE Train Loss: 73.63663482666016 | MSE Test Loss: 70.16587829589844\n",
      "Epoch: 160600 | MSE Train Loss: 73.63660430908203 | MSE Test Loss: 70.1657943725586\n",
      "Epoch: 160700 | MSE Train Loss: 73.63658142089844 | MSE Test Loss: 70.16571807861328\n",
      "Epoch: 160800 | MSE Train Loss: 73.63655090332031 | MSE Test Loss: 70.1656494140625\n",
      "Epoch: 160900 | MSE Train Loss: 73.63652801513672 | MSE Test Loss: 70.16557312011719\n",
      "Epoch: 161000 | MSE Train Loss: 73.6364974975586 | MSE Test Loss: 70.16549682617188\n",
      "Epoch: 161100 | MSE Train Loss: 73.63646697998047 | MSE Test Loss: 70.1654281616211\n",
      "Epoch: 161200 | MSE Train Loss: 73.6364517211914 | MSE Test Loss: 70.16535949707031\n",
      "Epoch: 161300 | MSE Train Loss: 73.63642120361328 | MSE Test Loss: 70.16529083251953\n",
      "Epoch: 161400 | MSE Train Loss: 73.63639068603516 | MSE Test Loss: 70.16520690917969\n",
      "Epoch: 161500 | MSE Train Loss: 73.63636016845703 | MSE Test Loss: 70.1651382446289\n",
      "Epoch: 161600 | MSE Train Loss: 73.63633728027344 | MSE Test Loss: 70.1650619506836\n",
      "Epoch: 161700 | MSE Train Loss: 73.63630676269531 | MSE Test Loss: 70.16498565673828\n",
      "Epoch: 161800 | MSE Train Loss: 73.63628387451172 | MSE Test Loss: 70.16490936279297\n",
      "Epoch: 161900 | MSE Train Loss: 73.6362533569336 | MSE Test Loss: 70.16483306884766\n",
      "Epoch: 162000 | MSE Train Loss: 73.63622283935547 | MSE Test Loss: 70.16475677490234\n",
      "Epoch: 162100 | MSE Train Loss: 73.63619995117188 | MSE Test Loss: 70.16468811035156\n",
      "Epoch: 162200 | MSE Train Loss: 73.63616943359375 | MSE Test Loss: 70.16461181640625\n",
      "Epoch: 162300 | MSE Train Loss: 73.63614654541016 | MSE Test Loss: 70.16453552246094\n",
      "Epoch: 162400 | MSE Train Loss: 73.63612365722656 | MSE Test Loss: 70.16446685791016\n",
      "Epoch: 162500 | MSE Train Loss: 73.63609313964844 | MSE Test Loss: 70.16439056396484\n",
      "Epoch: 162600 | MSE Train Loss: 73.63607025146484 | MSE Test Loss: 70.16432189941406\n",
      "Epoch: 162700 | MSE Train Loss: 73.63603973388672 | MSE Test Loss: 70.16424560546875\n",
      "Epoch: 162800 | MSE Train Loss: 73.6360092163086 | MSE Test Loss: 70.16417694091797\n",
      "Epoch: 162900 | MSE Train Loss: 73.635986328125 | MSE Test Loss: 70.16410064697266\n",
      "Epoch: 163000 | MSE Train Loss: 73.63595581054688 | MSE Test Loss: 70.16402435302734\n",
      "Epoch: 163100 | MSE Train Loss: 73.63594055175781 | MSE Test Loss: 70.16394805908203\n",
      "Epoch: 163200 | MSE Train Loss: 73.63591003417969 | MSE Test Loss: 70.16387939453125\n",
      "Epoch: 163300 | MSE Train Loss: 73.63587188720703 | MSE Test Loss: 70.16380310058594\n",
      "Epoch: 163400 | MSE Train Loss: 73.63585662841797 | MSE Test Loss: 70.16373443603516\n",
      "Epoch: 163500 | MSE Train Loss: 73.63583374023438 | MSE Test Loss: 70.16366577148438\n",
      "Epoch: 163600 | MSE Train Loss: 73.63581085205078 | MSE Test Loss: 70.16358947753906\n",
      "Epoch: 163700 | MSE Train Loss: 73.63577270507812 | MSE Test Loss: 70.16352081298828\n",
      "Epoch: 163800 | MSE Train Loss: 73.63575744628906 | MSE Test Loss: 70.16344451904297\n",
      "Epoch: 163900 | MSE Train Loss: 73.63572692871094 | MSE Test Loss: 70.16336822509766\n",
      "Epoch: 164000 | MSE Train Loss: 73.63569641113281 | MSE Test Loss: 70.16329956054688\n",
      "Epoch: 164100 | MSE Train Loss: 73.63567352294922 | MSE Test Loss: 70.16322326660156\n",
      "Epoch: 164200 | MSE Train Loss: 73.63565063476562 | MSE Test Loss: 70.16314697265625\n",
      "Epoch: 164300 | MSE Train Loss: 73.63562774658203 | MSE Test Loss: 70.1630859375\n",
      "Epoch: 164400 | MSE Train Loss: 73.6355972290039 | MSE Test Loss: 70.16300964355469\n",
      "Epoch: 164500 | MSE Train Loss: 73.63557434082031 | MSE Test Loss: 70.1629409790039\n",
      "Epoch: 164600 | MSE Train Loss: 73.63555908203125 | MSE Test Loss: 70.1628646850586\n",
      "Epoch: 164700 | MSE Train Loss: 73.63553619384766 | MSE Test Loss: 70.16280364990234\n",
      "Epoch: 164800 | MSE Train Loss: 73.635498046875 | MSE Test Loss: 70.16272735595703\n",
      "Epoch: 164900 | MSE Train Loss: 73.63548278808594 | MSE Test Loss: 70.16265106201172\n",
      "Epoch: 165000 | MSE Train Loss: 73.63545227050781 | MSE Test Loss: 70.16258239746094\n",
      "Epoch: 165100 | MSE Train Loss: 73.63542938232422 | MSE Test Loss: 70.16250610351562\n",
      "Epoch: 165200 | MSE Train Loss: 73.6353988647461 | MSE Test Loss: 70.16243743896484\n",
      "Epoch: 165300 | MSE Train Loss: 73.6353759765625 | MSE Test Loss: 70.16236114501953\n",
      "Epoch: 165400 | MSE Train Loss: 73.63536071777344 | MSE Test Loss: 70.16228485107422\n",
      "Epoch: 165500 | MSE Train Loss: 73.63533020019531 | MSE Test Loss: 70.16222381591797\n",
      "Epoch: 165600 | MSE Train Loss: 73.63531494140625 | MSE Test Loss: 70.16214752197266\n",
      "Epoch: 165700 | MSE Train Loss: 73.63528442382812 | MSE Test Loss: 70.16207885742188\n",
      "Epoch: 165800 | MSE Train Loss: 73.63526153564453 | MSE Test Loss: 70.16200256347656\n",
      "Epoch: 165900 | MSE Train Loss: 73.6352310180664 | MSE Test Loss: 70.16193389892578\n",
      "Epoch: 166000 | MSE Train Loss: 73.63520812988281 | MSE Test Loss: 70.161865234375\n",
      "Epoch: 166100 | MSE Train Loss: 73.63518524169922 | MSE Test Loss: 70.16179656982422\n",
      "Epoch: 166200 | MSE Train Loss: 73.63516235351562 | MSE Test Loss: 70.1617202758789\n",
      "Epoch: 166300 | MSE Train Loss: 73.63513946533203 | MSE Test Loss: 70.1616439819336\n",
      "Epoch: 166400 | MSE Train Loss: 73.6351089477539 | MSE Test Loss: 70.16158294677734\n",
      "Epoch: 166500 | MSE Train Loss: 73.63508605957031 | MSE Test Loss: 70.16151428222656\n",
      "Epoch: 166600 | MSE Train Loss: 73.63507843017578 | MSE Test Loss: 70.16143798828125\n",
      "Epoch: 166700 | MSE Train Loss: 73.63504028320312 | MSE Test Loss: 70.16136932373047\n",
      "Epoch: 166800 | MSE Train Loss: 73.63501739501953 | MSE Test Loss: 70.16129302978516\n",
      "Epoch: 166900 | MSE Train Loss: 73.63500213623047 | MSE Test Loss: 70.1612319946289\n",
      "Epoch: 167000 | MSE Train Loss: 73.63497161865234 | MSE Test Loss: 70.1611557006836\n",
      "Epoch: 167100 | MSE Train Loss: 73.63494873046875 | MSE Test Loss: 70.16108703613281\n",
      "Epoch: 167200 | MSE Train Loss: 73.63492584228516 | MSE Test Loss: 70.1610107421875\n",
      "Epoch: 167300 | MSE Train Loss: 73.63490295410156 | MSE Test Loss: 70.16093444824219\n",
      "Epoch: 167400 | MSE Train Loss: 73.63487243652344 | MSE Test Loss: 70.1608657836914\n",
      "Epoch: 167500 | MSE Train Loss: 73.63484954833984 | MSE Test Loss: 70.16079711914062\n",
      "Epoch: 167600 | MSE Train Loss: 73.63483428955078 | MSE Test Loss: 70.16073608398438\n",
      "Epoch: 167700 | MSE Train Loss: 73.63481140136719 | MSE Test Loss: 70.16065979003906\n",
      "Epoch: 167800 | MSE Train Loss: 73.6347885131836 | MSE Test Loss: 70.16059875488281\n",
      "Epoch: 167900 | MSE Train Loss: 73.634765625 | MSE Test Loss: 70.1605224609375\n",
      "Epoch: 168000 | MSE Train Loss: 73.63475036621094 | MSE Test Loss: 70.16046142578125\n",
      "Epoch: 168100 | MSE Train Loss: 73.63472747802734 | MSE Test Loss: 70.16038513183594\n",
      "Epoch: 168200 | MSE Train Loss: 73.63469696044922 | MSE Test Loss: 70.16031646728516\n",
      "Epoch: 168300 | MSE Train Loss: 73.63468170166016 | MSE Test Loss: 70.16024017333984\n",
      "Epoch: 168400 | MSE Train Loss: 73.63465118408203 | MSE Test Loss: 70.16017150878906\n",
      "Epoch: 168500 | MSE Train Loss: 73.63462829589844 | MSE Test Loss: 70.16010284423828\n",
      "Epoch: 168600 | MSE Train Loss: 73.63460540771484 | MSE Test Loss: 70.1600341796875\n",
      "Epoch: 168700 | MSE Train Loss: 73.63459014892578 | MSE Test Loss: 70.15996551513672\n",
      "Epoch: 168800 | MSE Train Loss: 73.63455963134766 | MSE Test Loss: 70.15989685058594\n",
      "Epoch: 168900 | MSE Train Loss: 73.63455200195312 | MSE Test Loss: 70.15982818603516\n",
      "Epoch: 169000 | MSE Train Loss: 73.63451385498047 | MSE Test Loss: 70.15975952148438\n",
      "Epoch: 169100 | MSE Train Loss: 73.6344985961914 | MSE Test Loss: 70.1596908569336\n",
      "Epoch: 169200 | MSE Train Loss: 73.63447570800781 | MSE Test Loss: 70.15962219238281\n",
      "Epoch: 169300 | MSE Train Loss: 73.63445281982422 | MSE Test Loss: 70.15955352783203\n",
      "Epoch: 169400 | MSE Train Loss: 73.63442993164062 | MSE Test Loss: 70.15948486328125\n",
      "Epoch: 169500 | MSE Train Loss: 73.63441467285156 | MSE Test Loss: 70.15941619873047\n",
      "Epoch: 169600 | MSE Train Loss: 73.63438415527344 | MSE Test Loss: 70.15934753417969\n",
      "Epoch: 169700 | MSE Train Loss: 73.63436126708984 | MSE Test Loss: 70.15927124023438\n",
      "Epoch: 169800 | MSE Train Loss: 73.63433837890625 | MSE Test Loss: 70.1592025756836\n",
      "Epoch: 169900 | MSE Train Loss: 73.63432312011719 | MSE Test Loss: 70.15914916992188\n",
      "Epoch: 170000 | MSE Train Loss: 73.63430786132812 | MSE Test Loss: 70.15907287597656\n",
      "Epoch: 170100 | MSE Train Loss: 73.63428497314453 | MSE Test Loss: 70.15899658203125\n",
      "Epoch: 170200 | MSE Train Loss: 73.63426208496094 | MSE Test Loss: 70.158935546875\n",
      "Epoch: 170300 | MSE Train Loss: 73.63423919677734 | MSE Test Loss: 70.15885925292969\n",
      "Epoch: 170400 | MSE Train Loss: 73.63421630859375 | MSE Test Loss: 70.15880584716797\n",
      "Epoch: 170500 | MSE Train Loss: 73.63419342041016 | MSE Test Loss: 70.15872192382812\n",
      "Epoch: 170600 | MSE Train Loss: 73.63417053222656 | MSE Test Loss: 70.15866088867188\n",
      "Epoch: 170700 | MSE Train Loss: 73.6341552734375 | MSE Test Loss: 70.15858459472656\n",
      "Epoch: 170800 | MSE Train Loss: 73.63414001464844 | MSE Test Loss: 70.15853118896484\n",
      "Epoch: 170900 | MSE Train Loss: 73.63411712646484 | MSE Test Loss: 70.15845489501953\n",
      "Epoch: 171000 | MSE Train Loss: 73.63409423828125 | MSE Test Loss: 70.15840148925781\n",
      "Epoch: 171100 | MSE Train Loss: 73.63407135009766 | MSE Test Loss: 70.1583251953125\n",
      "Epoch: 171200 | MSE Train Loss: 73.63404846191406 | MSE Test Loss: 70.15826416015625\n",
      "Epoch: 171300 | MSE Train Loss: 73.63402557373047 | MSE Test Loss: 70.15818786621094\n",
      "Epoch: 171400 | MSE Train Loss: 73.63400268554688 | MSE Test Loss: 70.15812683105469\n",
      "Epoch: 171500 | MSE Train Loss: 73.63398742675781 | MSE Test Loss: 70.1580581665039\n",
      "Epoch: 171600 | MSE Train Loss: 73.63397216796875 | MSE Test Loss: 70.15799713134766\n",
      "Epoch: 171700 | MSE Train Loss: 73.63394927978516 | MSE Test Loss: 70.15792083740234\n",
      "Epoch: 171800 | MSE Train Loss: 73.63392639160156 | MSE Test Loss: 70.15785217285156\n",
      "Epoch: 171900 | MSE Train Loss: 73.6339111328125 | MSE Test Loss: 70.15779113769531\n",
      "Epoch: 172000 | MSE Train Loss: 73.63388061523438 | MSE Test Loss: 70.15772247314453\n",
      "Epoch: 172100 | MSE Train Loss: 73.63386535644531 | MSE Test Loss: 70.15765380859375\n",
      "Epoch: 172200 | MSE Train Loss: 73.63385009765625 | MSE Test Loss: 70.15758514404297\n",
      "Epoch: 172300 | MSE Train Loss: 73.63382720947266 | MSE Test Loss: 70.15750885009766\n",
      "Epoch: 172400 | MSE Train Loss: 73.63380432128906 | MSE Test Loss: 70.15745544433594\n",
      "Epoch: 172500 | MSE Train Loss: 73.6337890625 | MSE Test Loss: 70.15737915039062\n",
      "Epoch: 172600 | MSE Train Loss: 73.6337661743164 | MSE Test Loss: 70.15731048583984\n",
      "Epoch: 172700 | MSE Train Loss: 73.63375091552734 | MSE Test Loss: 70.15724182128906\n",
      "Epoch: 172800 | MSE Train Loss: 73.63372802734375 | MSE Test Loss: 70.15718841552734\n",
      "Epoch: 172900 | MSE Train Loss: 73.63371276855469 | MSE Test Loss: 70.15711212158203\n",
      "Epoch: 173000 | MSE Train Loss: 73.6336898803711 | MSE Test Loss: 70.15705108642578\n",
      "Epoch: 173100 | MSE Train Loss: 73.63367462158203 | MSE Test Loss: 70.15699005126953\n",
      "Epoch: 173200 | MSE Train Loss: 73.63365936279297 | MSE Test Loss: 70.15692138671875\n",
      "Epoch: 173300 | MSE Train Loss: 73.63363647460938 | MSE Test Loss: 70.15685272216797\n",
      "Epoch: 173400 | MSE Train Loss: 73.63362121582031 | MSE Test Loss: 70.15678405761719\n",
      "Epoch: 173500 | MSE Train Loss: 73.63359832763672 | MSE Test Loss: 70.1567153930664\n",
      "Epoch: 173600 | MSE Train Loss: 73.63359069824219 | MSE Test Loss: 70.15665435791016\n",
      "Epoch: 173700 | MSE Train Loss: 73.63356018066406 | MSE Test Loss: 70.1565933227539\n",
      "Epoch: 173800 | MSE Train Loss: 73.63353729248047 | MSE Test Loss: 70.15652465820312\n",
      "Epoch: 173900 | MSE Train Loss: 73.6335220336914 | MSE Test Loss: 70.15645599365234\n",
      "Epoch: 174000 | MSE Train Loss: 73.63350677490234 | MSE Test Loss: 70.15638732910156\n",
      "Epoch: 174100 | MSE Train Loss: 73.63348388671875 | MSE Test Loss: 70.15632629394531\n",
      "Epoch: 174200 | MSE Train Loss: 73.63346099853516 | MSE Test Loss: 70.15626525878906\n",
      "Epoch: 174300 | MSE Train Loss: 73.63345336914062 | MSE Test Loss: 70.15619659423828\n",
      "Epoch: 174400 | MSE Train Loss: 73.63343048095703 | MSE Test Loss: 70.1561279296875\n",
      "Epoch: 174500 | MSE Train Loss: 73.63341522216797 | MSE Test Loss: 70.15605926513672\n",
      "Epoch: 174600 | MSE Train Loss: 73.63339233398438 | MSE Test Loss: 70.15599822998047\n",
      "Epoch: 174700 | MSE Train Loss: 73.63337707519531 | MSE Test Loss: 70.15592956542969\n",
      "Epoch: 174800 | MSE Train Loss: 73.63335418701172 | MSE Test Loss: 70.1558609008789\n",
      "Epoch: 174900 | MSE Train Loss: 73.63333892822266 | MSE Test Loss: 70.15579986572266\n",
      "Epoch: 175000 | MSE Train Loss: 73.63331604003906 | MSE Test Loss: 70.15573120117188\n",
      "Epoch: 175100 | MSE Train Loss: 73.63330078125 | MSE Test Loss: 70.15567016601562\n",
      "Epoch: 175200 | MSE Train Loss: 73.6332778930664 | MSE Test Loss: 70.15560913085938\n",
      "Epoch: 175300 | MSE Train Loss: 73.63326263427734 | MSE Test Loss: 70.15554809570312\n",
      "Epoch: 175400 | MSE Train Loss: 73.63324737548828 | MSE Test Loss: 70.15547943115234\n",
      "Epoch: 175500 | MSE Train Loss: 73.63322448730469 | MSE Test Loss: 70.15541076660156\n",
      "Epoch: 175600 | MSE Train Loss: 73.63320922851562 | MSE Test Loss: 70.15534973144531\n",
      "Epoch: 175700 | MSE Train Loss: 73.6332015991211 | MSE Test Loss: 70.15528869628906\n",
      "Epoch: 175800 | MSE Train Loss: 73.6331787109375 | MSE Test Loss: 70.15522003173828\n",
      "Epoch: 175900 | MSE Train Loss: 73.63316345214844 | MSE Test Loss: 70.1551513671875\n",
      "Epoch: 176000 | MSE Train Loss: 73.63314056396484 | MSE Test Loss: 70.15509033203125\n",
      "Epoch: 176100 | MSE Train Loss: 73.63312530517578 | MSE Test Loss: 70.155029296875\n",
      "Epoch: 176200 | MSE Train Loss: 73.63311004638672 | MSE Test Loss: 70.15496063232422\n",
      "Epoch: 176300 | MSE Train Loss: 73.63308715820312 | MSE Test Loss: 70.15489959716797\n",
      "Epoch: 176400 | MSE Train Loss: 73.63307189941406 | MSE Test Loss: 70.15484619140625\n",
      "Epoch: 176500 | MSE Train Loss: 73.63306427001953 | MSE Test Loss: 70.15476989746094\n",
      "Epoch: 176600 | MSE Train Loss: 73.6330337524414 | MSE Test Loss: 70.15470886230469\n",
      "Epoch: 176700 | MSE Train Loss: 73.63302612304688 | MSE Test Loss: 70.1546401977539\n",
      "Epoch: 176800 | MSE Train Loss: 73.63300323486328 | MSE Test Loss: 70.15457153320312\n",
      "Epoch: 176900 | MSE Train Loss: 73.63298797607422 | MSE Test Loss: 70.15451049804688\n",
      "Epoch: 177000 | MSE Train Loss: 73.63297271728516 | MSE Test Loss: 70.15444946289062\n",
      "Epoch: 177100 | MSE Train Loss: 73.63296508789062 | MSE Test Loss: 70.15438842773438\n",
      "Epoch: 177200 | MSE Train Loss: 73.6329345703125 | MSE Test Loss: 70.1543197631836\n",
      "Epoch: 177300 | MSE Train Loss: 73.63292694091797 | MSE Test Loss: 70.15425872802734\n",
      "Epoch: 177400 | MSE Train Loss: 73.63290405273438 | MSE Test Loss: 70.1541976928711\n",
      "Epoch: 177500 | MSE Train Loss: 73.63289642333984 | MSE Test Loss: 70.15412902832031\n",
      "Epoch: 177600 | MSE Train Loss: 73.63287353515625 | MSE Test Loss: 70.1540756225586\n",
      "Epoch: 177700 | MSE Train Loss: 73.63286590576172 | MSE Test Loss: 70.15399932861328\n",
      "Epoch: 177800 | MSE Train Loss: 73.63285064697266 | MSE Test Loss: 70.15394592285156\n",
      "Epoch: 177900 | MSE Train Loss: 73.63282775878906 | MSE Test Loss: 70.15387725830078\n",
      "Epoch: 178000 | MSE Train Loss: 73.6328125 | MSE Test Loss: 70.15381622314453\n",
      "Epoch: 178100 | MSE Train Loss: 73.6327896118164 | MSE Test Loss: 70.15374755859375\n",
      "Epoch: 178200 | MSE Train Loss: 73.63277435302734 | MSE Test Loss: 70.15369415283203\n",
      "Epoch: 178300 | MSE Train Loss: 73.63275909423828 | MSE Test Loss: 70.15362548828125\n",
      "Epoch: 178400 | MSE Train Loss: 73.63275146484375 | MSE Test Loss: 70.15357208251953\n",
      "Epoch: 178500 | MSE Train Loss: 73.63272857666016 | MSE Test Loss: 70.15350341796875\n",
      "Epoch: 178600 | MSE Train Loss: 73.6327133178711 | MSE Test Loss: 70.1534423828125\n",
      "Epoch: 178700 | MSE Train Loss: 73.63269805908203 | MSE Test Loss: 70.15337371826172\n",
      "Epoch: 178800 | MSE Train Loss: 73.6326904296875 | MSE Test Loss: 70.1533203125\n",
      "Epoch: 178900 | MSE Train Loss: 73.63267517089844 | MSE Test Loss: 70.15325927734375\n",
      "Epoch: 179000 | MSE Train Loss: 73.63265228271484 | MSE Test Loss: 70.15319061279297\n",
      "Epoch: 179100 | MSE Train Loss: 73.63263702392578 | MSE Test Loss: 70.15312957763672\n",
      "Epoch: 179200 | MSE Train Loss: 73.63262176513672 | MSE Test Loss: 70.15306091308594\n",
      "Epoch: 179300 | MSE Train Loss: 73.63261413574219 | MSE Test Loss: 70.15300750732422\n",
      "Epoch: 179400 | MSE Train Loss: 73.6325912475586 | MSE Test Loss: 70.15294647216797\n",
      "Epoch: 179500 | MSE Train Loss: 73.63257598876953 | MSE Test Loss: 70.15287780761719\n",
      "Epoch: 179600 | MSE Train Loss: 73.63256072998047 | MSE Test Loss: 70.15282440185547\n",
      "Epoch: 179700 | MSE Train Loss: 73.63255310058594 | MSE Test Loss: 70.15276336669922\n",
      "Epoch: 179800 | MSE Train Loss: 73.63253784179688 | MSE Test Loss: 70.15269470214844\n",
      "Epoch: 179900 | MSE Train Loss: 73.63251495361328 | MSE Test Loss: 70.15264129638672\n",
      "Epoch: 180000 | MSE Train Loss: 73.63249969482422 | MSE Test Loss: 70.1525650024414\n",
      "Epoch: 180100 | MSE Train Loss: 73.63248443603516 | MSE Test Loss: 70.15251159667969\n",
      "Epoch: 180200 | MSE Train Loss: 73.63247680664062 | MSE Test Loss: 70.15245056152344\n",
      "Epoch: 180300 | MSE Train Loss: 73.63246154785156 | MSE Test Loss: 70.15238952636719\n",
      "Epoch: 180400 | MSE Train Loss: 73.6324462890625 | MSE Test Loss: 70.15232849121094\n",
      "Epoch: 180500 | MSE Train Loss: 73.6324234008789 | MSE Test Loss: 70.15226745605469\n",
      "Epoch: 180600 | MSE Train Loss: 73.63241577148438 | MSE Test Loss: 70.15220642089844\n",
      "Epoch: 180700 | MSE Train Loss: 73.63240814208984 | MSE Test Loss: 70.15214538574219\n",
      "Epoch: 180800 | MSE Train Loss: 73.63238525390625 | MSE Test Loss: 70.15208435058594\n",
      "Epoch: 180900 | MSE Train Loss: 73.63237762451172 | MSE Test Loss: 70.15201568603516\n",
      "Epoch: 181000 | MSE Train Loss: 73.63236236572266 | MSE Test Loss: 70.15196228027344\n",
      "Epoch: 181100 | MSE Train Loss: 73.6323471069336 | MSE Test Loss: 70.15189361572266\n",
      "Epoch: 181200 | MSE Train Loss: 73.63233947753906 | MSE Test Loss: 70.15184020996094\n",
      "Epoch: 181300 | MSE Train Loss: 73.63230895996094 | MSE Test Loss: 70.15177917480469\n",
      "Epoch: 181400 | MSE Train Loss: 73.6323013305664 | MSE Test Loss: 70.1517105102539\n",
      "Epoch: 181500 | MSE Train Loss: 73.63228607177734 | MSE Test Loss: 70.15165710449219\n",
      "Epoch: 181600 | MSE Train Loss: 73.63227844238281 | MSE Test Loss: 70.15160369873047\n",
      "Epoch: 181700 | MSE Train Loss: 73.63226318359375 | MSE Test Loss: 70.15153503417969\n",
      "Epoch: 181800 | MSE Train Loss: 73.63224792480469 | MSE Test Loss: 70.15147399902344\n",
      "Epoch: 181900 | MSE Train Loss: 73.63224029541016 | MSE Test Loss: 70.15141296386719\n",
      "Epoch: 182000 | MSE Train Loss: 73.6322250366211 | MSE Test Loss: 70.15135955810547\n",
      "Epoch: 182100 | MSE Train Loss: 73.63220977783203 | MSE Test Loss: 70.15129089355469\n",
      "Epoch: 182200 | MSE Train Loss: 73.6322021484375 | MSE Test Loss: 70.15123748779297\n",
      "Epoch: 182300 | MSE Train Loss: 73.63218688964844 | MSE Test Loss: 70.15117645263672\n",
      "Epoch: 182400 | MSE Train Loss: 73.63217163085938 | MSE Test Loss: 70.15111541748047\n",
      "Epoch: 182500 | MSE Train Loss: 73.63216400146484 | MSE Test Loss: 70.15104675292969\n",
      "Epoch: 182600 | MSE Train Loss: 73.63214874267578 | MSE Test Loss: 70.15099334716797\n",
      "Epoch: 182700 | MSE Train Loss: 73.63213348388672 | MSE Test Loss: 70.15093231201172\n",
      "Epoch: 182800 | MSE Train Loss: 73.63211822509766 | MSE Test Loss: 70.15087127685547\n",
      "Epoch: 182900 | MSE Train Loss: 73.63211059570312 | MSE Test Loss: 70.15081024169922\n",
      "Epoch: 183000 | MSE Train Loss: 73.6321029663086 | MSE Test Loss: 70.15076446533203\n",
      "Epoch: 183100 | MSE Train Loss: 73.63208770751953 | MSE Test Loss: 70.15068817138672\n",
      "Epoch: 183200 | MSE Train Loss: 73.63207244873047 | MSE Test Loss: 70.150634765625\n",
      "Epoch: 183300 | MSE Train Loss: 73.63206481933594 | MSE Test Loss: 70.15058135986328\n",
      "Epoch: 183400 | MSE Train Loss: 73.63204956054688 | MSE Test Loss: 70.15052032470703\n",
      "Epoch: 183500 | MSE Train Loss: 73.63203430175781 | MSE Test Loss: 70.15045928955078\n",
      "Epoch: 183600 | MSE Train Loss: 73.63201904296875 | MSE Test Loss: 70.15039825439453\n",
      "Epoch: 183700 | MSE Train Loss: 73.63201141357422 | MSE Test Loss: 70.15033721923828\n",
      "Epoch: 183800 | MSE Train Loss: 73.63200378417969 | MSE Test Loss: 70.15028381347656\n",
      "Epoch: 183900 | MSE Train Loss: 73.63198852539062 | MSE Test Loss: 70.15021514892578\n",
      "Epoch: 184000 | MSE Train Loss: 73.6319808959961 | MSE Test Loss: 70.15016174316406\n",
      "Epoch: 184100 | MSE Train Loss: 73.63196563720703 | MSE Test Loss: 70.15010070800781\n",
      "Epoch: 184200 | MSE Train Loss: 73.63195037841797 | MSE Test Loss: 70.1500473022461\n",
      "Epoch: 184300 | MSE Train Loss: 73.6319351196289 | MSE Test Loss: 70.14997863769531\n",
      "Epoch: 184400 | MSE Train Loss: 73.63192749023438 | MSE Test Loss: 70.1499252319336\n",
      "Epoch: 184500 | MSE Train Loss: 73.63191223144531 | MSE Test Loss: 70.14987182617188\n",
      "Epoch: 184600 | MSE Train Loss: 73.63189697265625 | MSE Test Loss: 70.14981842041016\n",
      "Epoch: 184700 | MSE Train Loss: 73.63188934326172 | MSE Test Loss: 70.14974975585938\n",
      "Epoch: 184800 | MSE Train Loss: 73.63187408447266 | MSE Test Loss: 70.14969635009766\n",
      "Epoch: 184900 | MSE Train Loss: 73.63187408447266 | MSE Test Loss: 70.14962768554688\n",
      "Epoch: 185000 | MSE Train Loss: 73.63185119628906 | MSE Test Loss: 70.14958190917969\n",
      "Epoch: 185100 | MSE Train Loss: 73.63185119628906 | MSE Test Loss: 70.14952850341797\n",
      "Epoch: 185200 | MSE Train Loss: 73.6318359375 | MSE Test Loss: 70.14945983886719\n",
      "Epoch: 185300 | MSE Train Loss: 73.63182830810547 | MSE Test Loss: 70.14939880371094\n",
      "Epoch: 185400 | MSE Train Loss: 73.6318130493164 | MSE Test Loss: 70.14935302734375\n",
      "Epoch: 185500 | MSE Train Loss: 73.63179779052734 | MSE Test Loss: 70.14928436279297\n",
      "Epoch: 185600 | MSE Train Loss: 73.63179016113281 | MSE Test Loss: 70.14923858642578\n",
      "Epoch: 185700 | MSE Train Loss: 73.63178253173828 | MSE Test Loss: 70.14917755126953\n",
      "Epoch: 185800 | MSE Train Loss: 73.63175964355469 | MSE Test Loss: 70.14910888671875\n",
      "Epoch: 185900 | MSE Train Loss: 73.63175964355469 | MSE Test Loss: 70.14905548095703\n",
      "Epoch: 186000 | MSE Train Loss: 73.6317367553711 | MSE Test Loss: 70.14900970458984\n",
      "Epoch: 186100 | MSE Train Loss: 73.6317367553711 | MSE Test Loss: 70.14894104003906\n",
      "Epoch: 186200 | MSE Train Loss: 73.6317367553711 | MSE Test Loss: 70.14888763427734\n",
      "Epoch: 186300 | MSE Train Loss: 73.63172149658203 | MSE Test Loss: 70.1488265991211\n",
      "Epoch: 186400 | MSE Train Loss: 73.6317138671875 | MSE Test Loss: 70.14877319335938\n",
      "Epoch: 186500 | MSE Train Loss: 73.6316909790039 | MSE Test Loss: 70.14871215820312\n",
      "Epoch: 186600 | MSE Train Loss: 73.6316909790039 | MSE Test Loss: 70.1486587524414\n",
      "Epoch: 186700 | MSE Train Loss: 73.63167572021484 | MSE Test Loss: 70.14859008789062\n",
      "Epoch: 186800 | MSE Train Loss: 73.63166046142578 | MSE Test Loss: 70.1485366821289\n",
      "Epoch: 186900 | MSE Train Loss: 73.63166046142578 | MSE Test Loss: 70.14848327636719\n",
      "Epoch: 187000 | MSE Train Loss: 73.63163757324219 | MSE Test Loss: 70.14842224121094\n",
      "Epoch: 187100 | MSE Train Loss: 73.63163757324219 | MSE Test Loss: 70.14837646484375\n",
      "Epoch: 187200 | MSE Train Loss: 73.63162231445312 | MSE Test Loss: 70.14832305908203\n",
      "Epoch: 187300 | MSE Train Loss: 73.6316146850586 | MSE Test Loss: 70.14826202392578\n",
      "Epoch: 187400 | MSE Train Loss: 73.6316146850586 | MSE Test Loss: 70.14820861816406\n",
      "Epoch: 187500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14817810058594\n",
      "Epoch: 187600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14815521240234\n",
      "Epoch: 187700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 187800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 187900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 188400 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 188500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 188600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 188700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 188900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 189000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 189100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 189200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 189300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 189400 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 189500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 189600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 189700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 189800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 189900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 190000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190400 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 190800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 190900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191400 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 191900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192400 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192500 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192600 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192700 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192800 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 192900 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193000 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193100 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193200 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193300 | MSE Train Loss: 73.63159942626953 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 193900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 194900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 195900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 196900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 197900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 198900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 199900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 200900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 201000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813232421875\n",
      "Epoch: 201600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 201700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 201800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 201900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 202000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 202100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14813995361328\n",
      "Epoch: 202200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 202900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 203900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 204900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 205900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 206900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 207900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 208900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 209900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 210900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 211000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 211800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 211900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 212000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 212100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 212200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 212300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 212900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 213000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 213100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 213200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 213300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 213900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 214900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 215900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 216900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 217900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 218500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 218600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 218700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 218800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 218900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 219000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 219100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 219200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 219600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 219900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 220100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 220900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 221500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 221600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 221700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 221800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 221900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 222000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 222100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 222200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 222300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14812469482422\n",
      "Epoch: 222400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 222500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 222600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 222700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 222800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 222900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 223900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224800 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 224900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 225900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 226100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 226900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 227900 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228000 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 228900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14811706542969\n",
      "Epoch: 229000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229400 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229500 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229600 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229700 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 229900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230100 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230200 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230300 | MSE Train Loss: 73.63158416748047 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 230900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 231900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 232900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 233900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 234900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 235900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236300 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236400 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236500 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236600 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236700 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236800 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 236900 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237000 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237100 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237200 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 237300 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 237400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 237500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 237900 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238000 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238100 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238200 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238300 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238400 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238500 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238600 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 238700 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 238800 | MSE Train Loss: 73.63157653808594 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 238900 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239000 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239100 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239200 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 239300 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 239400 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239500 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239600 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239700 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239800 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 239900 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 240000 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240100 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240200 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240300 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 240500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 240600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 240900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241200 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241300 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241400 | MSE Train Loss: 73.6315689086914 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 241900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 242900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 243200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 243600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 243700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 243800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 243900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 244900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 245500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 245600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 245700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 245900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 246900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 247900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 248900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 249000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 249100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 249200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 249300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 249400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 249500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 249600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810943603516\n",
      "Epoch: 249700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 249800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 249900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 250900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 251900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 252900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 253900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 254000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 254600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 254700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 254800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 254900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 255900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 256900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 257900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 258900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 259900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 260500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 260700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 260900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 261200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 261300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.14810180664062\n",
      "Epoch: 261400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 261900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 262900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 263900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 264900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 265900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266700 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 266900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267000 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267100 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267200 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267300 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267400 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267500 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267600 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267800 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 267900 | MSE Train Loss: 73.63156127929688 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 268900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 269000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 269100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 269200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 269900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 270000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 270100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 270200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 270900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 271000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 271100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 271200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.1480941772461\n",
      "Epoch: 271300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 271900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 272000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 272100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 272200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 272300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 272400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 272500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 272600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 272700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 272800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 272900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 273000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 273100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 273900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 274900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 275900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 276900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 277900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 278900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 279300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 279900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 280900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14808654785156\n",
      "Epoch: 281800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 281900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 282900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 283900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284700 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284800 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 284900 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285000 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285100 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285200 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285300 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285400 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285500 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285600 | MSE Train Loss: 73.63155364990234 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 285900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 286900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 287900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 288900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 289900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 290900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 291600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 291700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 291800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 291900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 292900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 293900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 294800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 294900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 295700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 295800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 295900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296600 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296700 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296800 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 296900 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297000 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297500 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 297900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 298600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 298700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 298800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 298900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 299700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 299800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 299900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 300000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 300100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 300900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 301900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 302900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 303600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 303700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 303900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304100 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 304200 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 304300 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304400 | MSE Train Loss: 73.63154602050781 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 304700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 304800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.14807891845703\n",
      "Epoch: 304900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 305900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306600 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306700 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306800 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 306900 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307000 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307100 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307200 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307300 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307400 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307500 | MSE Train Loss: 73.63153839111328 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 307900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308400 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308500 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 308900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309400 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309500 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 309900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 310900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311400 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311500 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 311900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312400 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312500 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 312900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313200 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313300 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313400 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313500 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313600 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313700 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 313900 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314000 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 314100 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 314200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 314300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314800 | MSE Train Loss: 73.63153076171875 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 314900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 315300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 315400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 315900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 316900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 317000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 317100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 317200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 317400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 317900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 318000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 318100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 318200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480712890625\n",
      "Epoch: 318400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 318900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 319900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 320900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 321900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 322900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 323900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 324900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 325200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 325900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 326900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 327000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 327100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 327200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 327300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 327400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 327500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 327600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 327700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 327800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 327900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 328900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 329900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 330900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 331000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 331100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 331200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 331300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 331400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 331500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 331600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 331700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 331800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 331900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332300 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332400 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332500 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 332600 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 332700 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332800 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 332900 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333000 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333100 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333200 | MSE Train Loss: 73.63152313232422 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 333500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 333600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 333700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 333800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 333900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 334000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 334100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 334200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 334300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14806365966797\n",
      "Epoch: 334400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 334500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 334600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 334700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 334800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 334900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 335900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 336900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 337900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 338900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 339900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 340900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 341900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 342900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 343900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 344800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 344900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 345000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 345100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 345200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 345800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 345900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 346000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14805603027344\n",
      "Epoch: 346100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 346900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 347000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 347400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 347900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 348900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 349900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 350900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 351100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 351700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 351900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 352900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 353200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 353400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 353500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 353600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 353900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 354000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 354100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 354200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.1480484008789\n",
      "Epoch: 354300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 354900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355800 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 355900 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356000 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356100 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356200 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356300 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356400 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356500 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356600 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356700 | MSE Train Loss: 73.63151550292969 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 356900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 357900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 358900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 359900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 360900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 361900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 362900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363200 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363300 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363400 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363500 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363600 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363800 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 363900 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364000 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 364900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 365900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 366900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 367900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368700 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 368900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369100 | MSE Train Loss: 73.63150787353516 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 369900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 370900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 371900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372400 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 372900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373500 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373600 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373800 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 373900 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374000 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374100 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374200 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374300 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374400 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374700 | MSE Train Loss: 73.63150024414062 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 374900 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375000 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375100 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375200 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375300 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375400 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 375900 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 376000 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 376100 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 376200 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 376300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 376900 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 377000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 377100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 377200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 377300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 377400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 377500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 377600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 377700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 377800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 377900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 378000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 378100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 378200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 378300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 378900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379000 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379100 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379200 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379300 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379400 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 379900 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380000 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380100 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380200 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380300 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380400 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 380600 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 380700 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 380800 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 380900 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381000 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381100 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381200 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 381300 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381400 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381500 | MSE Train Loss: 73.6314926147461 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 381900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 382400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 382500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14804077148438\n",
      "Epoch: 382600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 382900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 383900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 384900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 385900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 386800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 386900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 387000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 387500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 387600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 387900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 388000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 388200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 388900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 389300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 389900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 390900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 391900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 392500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 392600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 392700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 392800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 392900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 393000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 393100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 393900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 394900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 395600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 395700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 395900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 396900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 397900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 398200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 398300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14803314208984\n",
      "Epoch: 398400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 398900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 399900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 400900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 401900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 402900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 403900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 404500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 404600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 404700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 404800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 404900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 405000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 405100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 405200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 405300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 405900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 406000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 406100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 406200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 406300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14802551269531\n",
      "Epoch: 406400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 406500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 406600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 406700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 406800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 406900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 407900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 408900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 409900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 410600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 410700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 410800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 410900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 411000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 411100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 411200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411400 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411500 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411700 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411800 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 411900 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 412000 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 412100 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 412200 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 412300 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412600 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412700 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 412900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413100 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 413200 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413300 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 413600 | MSE Train Loss: 73.63148498535156 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 413700 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 413800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 413900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 414000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 414100 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 414200 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 414300 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 414400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 414500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 414600 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801788330078\n",
      "Epoch: 414700 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 414800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 414900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415100 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415200 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415300 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415600 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415700 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 415900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416100 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416200 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416300 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 416400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 416500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416600 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416700 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 416800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 416900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 417000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 417100 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417200 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417300 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417400 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417500 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417600 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417700 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417800 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 417900 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418000 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418100 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418200 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418300 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418400 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418500 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418600 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 418700 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 418800 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 418900 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419000 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419100 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419200 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419300 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419400 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419500 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419600 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419700 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419800 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 419900 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 420000 | MSE Train Loss: 73.63147735595703 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 420100 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420200 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420300 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420400 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420700 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 420800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 420900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 421000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 421100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 421200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14801025390625\n",
      "Epoch: 421300 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421500 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421600 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421700 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421800 | MSE Train Loss: 73.6314697265625 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 421900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 422900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 423900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 424900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 425900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 426900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 427900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 428900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 429900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 430900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 431000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 431100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 431200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 431300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 431400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 431500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 431600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 431700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 431800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 431900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 432600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 432700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 432800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 432900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 433300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 433400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 433900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 434000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 434100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 434900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 435600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 435700 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 435800 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 435900 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436000 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436300 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436400 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436500 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436600 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436700 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 436900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 437000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 437100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 437200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 437300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 437900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438800 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 438900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 439800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 439900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 440600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 440700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 440800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 440900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 441900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 442800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 442900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 443900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 444900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 445300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 445400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 445500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 445900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 446500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 446600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 446700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 446800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 446900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447800 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 447900 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448000 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448100 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448200 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448300 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448400 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448500 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448600 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448700 | MSE Train Loss: 73.63146209716797 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448800 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 448900 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449000 | MSE Train Loss: 73.63145446777344 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14800262451172\n",
      "Epoch: 449900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 450900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 451900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 452900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 453900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 454900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 455900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 456900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 457900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 458900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 459900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 460900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 461900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 462900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 463000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 463100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 463200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 463300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 463900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 464000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 464100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 464200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 464300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464800 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 464900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 465000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 465100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 465200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 465300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 465400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 465500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 465600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 465700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14799499511719\n",
      "Epoch: 465800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 465900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 466900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467800 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 467900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 468900 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469000 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469100 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469200 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469300 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469400 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469500 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469600 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469700 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469800 | MSE Train Loss: 73.6314468383789 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 469900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 470600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 470700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 470800 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 470900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 471000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 471100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 471200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471800 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14798736572266\n",
      "Epoch: 471900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472800 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 472900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 473900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 474900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 475900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 476000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 476100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 476200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 476300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 476400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 476500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.14797973632812\n",
      "Epoch: 476600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 476700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 476800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 476900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477000 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477100 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477200 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477300 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477400 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477500 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477600 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 477900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 478900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 479900 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 480900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 481900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 482900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 483900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 484900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485700 | MSE Train Loss: 73.63143920898438 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 485900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 486900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 487900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 488900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489000 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489600 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489700 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489800 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 489900 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490100 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490200 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490300 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490400 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490500 | MSE Train Loss: 73.63143157958984 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 490900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 491900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 492900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 493900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 494900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 495900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 496900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 497900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 498900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499000 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499100 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499200 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499300 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499400 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499500 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499600 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499700 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499800 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n",
      "Epoch: 499900 | MSE Train Loss: 73.63142395019531 | MSE Test Loss: 70.1479721069336\n"
     ]
    }
   ],
   "source": [
    "#Set number of epochs\n",
    "epochs = 500000\n",
    "\n",
    "#Create lists to track loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ###Training\n",
    "    model_0.train()\n",
    "    y_pred = model_0(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ###Testing\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        test_pred = model_0(x_test)\n",
    "        test_loss = loss_fn(test_pred, y_test.type(torch.float))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MSE Train Loss: {loss} | MSE Test Loss: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f5c32a2ca60>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and Test Loss Curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[  1.7667,   1.1566, 101.0675, 402.7879,  60.0160]])), ('linear.bias', tensor([2.7109]))])\n"
     ]
    }
   ],
   "source": [
    "# Find the model's learned parameters\n",
    "print(model_0.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHFCAYAAAAUpjivAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABIuUlEQVR4nO3de1hVVeL/8c+Ru4iIqBwwUlI0DUxTM+2CpWKWWuO3sTJL8/JYmhOJY5n1DZ3CycbLfL10+5l4SXFmyupbTamTUqaOSPrzUuN08R5EKQoaAcL6/dGPPR5uAgIHNu/X8+zn8ay99tlrsw+eD2uvtbfDGGMEAABgU03c3QAAAIDaRNgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RthBo5aUlCSHw2Etnp6euuKKK/Twww/r5MmTddKG9u3ba+zYsdbrrVu3yuFwaOvWrVV6n+3btyshIUFnzpyp0fZJ0tixY9W+fftL1uvfv7/Lz/PipTLbN2TF56148fb2VuvWrXXjjTdq1qxZOnr0aKltij9/R44cqdK+EhMT9c4771Rpm7L21b9/f0VFRVXpfS7lww8/VEJCQpnrSn7Wgbri6e4GAPXBihUrdPXVVys3N1effvqp5s6dq5SUFO3fv1/+/v512pbrrrtOO3bsUNeuXau03fbt2zV79myNHTtWLVq0qJ3GVcJVV12lN998s1S5j4+PG1pT9xITE3XrrbeqsLBQp06d0j//+U+98cYbWrhwoV5//XU98MADVt0777xTO3bsUGhoaJX3cc899+juu++u9DbV3VdVffjhh1q6dGmZgWfDhg1q3rx5re4fKAthB5AUFRWlXr16SZL1RfWHP/xB77zzjsuX08V+/vlnNW3atMbb0rx5c91www01/r51xc/Pr960v7bOUUUiIyNdjn/48OGKj4/XwIEDNXbsWHXr1k3R0dGSpNatW6t169a12p7c3Fz5+vrWyb4upUePHm7dPxovLmMBZSj+siq+9DB27Fg1a9ZM+/fvV2xsrAICAjRgwABJUn5+vp5//nldffXV8vHxUevWrfXwww/rxx9/dHnPgoICzZgxQ06nU02bNtVNN92kXbt2ldp3eZex/vnPf2rYsGEKDg6Wr6+vOnTooLi4OElSQkKCfv/730uSIiIirEspF7/H+vXr1bdvX/n7+6tZs2YaPHiw9uzZU2r/SUlJ6ty5s3x8fNSlSxetWrWqWj/DihRfUtmyZYseffRRtWrVSsHBwRoxYoS+//77UvUr0/aKztGZM2c0fvx4tWzZUs2aNdOdd96p7777Tg6Hw+qB+Oyzz+RwOLRu3bpS+1+1apUcDodSU1OrdbwtW7bUq6++qgsXLmjhwoWlfg4XX1ras2ePhg4dqjZt2sjHx0dhYWG68847deLECUmSw+HQ+fPntXLlSus89+/f3+X9Nm7cqHHjxql169Zq2rSp8vLyKrxk9tlnn+mGG26Qn5+f2rZtq2effVaFhYXW+vI+k0eOHJHD4VBSUpKkX8/B0qVLrXYWL8X7LOsy1rFjxzR69GjreLt06aL58+erqKio1H7+9Kc/acGCBYqIiFCzZs3Ut29f7dy5swpnAo0VPTtAGb755htJcvlLOD8/X8OHD9ekSZP01FNP6cKFCyoqKtJdd92lzz77TDNmzFC/fv109OhRPffcc+rfv792794tPz8/SdLEiRO1atUqTZ8+XYMGDdKBAwc0YsQI5eTkXLI9H3/8sYYNG6YuXbpowYIFuvLKK3XkyBFt3LhRkjRhwgSdPn1aixcv1ttvv21dqii+FJaYmKhnnnlGDz/8sJ555hnl5+frpZde0s0336xdu3ZZ9ZKSkvTwww/rrrvu0vz583X27FklJCQoLy9PTZpU/m+jCxculCpr0qRJqfeYMGGC7rzzTq1du1bHjx/X73//e40ePVqffPKJVaeyba/oHA0bNky7d+9WQkKCdZnw9ttvd2nLzTffrB49emjp0qW6//77XdYtWbJEvXv3Vu/evSv9Myipd+/eCg0N1aefflpunfPnz2vQoEGKiIjQ0qVLFRISooyMDG3ZssX6nOzYsUO33Xabbr31Vj377LOSVOrS0Lhx43TnnXdq9erVOn/+vLy8vMrdZ0ZGhu677z499dRTmjNnjj744AM9//zzysrK0pIlS6p0jM8++6zOnz+vv/3tb9qxY4dVXt6lsx9//FH9+vVTfn6+/vCHP6h9+/Z6//33NX36dH377bdatmyZS/2lS5fq6quv1qJFi6z93XHHHTp8+LACAwOr1FY0MgZoxFasWGEkmZ07d5qCggKTk5Nj3n//fdO6dWsTEBBgMjIyjDHGjBkzxkgyb7zxhsv269atM5LMW2+95VKemppqJJlly5YZY4z56quvjCTzxBNPuNR78803jSQzZswYq2zLli1GktmyZYtV1qFDB9OhQweTm5tb7rG89NJLRpI5fPiwS/mxY8eMp6enmTp1qkt5Tk6OcTqdZuTIkcYYYwoLC01YWJi57rrrTFFRkVXvyJEjxsvLy7Rr167cfReLiYkxkspcxo8fb9Ur/rlPnjzZZft58+YZSSY9Pb1KbTem/HP0wQcfGEnm5ZdfdimfO3eukWSee+65Uu3as2ePVbZr1y4jyaxcubLCYy8+b3/961/LrdOnTx/j5+dXan/F52z37t1GknnnnXcq3Je/v7/LZ6bk+z300EPlrrv481F8vt59912XuhMnTjRNmjQxR48edTm2iz+Txhhz+PBhI8msWLHCKpsyZYop76ulXbt2Lu1+6qmnjCTzz3/+06Xeo48+ahwOhzl06JDLfqKjo82FCxesesXnZt26dWXuDyjGZSxAv1628vLyUkBAgIYOHSqn06m///3vCgkJcan3X//1Xy6v33//fbVo0ULDhg3ThQsXrKV79+5yOp1Wt/+WLVskqdT4n5EjR8rTs+IO1n//+9/69ttvNX78ePn6+lb52D7++GNduHBBDz30kEsbfX19FRMTY7Xx0KFD+v777zVq1Cg5HA5r+3bt2qlfv36V3l+HDh2UmppaainuhbjY8OHDXV5369ZN0n8uH1a27RcreY5SUlIk/fqzvljJ3pvisjZt2liXYiRp8eLFat26te69995KHH3FjDEVru/YsaOCgoL05JNP6pVXXtGXX35Zrf2U/BlUJCAgoNR5GDVqlIqKiirshaoJn3zyibp27arrr7/epXzs2LEyxrj08Em/DrL28PCwXpf8vADl4TIWoF/HZHTp0kWenp4KCQkps9u9adOmpS4X/PDDDzpz5oy8vb3LfN+ffvpJknTq1ClJktPpdFnv6emp4ODgCttWPPbniiuuqNzBlPDDDz9IUrmXYIovLZXXxuKyyk6P9vX1tQZ7X0rJYy+esZWbmyup8m0vVtY5OnXqlDw9PdWyZUuX8pJBtnj/kyZN0vz58/XSSy+poKBAf/nLXzRt2rQamU127NgxhYWFlbs+MDBQKSkpeuGFF/T0008rKytLoaGhmjhxop555pkKL0ddrCozrsr6ORR/Boo/E7Xl1KlTZd6SoPhnVHL/l/q8AOUh7ACSunTpcskv6It7O4oVD6z96KOPytwmICBA0n/+k87IyFDbtm2t9RcuXLjkF0rxuKHiAapV1apVK0nS3/72N7Vr167cehe3saSyyupCZdterKxzFBwcrAsXLuj06dMugae8Y3r00Uf1xz/+UW+88YZ++eUXXbhwQY888kg1j+A/du3apYyMDI0fP77CetHR0UpOTpYxRvv27VNSUpLmzJkjPz8/PfXUU5XaV1k/h/IUB8qLFf9sij8TxT2KeXl5LvWKw3x1BQcHKz09vVR58SD14vMPXC4uYwGXYejQoTp16pQKCwvVq1evUkvnzp0lyZotU/L+M3/5y1/KHMx7sU6dOqlDhw564403Sn3ZXKy8v3IHDx4sT09Pffvtt2W2sTjkde7cWaGhoVq3bp3L5ZajR49q+/btlfuB1LDKtr0iMTExkn6d0XWx5OTkMuuHhobqt7/9rZYtW6ZXXnlFw4YN05VXXnlZx3H69Gk98sgj8vLy0hNPPFGpbRwOh6699lotXLhQLVq00BdffGGt8/HxqbHejJycHL333nsuZWvXrlWTJk10yy23SJLV+7Jv3z6XeiW3K26bVLnelgEDBujLL790OTbpP7Pfbr311kofB1ARenaAy3DffffpzTff1B133KHHH39c119/vby8vHTixAlt2bJFd911l37zm9+oS5cuGj16tBYtWiQvLy8NHDhQBw4c0J/+9KdK3WRt6dKlGjZsmG644QY98cQTuvLKK3Xs2DF9/PHHVoAqvnfLn//8Z40ZM0ZeXl7q3Lmz2rdvrzlz5mjWrFn67rvvdPvttysoKEg//PCDdu3aJX9/f82ePVtNmjTRH/7wB02YMEG/+c1vNHHiRJ05c0YJCQllXtoqT25ubrnTgat6/53Ktr0it99+u2688UbFx8crOztbPXv21I4dO6wp9WXNMnv88cfVp08fSb/ecLIqvv76a+3cuVNFRUXWTQWXL1+u7OxsrVq1Stdcc025277//vtatmyZ7r77bl111VUyxujtt9/WmTNnNGjQIKtedHS0tm7dqv/93/9VaGioAgICrGBdVcHBwXr00Ud17NgxderUSR9++KFef/11Pfroo1bIczqdGjhwoObOnaugoCC1a9dO//jHP/T222+Xer/iz+GLL76oIUOGyMPDQ926dSvzUu8TTzyhVatW6c4779ScOXPUrl07ffDBB1q2bJkeffRRderUqVrHBJTi1uHRgJsVz1BJTU2tsN6YMWOMv79/mesKCgrMn/70J3PttdcaX19f06xZM3P11VebSZMmma+//tqql5eXZ+Lj402bNm2Mr6+vueGGG8yOHTtKzVApb+bLjh07zJAhQ0xgYKDx8fExHTp0KDW7a+bMmSYsLMw0adKk1Hu888475tZbbzXNmzc3Pj4+pl27duaee+4xmzdvdnmP//N//o+JjIw03t7eplOnTuaNN94wY8aMuezZWJJMQUGBMab8n3t5x16Ztld0jk6fPm0efvhh06JFC9O0aVMzaNAgs3PnTiPJ/PnPfy5zm/bt25suXbpc8phLtr148fT0NMHBwaZv377m6aefNkeOHCm1TckZUv/617/M/fffbzp06GD8/PxMYGCguf76601SUpLLdnv37jU33nijadq0qZFkYmJiXN6vrM9zebOxrrnmGrN161bTq1cv4+PjY0JDQ83TTz9tnati6enp5p577jEtW7Y0gYGBZvTo0dbssYtnY+Xl5ZkJEyaY1q1bG4fD4bLPkp91Y4w5evSoGTVqlAkODjZeXl6mc+fO5qWXXjKFhYVWneLZWC+99FKp41KJGXVAWRzGXGJ6AADY0Nq1a/XAAw/o888/LzXbbN++fbr22mu1dOlSTZ482U0tBFBTCDsAbG/dunU6efKkoqOj1aRJE+3cuVMvvfSSevToYU1Nl6Rvv/1WR48e1dNPP61jx47pm2++qfPHTQCoeYzZAWB7AQEBSk5O1vPPP6/z588rNDRUY8eO1fPPP+9S7w9/+INWr16tLl266K9//StBB7AJenYAAICtMfUcAADYGmEHAADYGmEHAADYGgOUJRUVFen7779XQEBAlW6zDgAA3McYo5ycHIWFhZV5g9BihB39+hyW8PBwdzcDAABUw/Hjxyt8WDJhR/95WOPx48crdet+AADgftnZ2QoPD7e+x8tD2NF/nhDcvHlzwg4AAA3MpYagMEAZAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYGmEHAADYmqe7G4CynTyTq6zz+dbrIH9vtW3h58YWAQDQMBF26qGTZ3I1cH6KcgsKrTI/Lw9tjo8h8AAAUEWEnXoo63y+cgsKteje7urYppm+yTynuPV7lXU+n7ADAEAVEXbqsY5tmimqbaC7mwEAQIPGAGUAAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrhB0AAGBrbg077du3l8PhKLVMmTJFkmSMUUJCgsLCwuTn56f+/fvr4MGDLu+Rl5enqVOnqlWrVvL399fw4cN14sQJdxwOAACoh9wadlJTU5Wenm4tmzZtkiT99re/lSTNmzdPCxYs0JIlS5Samiqn06lBgwYpJyfHeo+4uDht2LBBycnJ2rZtm86dO6ehQ4eqsLDQLccEAADqF7eGndatW8vpdFrL+++/rw4dOigmJkbGGC1atEizZs3SiBEjFBUVpZUrV+rnn3/W2rVrJUlnz57V8uXLNX/+fA0cOFA9evTQmjVrtH//fm3evNmdhwYAAOqJejNmJz8/X2vWrNG4cePkcDh0+PBhZWRkKDY21qrj4+OjmJgYbd++XZKUlpamgoIClzphYWGKioqy6pQlLy9P2dnZLgsAALCnehN23nnnHZ05c0Zjx46VJGVkZEiSQkJCXOqFhIRY6zIyMuTt7a2goKBy65Rl7ty5CgwMtJbw8PAaPBIAAFCf1Juws3z5cg0ZMkRhYWEu5Q6Hw+W1MaZUWUmXqjNz5kydPXvWWo4fP179hgMAgHqtXoSdo0ePavPmzZowYYJV5nQ6JalUD01mZqbV2+N0OpWfn6+srKxy65TFx8dHzZs3d1kAAIA91Yuws2LFCrVp00Z33nmnVRYRESGn02nN0JJ+HdeTkpKifv36SZJ69uwpLy8vlzrp6ek6cOCAVQcAADRunu5uQFFRkVasWKExY8bI0/M/zXE4HIqLi1NiYqIiIyMVGRmpxMRENW3aVKNGjZIkBQYGavz48YqPj1dwcLBatmyp6dOnKzo6WgMHDnTXIQEAgHrE7WFn8+bNOnbsmMaNG1dq3YwZM5Sbm6vJkycrKytLffr00caNGxUQEGDVWbhwoTw9PTVy5Ejl5uZqwIABSkpKkoeHR10eBgAAqKfcHnZiY2NljClzncPhUEJCghISEsrd3tfXV4sXL9bixYtrqYUAAKAhqxdjdgAAAGoLYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANgaYQcAANia28POyZMnNXr0aAUHB6tp06bq3r270tLSrPXGGCUkJCgsLEx+fn7q37+/Dh486PIeeXl5mjp1qlq1aiV/f38NHz5cJ06cqOtDAQAA9ZBbw05WVpZuvPFGeXl56e9//7u+/PJLzZ8/Xy1atLDqzJs3TwsWLNCSJUuUmpoqp9OpQYMGKScnx6oTFxenDRs2KDk5Wdu2bdO5c+c0dOhQFRYWuuGoAABAfeLpzp2/+OKLCg8P14oVK6yy9u3bW/82xmjRokWaNWuWRowYIUlauXKlQkJCtHbtWk2aNElnz57V8uXLtXr1ag0cOFCStGbNGoWHh2vz5s0aPHhwnR5TdZw8k6us8/nW628yz7mxNQAA2Itbw857772nwYMH67e//a1SUlLUtm1bTZ48WRMnTpQkHT58WBkZGYqNjbW28fHxUUxMjLZv365JkyYpLS1NBQUFLnXCwsIUFRWl7du3lxl28vLylJeXZ73Ozs6uxaOs2MkzuRo4P0W5Ba69UH5eHgry93ZTqwAAsA+3hp3vvvtOL7/8sqZNm6ann35au3bt0u9+9zv5+PjooYceUkZGhiQpJCTEZbuQkBAdPXpUkpSRkSFvb28FBQWVqlO8fUlz587V7Nmza+GIqi7rfL5yCwq16N7u6timmVUe5O+tti383NgyAADswa1hp6ioSL169VJiYqIkqUePHjp48KBefvllPfTQQ1Y9h8Phsp0xplRZSRXVmTlzpqZNm2a9zs7OVnh4eHUPo0Z0bNNMUW0Da/x9S14ikwhSAIDGxa1hJzQ0VF27dnUp69Kli9566y1JktPplPRr701oaKhVJzMz0+rtcTqdys/PV1ZWlkvvTmZmpvr161fmfn18fOTj41Ojx1IfVXSJbHN8DIEHANAouHU21o033qhDhw65lP373/9Wu3btJEkRERFyOp3atGmTtT4/P18pKSlWkOnZs6e8vLxc6qSnp+vAgQPlhp3G4uJLZO9PvUnvT71Ji+7trtyCwlK9PQAA2JVbe3aeeOIJ9evXT4mJiRo5cqR27dql1157Ta+99pqkXy9fxcXFKTExUZGRkYqMjFRiYqKaNm2qUaNGSZICAwM1fvx4xcfHKzg4WC1bttT06dMVHR1tzc5q7GrrEhkAAA2BW8NO7969tWHDBs2cOVNz5sxRRESEFi1apAceeMCqM2PGDOXm5mry5MnKyspSnz59tHHjRgUEBFh1Fi5cKE9PT40cOVK5ubkaMGCAkpKS5OHh4Y7DAgAA9Yhbw44kDR06VEOHDi13vcPhUEJCghISEsqt4+vrq8WLF2vx4sW10EIAANCQuf1xEQAAALWJsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGyNsAMAAGzNrWEnISFBDofDZXE6ndZ6Y4wSEhIUFhYmPz8/9e/fXwcPHnR5j7y8PE2dOlWtWrWSv7+/hg8frhMnTtT1oQAAgHrK7T0711xzjdLT061l//791rp58+ZpwYIFWrJkiVJTU+V0OjVo0CDl5ORYdeLi4rRhwwYlJydr27ZtOnfunIYOHarCwkJ3HA4AAKhnPN3eAE9Pl96cYsYYLVq0SLNmzdKIESMkSStXrlRISIjWrl2rSZMm6ezZs1q+fLlWr16tgQMHSpLWrFmj8PBwbd68WYMHD67TYwEAAPWP23t2vv76a4WFhSkiIkL33XefvvvuO0nS4cOHlZGRodjYWKuuj4+PYmJitH37dklSWlqaCgoKXOqEhYUpKirKqlOWvLw8ZWdnuywAAMCe3Bp2+vTpo1WrVunjjz/W66+/royMDPXr10+nTp1SRkaGJCkkJMRlm5CQEGtdRkaGvL29FRQUVG6dssydO1eBgYHWEh4eXsNHBgAA6gu3hp0hQ4bov/7rvxQdHa2BAwfqgw8+kPTr5apiDofDZRtjTKmyki5VZ+bMmTp79qy1HD9+/DKOAgAA1Gduv4x1MX9/f0VHR+vrr7+2xvGU7KHJzMy0enucTqfy8/OVlZVVbp2y+Pj4qHnz5i4LAACwp3oVdvLy8vTVV18pNDRUERERcjqd2rRpk7U+Pz9fKSkp6tevnySpZ8+e8vLycqmTnp6uAwcOWHUAAEDj5tbZWNOnT9ewYcN05ZVXKjMzU88//7yys7M1ZswYORwOxcXFKTExUZGRkYqMjFRiYqKaNm2qUaNGSZICAwM1fvx4xcfHKzg4WC1bttT06dOty2IAAABuDTsnTpzQ/fffr59++kmtW7fWDTfcoJ07d6pdu3aSpBkzZig3N1eTJ09WVlaW+vTpo40bNyogIMB6j4ULF8rT01MjR45Ubm6uBgwYoKSkJHl4eLjrsAAAQD3i1rCTnJxc4XqHw6GEhAQlJCSUW8fX11eLFy/W4sWLa7h1AADADurVmB0AAICaRtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC2RtgBAAC25unuBsA9vsk8Z/07yN9bbVv4ubE1AADUHsJOIxPk7y0/Lw/Frd9rlfl5eWhzfAyBBwBgS4SdRqZtCz9tjo9R1vl8Sb/28MSt36us8/mEHQCALRF2GqG2LfwINgCARqNaA5SvuuoqnTp1qlT5mTNndNVVV112owAAAGpKtcLOkSNHVFhYWKo8Ly9PJ0+evOxGAQAA1JQqXcZ67733rH9//PHHCgwMtF4XFhbqH//4h9q3b19jjQMAALhcVQo7d999tyTJ4XBozJgxLuu8vLzUvn17zZ8/v8YaBwAAcLmqFHaKiookSREREUpNTVWrVq1qpVEAAAA1pVqzsQ4fPlzT7QAAAKgV1Z56/o9//EP/+Mc/lJmZafX4FHvjjTcuu2EAAAA1oVphZ/bs2ZozZ4569eql0NBQORyOmm4XAABAjahW2HnllVeUlJSkBx98sKbbAwAAUKOqdZ+d/Px89evXr6bbAgAAUOOqFXYmTJigtWvX1nRbAAAAaly1LmP98ssveu2117R582Z169ZNXl5eLusXLFhQI40DAAC4XNUKO/v27VP37t0lSQcOHHBZx2BlAABQn1TrMtaWLVvKXT755JNqNWTu3LlyOByKi4uzyowxSkhIUFhYmPz8/NS/f38dPHjQZbu8vDxNnTpVrVq1kr+/v4YPH64TJ05Uqw0AAMB+qhV2alpqaqpee+01devWzaV83rx5WrBggZYsWaLU1FQ5nU4NGjRIOTk5Vp24uDht2LBBycnJ2rZtm86dO6ehQ4eW+aBSAADQ+FTrMtatt95a4eWqqvTunDt3Tg888IBef/11Pf/881a5MUaLFi3SrFmzNGLECEnSypUrFRISorVr12rSpEk6e/asli9frtWrV2vgwIGSpDVr1ig8PFybN2/W4MGDq3N4AADARqrVs9O9e3dde+211tK1a1fl5+friy++UHR0dJXea8qUKbrzzjutsFLs8OHDysjIUGxsrFXm4+OjmJgYbd++XZKUlpamgoIClzphYWGKioqy6gAAgMatWj07CxcuLLM8ISFB586dq/T7JCcn64svvlBqamqpdRkZGZKkkJAQl/KQkBAdPXrUquPt7a2goKBSdYq3L0teXp7y8vKs19nZ2ZVuMwAAaFhqdMzO6NGjK/1crOPHj+vxxx/XmjVr5OvrW269kpfLjDGXnPF1qTpz585VYGCgtYSHh1eqzQAAoOGp0bCzY8eOCoPLxdLS0pSZmamePXvK09NTnp6eSklJ0f/8z//I09PT6tEp2UOTmZlprXM6ncrPz1dWVla5dcoyc+ZMnT171lqOHz9elcMEAAANSLUuYxUPGC5mjFF6erp2796tZ599tlLvMWDAAO3fv9+l7OGHH9bVV1+tJ598UldddZWcTqc2bdqkHj16SPr1MRUpKSl68cUXJUk9e/aUl5eXNm3apJEjR0qS0tPTdeDAAc2bN6/cffv4+MjHx6fSxwsAABquaoWdwMBAl9dNmjRR586dNWfOHJfBwhUJCAhQVFSUS5m/v7+Cg4Ot8ri4OCUmJioyMlKRkZFKTExU06ZNNWrUKKsd48ePV3x8vIKDg9WyZUtNnz5d0dHRpQY8AwCAxqlaYWfFihU13Y4yzZgxQ7m5uZo8ebKysrLUp08fbdy4UQEBAVadhQsXytPTUyNHjlRubq4GDBigpKQkeXh41Ekb69I3ma6Dv4P8vdW2hZ+bWgMAQMNQrbBTLC0tTV999ZUcDoe6du1qXW6qrq1bt7q8djgcSkhIUEJCQrnb+Pr6avHixVq8ePFl7bs+C/L3lp+Xh+LW73Up9/Py0Ob4GAIPAAAVqFbYyczM1H333aetW7eqRYsWMsbo7NmzuvXWW5WcnKzWrVvXdDsbtbYt/LQ5PkZZ5/Otsm8yzylu/V5lnc8n7AAAUIFqzcaaOnWqsrOzdfDgQZ0+fVpZWVk6cOCAsrOz9bvf/a6m2wj9Gnii2gZaS8c2zdzdJAAAGoRq9ex89NFH2rx5s7p06WKVde3aVUuXLq30AGUAAIC6UK2enaKiInl5eZUq9/LyUlFR0WU3CgAAoKZUK+zcdtttevzxx/X9999bZSdPntQTTzyhAQMG1FjjAAAALle1ws6SJUuUk5Oj9u3bq0OHDurYsaMiIiKUk5Nj61lRAACg4anWmJ3w8HB98cUX2rRpk/71r3/JGKOuXbtyIz8AAFDvVKln55NPPlHXrl2tp4QPGjRIU6dO1e9+9zv17t1b11xzjT777LNaaSgAAEB1VCnsLFq0SBMnTlTz5s1LrQsMDNSkSZO0YMGCGmscAADA5apS2Pm///f/6vbbby93fWxsrNLS0i67UQAAADWlSmHnhx9+KHPKeTFPT0/9+OOPl90oAACAmlKlsNO2bVvt37+/3PX79u1TaGjoZTcKAACgplQp7Nxxxx367//+b/3yyy+l1uXm5uq5557T0KFDa6xxAAAAl6tKU8+feeYZvf322+rUqZMee+wxde7cWQ6HQ1999ZWWLl2qwsJCzZo1q7baiko4eSbXemDoN5nn3NwaAADcr0phJyQkRNu3b9ejjz6qmTNnyhgjSXI4HBo8eLCWLVumkJCQWmkoLu3kmVwNnJ+i3IJCq8zPy0NB/t5ubBUAAO5V5ZsKtmvXTh9++KGysrL0zTffyBijyMhIBQUF1Ub7UAVZ5/OVW1CoRfd2t56KHuTvrbYt/NzcMgAA3Kdad1CWpKCgIPXu3bsm24Ia0rFNM0W1DXR3MwAAqBeq9WwsAACAhoKwAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbI2wAwAAbM2tYefll19Wt27d1Lx5czVv3lx9+/bV3//+d2u9MUYJCQkKCwuTn5+f+vfvr4MHD7q8R15enqZOnapWrVrJ399fw4cP14kTJ+r6UAAAQD3l1rBzxRVX6I9//KN2796t3bt367bbbtNdd91lBZp58+ZpwYIFWrJkiVJTU+V0OjVo0CDl5ORY7xEXF6cNGzYoOTlZ27Zt07lz5zR06FAVFha667AAAEA94tawM2zYMN1xxx3q1KmTOnXqpBdeeEHNmjXTzp07ZYzRokWLNGvWLI0YMUJRUVFauXKlfv75Z61du1aSdPbsWS1fvlzz58/XwIED1aNHD61Zs0b79+/X5s2b3XloAACgnqg3Y3YKCwuVnJys8+fPq2/fvjp8+LAyMjIUGxtr1fHx8VFMTIy2b98uSUpLS1NBQYFLnbCwMEVFRVl1ypKXl6fs7GyXBQAA2JOnuxuwf/9+9e3bV7/88ouaNWumDRs2qGvXrlZYCQkJcakfEhKio0ePSpIyMjLk7e2toKCgUnUyMjLK3efcuXM1e/bsGj4S1Dcnz+Qq63y+S1mQv7fatvBzU4sAAO7g9rDTuXNn7d27V2fOnNFbb72lMWPGKCUlxVrvcDhc6htjSpWVdKk6M2fO1LRp06zX2dnZCg8Pr+YRoD46eSZXA+enKLfAdeyWn5eHNsfHEHgAoBFxe9jx9vZWx44dJUm9evVSamqq/vznP+vJJ5+U9GvvTWhoqFU/MzPT6u1xOp3Kz89XVlaWS+9OZmam+vXrV+4+fXx85OPjUxuHg3oi63y+cgsKteje7urYppkk6ZvMc4pbv1dZ5/MJOwDQiNSbMTvFjDHKy8tTRESEnE6nNm3aZK3Lz89XSkqKFWR69uwpLy8vlzrp6ek6cOBAhWEHjUfHNs0U1TZQUW0DrdADAGhc3Nqz8/TTT2vIkCEKDw9XTk6OkpOTtXXrVn300UdyOByKi4tTYmKiIiMjFRkZqcTERDVt2lSjRo2SJAUGBmr8+PGKj49XcHCwWrZsqenTpys6OloDBw5056EBAIB6wq1h54cfftCDDz6o9PR0BQYGqlu3bvroo480aNAgSdKMGTOUm5uryZMnKysrS3369NHGjRsVEBBgvcfChQvl6empkSNHKjc3VwMGDFBSUpI8PDzcdVgAAKAecWvYWb58eYXrHQ6HEhISlJCQUG4dX19fLV68WIsXL67h1gEAADuod2N2AAAAahJhBwAA2BphBwAA2BphBwAA2BphBwAA2Jrb76AM1JSLn4X1Tea5cutdvI5nZQGA/RF2YAtlPQvLz8tDQf7e1usgf2/5eXkobv1elzo8KwsA7I2wA1so61lYJXtt2rbw0+b4GJfeH56VBQD2R9iBrRQ/C6s8bVv4EWwAoJFhgDIAALA1wg4AALA1wg4AALA1wg4AALA1wg4AALA1wg4AALA1wg4AALA17rMDSaUfr8BjFAAAdkHYaeTKeoSCxGMUAAD2Qdhp5Eo+QkHiMQoAAHsh7IBHKAAAbI2w08BdPNam5LgbAABA2GmwKhprE+Tv7Z5GAQBQDxF2GqiyxtpIzKICAKAkwk4DxlgbAAAujbCDRo97DAGAvRF20GhxjyEAaBwIO2i0uMcQADQOhB00aox7AgD740GgAADA1gg7AADA1gg7AADA1gg7AADA1gg7AADA1gg7AADA1gg7AADA1twadubOnavevXsrICBAbdq00d13361Dhw651DHGKCEhQWFhYfLz81P//v118OBBlzp5eXmaOnWqWrVqJX9/fw0fPlwnTpyoy0MBcJlOnsnVgZNnXZaTZ3Ld3SwANuDWmwqmpKRoypQp6t27ty5cuKBZs2YpNjZWX375pfz9/SVJ8+bN04IFC5SUlKROnTrp+eef16BBg3To0CEFBARIkuLi4vS///u/Sk5OVnBwsOLj4zV06FClpaXJw8PDnYcIoBJOnsnVwPkpyi0odCnn0R0AaoJbw85HH33k8nrFihVq06aN0tLSdMstt8gYo0WLFmnWrFkaMWKEJGnlypUKCQnR2rVrNWnSJJ09e1bLly/X6tWrNXDgQEnSmjVrFB4ers2bN2vw4MF1flwAqibrfL5yCwq16N7u6timmSQe3QGg5tSrMTtnz56VJLVs2VKSdPjwYWVkZCg2Ntaq4+Pjo5iYGG3fvl2SlJaWpoKCApc6YWFhioqKsuqUlJeXp+zsbJcFgPt1bNNMUW0DFdU20Ao9AHC56k3YMcZo2rRpuummmxQVFSVJysjIkCSFhIS41A0JCbHWZWRkyNvbW0FBQeXWKWnu3LkKDAy0lvDw8Jo+HAAAUE/Um7Dz2GOPad++fVq3bl2pdQ6Hw+W1MaZUWUkV1Zk5c6bOnj1rLcePH69+wwEAQL1WL8LO1KlT9d5772nLli264oorrHKn0ylJpXpoMjMzrd4ep9Op/Px8ZWVllVunJB8fHzVv3txlAQAA9uTWsGOM0WOPPaa3335bn3zyiSIiIlzWR0REyOl0atOmTVZZfn6+UlJS1K9fP0lSz5495eXl5VInPT1dBw4csOoAVfVN5jmmP9eyi6eaf5N5zt3NAWBjbp2NNWXKFK1du1bvvvuuAgICrB6cwMBA+fn5yeFwKC4uTomJiYqMjFRkZKQSExPVtGlTjRo1yqo7fvx4xcfHKzg4WC1bttT06dMVHR1tzc4CKivI31t+Xh6KW7/XKmP6c80ra6q5n5eHgvy93dgqAHbl1rDz8ssvS5L69+/vUr5ixQqNHTtWkjRjxgzl5uZq8uTJysrKUp8+fbRx40brHjuStHDhQnl6emrkyJHKzc3VgAEDlJSUxD12UGVtW/hpc3yMss7nS2L6c20pa6p5kL83P2MAtcKtYccYc8k6DodDCQkJSkhIKLeOr6+vFi9erMWLF9dg69BYtW3hx5duHSmeag4AtaleDFAGAACoLW7t2QGq6+SZXOtSkyQGuAIAykXYQYNT0XOUGOAKACiJsIMGp6zBrRIDXAEAZSPsoMFicCsAoDIIOwBqHWOsALgTYQdArWKMFQB3I+wAqFWMsQLgboQdAHWCMVYA3IWbCgIAAFsj7AAAAFvjMhbKdfGMGcZXAAAaKsIOSgny95afl4fi1u+1yvy8PLQ5PobAgzpH6AZwuQg7KKVtCz9tjo+x7ovyTeY5xa3fq6zz+XzRoM4Quuu/kvdPIoyiviLsoExtW/jxnxbcitBdv5QMNqfO5+uR1Wku908ijKK+IuwAqLfKCt0l775Mb0Ltq+jGkCvHXa9gf2/CKOo1wg6ABqGsy1oSvQl1gRtDoqEj7ABoEEpe1pK4tFXXuDEkGirCDoAGg7FkAKqDmwoCAABbo2cHqAQGxdZv3Iun5l08+6rk5x9oaAg7QAUYFFu/cS+e2lHW7Cs/Lw8F+Xu7sVVA9RF2gAowKLZ66qpXgHvx1I6yZl9VtseMXlDUR4Qd4BIYFFs1dd0rwPmpPVWZfUUvKOozwg6AGnU5vQJwj5J3R65Obxy9oKjPCDsAagX3ZGkYKro7clV74+hlQ31F2EGDwMyQ+qsmegXgPtwdGY0BYQf1HjND6q+a7BWAe9ETBzsj7KDeYwxI/UWvAICGgLCDBoO/POsvzg0qwk0f4W6EHQBAreCmj6gvCDsAYGMlB5BLdde7wk0fUV8QdgDApioaQP7Kgz0V7O9d67PnmI6O+oCwAwA2VdYA8lPn8/XI6jSNeWOXVY/Zc7A7wg4A2FzJAeQl73TMoGHYXRN37vzTTz/VsGHDFBYWJofDoXfeecdlvTFGCQkJCgsLk5+fn/r376+DBw+61MnLy9PUqVPVqlUr+fv7a/jw4Tpx4kQdHgUANCxtW/gpqm2gtRB0YHduDTvnz5/XtddeqyVLlpS5ft68eVqwYIGWLFmi1NRUOZ1ODRo0SDk5OVaduLg4bdiwQcnJydq2bZvOnTunoUOHqrCwsMz3BGB/32Se04GTZ63l5JlcdzcJgBu59TLWkCFDNGTIkDLXGWO0aNEizZo1SyNGjJAkrVy5UiEhIVq7dq0mTZqks2fPavny5Vq9erUGDhwoSVqzZo3Cw8O1efNmDR48uM6OBYD78eRtAGWpt2N2Dh8+rIyMDMXGxlplPj4+iomJ0fbt2zVp0iSlpaWpoKDApU5YWJiioqK0ffv2csNOXl6e8vLyrNfZ2dm1dyAA6gxP3m6YSk6PZwwRalq9DTsZGRmSpJCQEJfykJAQHT161Krj7e2toKCgUnWKty/L3LlzNXv27BpuMYD6gKnODUt5z76jJw41ya1jdirD4XC4vDbGlCor6VJ1Zs6cqbNnz1rL8ePHa6StQGNw8kyuNRaGJ5zjcl08Pf79qTdp0b3dlVtQWOpGiMDlqLc9O06nU9KvvTehoaFWeWZmptXb43Q6lZ+fr6ysLJfenczMTPXr16/c9/bx8ZGPj08ttRywL55Aj9rC89VQm+pt2ImIiJDT6dSmTZvUo0cPSVJ+fr5SUlL04osvSpJ69uwpLy8vbdq0SSNHjpQkpaen68CBA5o3b57b2g7YFU+gr1/c+SgIoCFxa9g5d+6cvvnmG+v14cOHtXfvXrVs2VJXXnml4uLilJiYqMjISEVGRioxMVFNmzbVqFGjJEmBgYEaP3684uPjFRwcrJYtW2r69OmKjo62ZmcBqHn8Fe5+FT0Kor6Pd7n48ieXQlEX3Bp2du/erVtvvdV6PW3aNEnSmDFjlJSUpBkzZig3N1eTJ09WVlaW+vTpo40bNyogIMDaZuHChfL09NTIkSOVm5urAQMGKCkpSR4eHnV+PABQV8rqZavvM88qujUAl0JRm9wadvr37y9jTLnrHQ6HEhISlJCQUG4dX19fLV68WIsXL66FFgJA/daQetnKujWAxKU31L56O2YHqO8u7n7nP+v6j/NVP3BrALgDYQeoorK64hvCOInGivPVMJUcy0NAxeUg7KDS+M/nVyW74uv7OInGjvPVsPDID9QGwg4uif98SqMrvmFpbOer+A+ThjjTiUd+oDYQdnBJ/OcDNAzlXbJraDOdGls4Re0j7KBS+M8HqP/K+sOksV5uBi5G2AEAG+EPE6A0wo4bXHyL94Z4TR0AgIaEsFPHeJDipZV83g+BEABwOQg7dYwHKVasouf9EAgBANVB2HGThnSL97pUVhiUCISoedw3Cmg8CDuolwiDqC0N+b5RjX28H4/8QHURdgCUy47jpxrqfaMa83g/HvmBy0XYAVAmO4+faojTsxvzeD8e+YHLRdgBUCbGT7lXeb1qjfUSb0MMqKg/CDsAKtRYv1zdyc69aoA7EHYAoJ6hVw2oWYQdoIbYYSpzY5/tU9/QqwbUDMIOcJka8lTmizXm2T7FmNoM2BNhB7hMDXUqc0mNebaPu6c223GKf12wQ28q6gZhB6gBdpop0hgvnbhzajODkauuot7UVx7sqeD//3Mj/KAYYQcAVLnAWrIHpia+TBmMXHVl9aaeOp+vR1anacwbu6yyhnYpGbWHsAPUIsaA2Ed5Y5pq6su0MfaoXY6ywik3HkR5CDu4LHyZl83dY0BQMy7+fH+Tec6lB4Yv0/rHTpeTUbMIO6gWvswr1hBub8+g2PJVNCakd0TLenMOAVQOYQfV0hC+zN2tPv+VyaDYipU1JkSqXu9lyVBZ3fdB9TBjCxJhB5ehpr7MuZFd3WNQ7KVV9/N98ee5eNBsWaGSXtDaZZf7X6FmEHbgVtzIzr0YFFuzyvs8rxx3vTUdurgXNPXwaWX9/7E/qHl2uf8VagZhBzWqql3GjflGdrCfynyeyxvvRsCvefX5UjLqFmEHNeJyu4wbSw+DO8cPcLmwdpScsSVV/Hkuq8eBgA/ULsIOagRdxhVz9/gBLhfWvIrO6aV+rvQ4AHWLsIMaU5070DaWHgZ3h0EuF9a8mpyxBaB2EXZQ64oDTUUzUxpDD0N5YbAub8zYWC4X1hV6aICGgbCDWlPeQMyLZ6YU12uMXxg1eWPGsu7lcrHG0oMGVMal/sCojWegwb0IO6g1DMSsWHk3ZiyeklzsUj+z8m4QWFJj6UEDylOZPzBq+xlocA/bhJ1ly5bppZdeUnp6uq655hotWrRIN998s7ub1ejRzV+xi38+lR3EXNa4p7JuEFgSQRONXWX+wCjvGWgl/wgpiR6i+s0WYWf9+vWKi4vTsmXLdOONN+rVV1/VkCFD9OWXX+rKK690d/OASqnMIOaKHvPAM5uAS6vsHxjFv0/l1SnJz8tDrzzY07pEX9YYRXqI3McWYWfBggUaP368JkyYIElatGiRPv74Y7388suaO3eum1sHVN6lBjGX14vDX4xA1VVmRl15dS5WHGzGvLHLpfziMYrVvUyNmtHgw05+fr7S0tL01FNPuZTHxsZq+/btbmrVfzTWqdaoGeWNMaAXB6gZlbnUXpk6lwpNFfUiXdwjVJbKBKLaumRWmQfZXmqCRE22p7oafNj56aefVFhYqJCQEJfykJAQZWRklLlNXl6e8vLyrNdnz56VJGVnZ9do274/k6vhS7bpl4Iil3JfrybyLPxF2dmOGt0f7CegibRhYg+d+fk//5G0aOqtgCYFys4ucGPLAFwsoIkUEFDy//T//J6W9bt8+ucCxSXv0YMvb63wvX29mmjRfT3UsqlXmeuL3+fi75pLbVMZZb1vyfcur05Zx/DeYzcprIYDT/H3tjGmwnoNPuwUczhcP2TGmFJlxebOnavZs2eXKg8PD6+VtpWly0t1tisAQAN3ZzW+M6qzTW2+d21+7+Xk5CgwsPx7iDX4sNOqVSt5eHiU6sXJzMws1dtTbObMmZo2bZr1uqioSKdPn1ZwcHC5Aaks2dnZCg8P1/Hjx9W8efPqHQDqDOerYeF8NRycq4bFTufLGKOcnByFhYVVWK/Bhx1vb2/17NlTmzZt0m9+8xurfNOmTbrrrrvK3MbHx0c+Pj4uZS1atKh2G5o3b97gPzCNCeerYeF8NRycq4bFLueroh6dYg0+7EjStGnT9OCDD6pXr17q27evXnvtNR07dkyPPPKIu5sGAADczBZh595779WpU6c0Z84cpaenKyoqSh9++KHatWvn7qYBAAA3s0XYkaTJkydr8uTJdbpPHx8fPffcc6UuiaF+4nw1LJyvhoNz1bA0xvPlMJearwUAANCANXF3AwAAAGoTYQcAANgaYQcAANgaYQcAANgaYecyLFu2TBEREfL19VXPnj312WefubtJjV5CQoIcDofL4nQ6rfXGGCUkJCgsLEx+fn7q37+/Dh486MYWNy6ffvqphg0bprCwMDkcDr3zzjsu6ytzfvLy8jR16lS1atVK/v7+Gj58uE6cOFGHR9F4XOp8jR07ttTv2w033OBSh/NVN+bOnavevXsrICBAbdq00d13361Dhw651GnMv1+EnWpav3694uLiNGvWLO3Zs0c333yzhgwZomPHjrm7aY3eNddco/T0dGvZv3+/tW7evHlasGCBlixZotTUVDmdTg0aNEg5OTlubHHjcf78eV177bVasmRJmesrc37i4uK0YcMGJScna9u2bTp37pyGDh2qwsLCujqMRuNS50uSbr/9dpfftw8//NBlPeerbqSkpGjKlCnauXOnNm3apAsXLig2Nlbnz5+36jTq3y+Darn++uvNI4884lJ29dVXm6eeespNLYIxxjz33HPm2muvLXNdUVGRcTqd5o9//KNV9ssvv5jAwEDzyiuv1FELUUyS2bBhg/W6MufnzJkzxsvLyyQnJ1t1Tp48aZo0aWI++uijOmt7Y1TyfBljzJgxY8xdd91V7jacL/fJzMw0kkxKSooxht8venaqIT8/X2lpaYqNjXUpj42N1fbt293UKhT7+uuvFRYWpoiICN1333367rvvJEmHDx9WRkaGy3nz8fFRTEwM560eqMz5SUtLU0FBgUudsLAwRUVFcQ7dZOvWrWrTpo06deqkiRMnKjMz01rH+XKfs2fPSpJatmwpid8vwk41/PTTTyosLCz1VPWQkJBST19H3erTp49WrVqljz/+WK+//royMjLUr18/nTp1yjo3nLf6qTLnJyMjQ97e3goKCiq3DurOkCFD9Oabb+qTTz7R/PnzlZqaqttuu015eXmSOF/uYozRtGnTdNNNNykqKkoSv1+2eVyEOzgcDpfXxphSZahbQ4YMsf4dHR2tvn37qkOHDlq5cqU1cJLzVr9V5/xwDt3j3nvvtf4dFRWlXr16qV27dvrggw80YsSIcrfjfNWuxx57TPv27dO2bdtKrWusv1/07FRDq1at5OHhUSrpZmZmlkrNcC9/f39FR0fr66+/tmZlcd7qp8qcH6fTqfz8fGVlZZVbB+4TGhqqdu3a6euvv5bE+XKHqVOn6r333tOWLVt0xRVXWOWN/feLsFMN3t7e6tmzpzZt2uRSvmnTJvXr189NrUJZ8vLy9NVXXyk0NFQRERFyOp0u5y0/P18pKSmct3qgMuenZ8+e8vLycqmTnp6uAwcOcA7rgVOnTun48eMKDQ2VxPmqS8YYPfbYY3r77bf1ySefKCIiwmV9o//9ctvQ6AYuOTnZeHl5meXLl5svv/zSxMXFGX9/f3PkyBF3N61Ri4+PN1u3bjXfffed2blzpxk6dKgJCAiwzssf//hHExgYaN5++22zf/9+c//995vQ0FCTnZ3t5pY3Djk5OWbPnj1mz549RpJZsGCB2bNnjzl69KgxpnLn55FHHjFXXHGF2bx5s/niiy/MbbfdZq699lpz4cIFdx2WbVV0vnJyckx8fLzZvn27OXz4sNmyZYvp27evadu2LefLDR599FETGBhotm7datLT063l559/tuo05t8vws5lWLp0qWnXrp3x9vY21113nTXFD+5z7733mtDQUOPl5WXCwsLMiBEjzMGDB631RUVF5rnnnjNOp9P4+PiYW265xezfv9+NLW5ctmzZYiSVWsaMGWOMqdz5yc3NNY899php2bKl8fPzM0OHDjXHjh1zw9HYX0Xn6+effzaxsbGmdevWxsvLy1x55ZVmzJgxpc4F56tulHWeJJkVK1ZYdRrz75fDGGPqujcJAACgrjBmBwAA2BphBwAA2BphBwAA2BphBwAA2BphBwAA2BphBwAA2BphBwAA2BphBwD066MO2rRpoyNHjritDUuWLNHw4cPdtn/Argg7AKpk7NixcjgcpZbbb7/d3U27LHPnztWwYcPUvn17l/K33npLt912m4KCgtS0aVN17txZ48aN0549eyr1vvn5+WrVqpWef/75cvfbqlUr5efna+LEiUpNTS3zadUAqo+wA6DKbr/9dqWnp7ss69atq9V95ufn19p75+bmavny5ZowYYJL+ZNPPql7771X3bt313vvvaeDBw/qtddeU4cOHfT0009X6r29vb01evRoJSUlqawb1q9YsUIPPvigvL295ePjo1GjRmnx4sU1clwA/j83P64CQAMzZswYc9ddd1VYR5J5/fXXzd133238/PxMx44dzbvvvutS5+DBg2bIkCHG39/ftGnTxowePdr8+OOP1vqYmBgzZcoU88QTT5jg4GBzyy23GGOMeffdd03Hjh2Nr6+v6d+/v0lKSjKSTFZWljl37pwJCAgwf/3rX1329d5775mmTZuW+8DXt956y7Rq1cqlbMeOHUaS+fOf/1zmNkVFRaX2cd111xkfHx8TERFhEhISTEFBgTHGmH379hlJZuvWrS7bfPrpp0aSy/OJtm7dary9vV0e4Ajg8tCzA6BWzJ49WyNHjtS+fft0xx136IEHHtDp06clSenp6YqJiVH37t21e/duffTRR/rhhx80cuRIl/dYuXKlPD099fnnn+vVV1/VkSNHdM899+juu+/W3r17NWnSJM2aNcuq7+/vr/vuu08rVqxweZ8VK1bonnvuUUBAQJlt/fTTT9WrVy+XsnXr1qlZs2aaPHlymds4HA7r3x9//LFGjx6t3/3ud/ryyy/16quvKikpSS+88IIkKTo6Wr179y7VrjfeeEPXX3+9oqKirLJevXqpoKBAu3btKnO/AKrB3WkLQMMyZswY4+HhYfz9/V2WOXPmWHUkmWeeecZ6fe7cOeNwOMzf//53Y4wxzz77rImNjXV53+PHjxtJ5tChQ8aYX3t2unfv7lLnySefNFFRUS5ls2bNsnp2jDHmn//8p/Hw8DAnT540xhjz448/Gi8vr1K9Khe76667zLhx41zKbr/9dtOtWzeXsvnz57sc85kzZ4wxxtx8880mMTHRpe7q1atNaGio9frll182/v7+JicnxxhjTE5OjvH39zevvvpqqfYEBQWZpKSkctsLoGro2QFQZbfeeqv27t3rskyZMsWlTrdu3ax/+/v7KyAgQJmZmZKktLQ0bdmyRc2aNbOWq6++WpL07bffWtuV7G05dOiQevfu7VJ2/fXXl3p9zTXXaNWqVZKk1atX68orr9Qtt9xS7vHk5ubK19e3VPnFvTeSNG7cOO3du1evvvqqzp8/b43BSUtL05w5c1yOZ+LEiUpPT9fPP/8sSbr//vtVVFSk9evXS5LWr18vY4zuu+++Uvv18/OztgNw+Tzd3QAADY+/v786duxYYR0vLy+X1w6HQ0VFRZKkoqIiDRs2TC+++GKp7UJDQ132czFjTKkAYsoY9DthwgQtWbJETz31lFasWKGHH3641HYXa9WqlbKyslzKIiMjtW3bNhUUFFjH0qJFC7Vo0UInTpxwqVtUVKTZs2drxIgRpd67OEQFBgbqnnvu0YoVKzR+/Hjr0lrz5s1LbXP69Gm1bt263PYCqBp6dgDUueuuu04HDx5U+/bt1bFjR5elZMC52NVXX63U1FSXst27d5eqN3r0aB07dkz/8z//o4MHD2rMmDEVtqdHjx768ssvXcruv/9+nTt3TsuWLavU8Rw6dKjUsXTs2FFNmvznv9nx48fr888/1/vvv6/PP/9c48ePL/Ve3377rX755Rf16NHjkvsFUDmEHQBVlpeXp4yMDJflp59+qvT2U6ZM0enTp3X//fdr165d+u6777Rx40aNGzdOhYWF5W43adIk/etf/9KTTz6pf//73/rLX/6ipKQkSa6XnIKCgjRixAj9/ve/V2xsrK644ooK2zN48GAdPHjQpXenb9++io+PV3x8vKZNm6Zt27bp6NGj2rlzp5YvXy6Hw2EFmf/+7//WqlWrlJCQoIMHD+qrr77S+vXr9cwzz7jsJyYmRh07dtRDDz2kjh07lnlp7bPPPtNVV12lDh06XPLnCKByCDsAquyjjz5SaGioy3LTTTdVevuwsDB9/vnnKiws1ODBgxUVFaXHH39cgYGBLj0hJUVEROhvf/ub3n77bXXr1k0vv/yyNRvLx8fHpe748eOVn5+vcePGXbI90dHR6tWrl/7yl7+4lP/pT3/S2rVrtWfPHg0dOlSRkZH67W9/q6KiIu3YscO6BDV48GC9//772rRpk3r37q0bbrhBCxYsULt27Urta9y4ccrKyiq3XevWrdPEiRMv2WYAlecwZV3wBoAG4oUXXtArr7yi48ePu5S/+eabevzxx/X999/L29v7ku/z4Ycfavr06Tpw4ECFgas2HThwQAMGDNC///1vBQYGuqUNgB0xQBlAg7Js2TL17t1bwcHB+vzzz/XSSy/pscces9b//PPPOnz4sObOnatJkyZVKuhI0h133KGvv/5aJ0+eVHh4eG01v0Lff/+9Vq1aRdABahg9OwAalCeeeELr16/X6dOndeWVV+rBBx/UzJkz5en5699uCQkJeuGFF3TLLbfo3XffVbNmzdzcYgDuRtgBAAC2xgBlAABga4QdAABga4QdAABga4QdAABga4QdAABga4QdAABga4QdAABga4QdAABga4QdAABga/8PNxWSNJeCKwYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "#Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  y_preds = model_0(x_test)\n",
    "\n",
    "plt.hist(y_preds[:,0].numpy(),100,histtype='step')\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Predicted Energy Distribution')\n",
    "plt.savefig(\"linreg_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorIntersect(t1, t2):\n",
    "    a = set((tuple(i) for i in t1.numpy()))\n",
    "    b = set((tuple(i) for i in t2.numpy()))\n",
    "    c = a.intersection(b)\n",
    "    tensorform = torch.from_numpy(np.array(list(c)))\n",
    "\n",
    "    return tensorform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_150GeV = tensorIntersect(features(data_tensor_150GeV), x_test)\n",
    "test_100GeV = tensorIntersect(features(data_tensor_100GeV), x_test)\n",
    "test_50GeV = tensorIntersect(features(data_tensor_50GeV), x_test)\n",
    "test_20GeV = tensorIntersect(features(data_tensor_20GeV), x_test)\n",
    "test_10GeV = tensorIntersect(features(data_tensor_10GeV), x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the model in evaluation mode\n",
    "model_0.eval()\n",
    "\n",
    "#Setup the inference mode context manager\n",
    "with torch.inference_mode():\n",
    "  y_preds_150GeV = model_0(test_150GeV)\n",
    "  y_preds_100GeV = model_0(test_100GeV)\n",
    "  y_preds_50GeV = model_0(test_50GeV)\n",
    "  y_preds_20GeV = model_0(test_20GeV)\n",
    "  y_preds_10GeV = model_0(test_10GeV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.39064, 21.229746, 50.739407, 99.641106, 147.66544)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peak_preds = norm.fit(y_preds_10GeV)[0], norm.fit(y_preds_20GeV)[0], norm.fit(y_preds_50GeV)[0], norm.fit(y_preds_100GeV)[0], norm.fit(y_preds_150GeV)[0]\n",
    "true_peaks = [10,20,50,100,150]\n",
    "peak_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(true_peaks,peak_preds)\n",
    "plt.xlabel('Particle Energy (GeV)')\n",
    "plt.ylabel('Reconstructed Energy (GeV)')\n",
    "plt.plot(np.arange(1,151),np.arange(1,151))\n",
    "plt.title('Linearity')\n",
    "plt.savefig(\"linreg_linearity.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get energy resolution from distribution of predictions\n",
    "def res(preds,energy):\n",
    "    return norm.fit(preds)[1]/energy\n",
    "\n",
    "energy_list = [150,100,50,20,10]\n",
    "resolutions = res(y_preds_150GeV,150), res(y_preds_100GeV,100), res(y_preds_50GeV,50), res(y_preds_20GeV,20), res(y_preds_10GeV,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Curve fit for energy resolution as a function of energy\n",
    "def f(E, a, b):\n",
    "    return np.sqrt((a/np.sqrt(E))**2 + b**2)\n",
    "\n",
    "popt, pcov = curve_fit(f, energy_list, resolutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.91554455, 0.04477174]),\n",
       " array([[ 0.00060016, -0.00020255],\n",
       "        [-0.00020255,  0.00016713]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Include gaussian fit in loss fn\n",
    "\n",
    "popt, pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHFCAYAAAAOmtghAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABN7klEQVR4nO3deXyM1/4H8M/sk3WybxJJCEVj32prq0qLottVVVupS+202qreKr+2bt2WthSlxdWq6qKtotq0qKUUIbVelCBIRNbJNpnJzPn9ERlGFhEzeZInn/frNa9kzpxn5nvQ5NNzzvM8CiGEABEREZFMKKUugIiIiMiZGG6IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFYYboiIiEhWGG6IiIhIVhhuiIiISFYYbohqiVWrVkGhUJT72L59u9QlulRUVJTDeD08PNCmTRssWrQINeFC61FRURgxYkSVjv3iiy/w/vvvl/maQqHAG2+8UeW6iOoitdQFENHtWblyJZo0aVKqvVmzZhJUU726dOmCd999FwBw+fJlzJ8/HxMnToTRaMSrr74qcXVV98UXX+Do0aOYMmVKqdf27NmD8PDw6i+KqBZjuCGqZWJjY9GuXTupy4DFYoFCoYBaXX0/Rnx8fHDPPffYnz/44IOoX78+Pv7441odbipy43iJqHK4LEUkQwqFAhMmTMBnn32Gpk2bwt3dHS1btsTGjRtL9T19+jQGDx6MoKAg6HQ6NG3aFB999JFDn+3bt0OhUOCzzz7DCy+8gHr16kGn0+Hvv/8GACxfvhyNGzeGTqdDs2bN8MUXX2DEiBGIiooCAAgh0KhRIzz00EOlPj83NxcGgwHjx4+/7XF6e3ujcePGuHLlikO72WzGm2++iSZNmkCn0yEwMBDPPvssrl696tBv69atuP/+++Hv7w83NzfUr18fTzzxBPLz8+19MjIyMG7cONSrVw9arRYNGjTAzJkzUVhYWGFtJcuI586dc2gv+bMsWUa8//77sWnTJpw/f95h2a1EWctSR48exYABA+Dr6wu9Xo9WrVrhv//9b5mfs3btWsycORNhYWHw9vbGgw8+iJMnT1ZYO1Ftx5kbolrGarWiqKjIoU2hUEClUjm0bdq0Cfv378ecOXPg6emJefPm4bHHHsPJkyfRoEEDAMDx48fRuXNn1K9fH++99x5CQkLw888/Y9KkSUhLS8OsWbMc3nPGjBno1KkTli5dCqVSiaCgICxbtgxjxozBE088gQULFiA7OxuzZ892+OWvUCgwceJETJkyBadPn0ajRo3sr61evRpGo7FK4aaoqAhJSUlo3Lixvc1ms2HAgAHYuXMnXnrpJXTu3Bnnz5/HrFmzcP/99+PAgQNwc3PDuXPn0LdvX3Tr1g0rVqyAj48PLl26hC1btsBsNsPd3R0mkwndu3fHmTNnMHv2bLRo0QI7d+7E3LlzkZCQgE2bNt12zTdbvHgx/vnPf+LMmTP47rvvbtn/5MmT6Ny5M4KCgvDhhx/C398fn3/+OUaMGIErV67gpZdecuj/6quvokuXLvjkk09gNBrx8ssvo1+/fjhx4kSpfzNEsiGIqFZYuXKlAFDmQ6VSOfQFIIKDg4XRaLS3paSkCKVSKebOnWtve+ihh0R4eLjIzs52OH7ChAlCr9eLjIwMIYQQ27ZtEwDEvffe69DParWKkJAQ0bFjR4f28+fPC41GIyIjI+1tRqNReHl5icmTJzv0bdasmejevfstxx8ZGSn69OkjLBaLsFgs4vz582L06NFCo9GIjRs32vutXbtWABDffvutw/H79+8XAMTixYuFEEJ88803AoBISEgo9zOXLl0qAIivvvrKof2dd94RAMQvv/ziUN/w4cPtz0v+vhITEx2OLfmz3LZtm72tb9++Dn9WNwIgZs2aZX8+aNAgodPpxIULFxz69e7dW7i7u4usrCyHz+nTp49Dv6+++koAEHv27Cl33ES1HZeliGqZ1atXY//+/Q6PP//8s1S/7t27w8vLy/48ODgYQUFBOH/+PADAZDLht99+w2OPPQZ3d3cUFRXZH3369IHJZMLevXsd3vOJJ55weH7y5EmkpKRg4MCBDu3169dHly5dHNq8vLzw7LPPYtWqVcjLywNQvCx0/PhxTJgwoVJj37x5MzQaDTQaDSIjI7F8+XIsXLgQffv2tffZuHEjfHx80K9fP4cxtWrVCiEhIfbloFatWkGr1eKf//wn/vvf/+Ls2bOlPm/r1q3w8PDAk08+6dBeclbUb7/9Vqm6nWnr1q3o0aMHIiIiStWUn5+PPXv2OLT379/f4XmLFi0AwP7vgEiOGG6IapmmTZuiXbt2Do+2bduW6ufv71+qTafToaCgAACQnp6OoqIiLFy40B4YSh59+vQBAKSlpTkcHxoa6vA8PT0dQHFwullZbRMnTkROTg7WrFkDAFi0aBHCw8MxYMCAygwdXbt2xf79+7F371589tlniIqKwoQJE7Br1y57nytXriArKwtarbbUuFJSUuxjatiwIX799VcEBQVh/PjxaNiwIRo2bIgPPvjAYXwhISEOe2AAICgoCGq12j7+6pSenl7q7wEAwsLC7K/f6OZ/BzqdDgDs/w6I5Ih7bojqKF9fX6hUKgwdOrTc/S7R0dEOz2/+JV/yi/PmDb0AkJKSUqotJiYGvXv3xkcffYTevXtjw4YNmD17dqX3fhgMBvuZYh07dkTHjh3RsmVLjBs3DgkJCVAqlQgICIC/vz+2bNlS5nvcOJvVrVs3dOvWDVarFQcOHMDChQsxZcoUBAcHY9CgQfD398eff/4JIYTD2FNTU1FUVISAgIBya9Xr9QBQauPxzYHxdvn7+yM5OblU++XLlwGgwpqI6grO3BDVUe7u7ujevTsOHTqEFi1alJoNateuXZmzPze66667EBISgq+++sqh/cKFC/jjjz/KPGby5Mk4fPgwhg8fDpVKhdGjR1d5DI0aNcJLL72EI0eOYN26dQCARx55BOnp6bBarWWO6a677ir1PiqVCh07drSfJXbw4EEAQI8ePZCbm4vvv//eof/q1avtr5en5Eyxw4cPO7Rv2LChVN8bZ9RupUePHti6das9zNxYk7u7O08dJwJnbohqnaNHj5Y6WwooXmYJDAy8rff64IMP0LVrV3Tr1g3PP/88oqKikJOTg7///hs//vgjtm7dWuHxSqUSs2fPxpgxY/Dkk09i5MiRyMrKwuzZsxEaGgqlsvT/P/Xs2RPNmjXDtm3bMGTIEAQFBd1WzTd78cUXsXTpUsyePRsDBw7EoEGDsGbNGvTp0weTJ09Ghw4doNFocPHiRWzbtg0DBgzAY489hqVLl2Lr1q3o27cv6tevD5PJhBUrVgAovn4OAAwbNgwfffQRhg8fjnPnzqF58+bYtWsX3n77bfTp08feryzt27fHXXfdhRdffBFFRUXw9fXFd99957CEVqJ58+ZYv349lixZgrZt20KpVJZ7LaNZs2Zh48aN6N69O15//XX4+flhzZo12LRpE+bNmweDwXBHf55EsiD1jmYiqpyKzpYCIJYvX27vC0CMHz++1HvcfEaPEEIkJiaKkSNHinr16gmNRiMCAwNF586dxZtvvmnvU3Lmzddff11mbcuWLRMxMTFCq9WKxo0bixUrVogBAwaI1q1bl9n/jTfeEADE3r17Kz3+yMhI0bdv3zJf++ijjwQA8d///lcIIYTFYhHvvvuuaNmypdDr9cLT01M0adJEjBkzRpw+fVoIIcSePXvEY489JiIjI4VOpxP+/v7ivvvuExs2bHB47/T0dDF27FgRGhoq1Gq1iIyMFDNmzBAmk6lUfTf/2Z46dUr06tVLeHt7i8DAQDFx4kSxadOmUmdLZWRkiCeffFL4+PgIhUIhbvzRjJvOlhJCiCNHjoh+/foJg8EgtFqtaNmypVi5cqVDn/L+zhITEwWAUv2J5EQhRA24KQsRyUpWVhYaN26MRx99FMuWLSv1ert27aBQKLB//34JqiMiueOyFBHdkZSUFLz11lvo3r07/P39cf78eSxYsAA5OTmYPHmyvZ/RaMTRo0exceNGxMfHV+qCdUREVcFwQ0R3RKfT4dy5cxg3bhwyMjLsm1qXLl2Ku+++297v4MGD9gA0a9YsPProo9IVTUSyxmUpIiIikhWeCk5ERESywnBDREREssJwQ0RERLJS5zYU22w2XL58GV5eXqUuJU9EREQ1kxACOTk5CAsLK/MCoTeqc+Hm8uXLpe6mS0RERLVDUlISwsPDK+xT58JNyU3zkpKS4O3tLXE1REREVBlGoxEREREON78tT50LNyVLUd7e3gw3REREtUxltpRwQzERERHJCsMNERERyQrDDREREckKww0RERHJCsMNERERyQrDDREREckKww0RERHJCsMNERERyQrDDREREckKww0RERHJCsMNERERyQrDDREREckKw42TWG0CydkFuJCeL3UpREREdVqduyu4q6TmmNBp7laolQr8/XYfqcshIiKqszhz4yQ6tQoAUGQTsNmExNUQERHVXQw3TqJVX/+jNFttElZCRERUtzHcOIlWdf2PsrCI4YaIiEgqDDdOolEp7N+bGW6IiIgkw3DjJAqFwr40xWUpIiIi6TDcOJHu2tJUocUqcSVERER1F8ONE+k0nLkhIiKSGsONE5VsKuaeGyIiIukw3DiRfc8Nww0REZFkGG6ciOGGiIhIegw3TlQSbnidGyIiIukw3DhRyZ4bhhsiIiLpMNw4Ucn9pXi2FBERkXQYbpyIe26IiIikx3DjRAw3RERE0mO4caLr4YZXKCYiIpIKw40T6bihmIiISHIMN05kv/0Cww0REZFkGG6cyH77BZ4tRUREJBmGGyfihmIiIiLpMdw4Ea9QTEREJD2GGyfSqoov4sdwQ0REJB2GGyfishQREZH0GG6cSKfmhmIiIiKpMdw4ES/iR0REJD2GGyfishQREZH0GG6ciMtSRERE0mO4caKSi/gVWhhuiIiIpMJw40T22y9w5oaIiEgyDDdOVHKdG+65ISIikg7DjRNxQzEREZH0GG6ciLdfICIikh7DjRPZNxQz3BAREUmG4caJeBE/IiIi6THcOBGvc0NERCQ9hhsn0nFDMRERkeQYbpyoZFnKJoAizt4QERFJguHGiUrCDcClKSIiIqkw3DhRydlSAG/BQEREJBWGGydSq5RQKRUAOHNDREQkFYYbJyuZveGmYiIiImkw3DgZr1JMREQkLYYbJ+P9pYiIiKTFcONk12/BwKsUExERSYHhxsl4IT8iIiJpMdw4mZa3YCAiIpIUw42TceaGiIhIWgw3TsYNxURERNJiuHEyLksRERFJi+HGyexnS/H2C0RERJJguHEy+0X8OHNDREQkCcnDzeLFixEdHQ29Xo+2bdti586dFfZfs2YNWrZsCXd3d4SGhuLZZ59Fenp6NVV7azq1CgD33BAREUlF0nCzbt06TJkyBTNnzsShQ4fQrVs39O7dGxcuXCiz/65duzBs2DCMGjUKx44dw9dff439+/fjueeeq+bKy8cNxURERNKSNNzMnz8fo0aNwnPPPYemTZvi/fffR0REBJYsWVJm/7179yIqKgqTJk1CdHQ0unbtijFjxuDAgQPVXHn5GG6IiIikJVm4MZvNiI+PR69evRzae/XqhT/++KPMYzp37oyLFy9i8+bNEELgypUr+Oabb9C3b9/qKLlSePsFIiIiaUkWbtLS0mC1WhEcHOzQHhwcjJSUlDKP6dy5M9asWYOnnnoKWq0WISEh8PHxwcKFC8v9nMLCQhiNRoeHK/EifkRERNKSfEOxQqFweC6EKNVW4vjx45g0aRJef/11xMfHY8uWLUhMTMTYsWPLff+5c+fCYDDYHxEREU6t/2Y6XueGiIhIUpKFm4CAAKhUqlKzNKmpqaVmc0rMnTsXXbp0wfTp09GiRQs89NBDWLx4MVasWIHk5OQyj5kxYways7Ptj6SkJKeP5Ubcc0NERCQtycKNVqtF27ZtERcX59AeFxeHzp07l3lMfn4+lErHklWq4lOvhRBlHqPT6eDt7e3wcCWGGyIiImlJuiw1bdo0fPLJJ1ixYgVOnDiBqVOn4sKFC/ZlphkzZmDYsGH2/v369cP69euxZMkSnD17Frt378akSZPQoUMHhIWFSTUMB9c3FDPcEBERSUEt5Yc/9dRTSE9Px5w5c5CcnIzY2Fhs3rwZkZGRAIDk5GSHa96MGDECOTk5WLRoEV544QX4+PjggQcewDvvvCPVEErRXruIH8MNERGRNBSivPUcmTIajTAYDMjOznbJEtU38Rfx4td/4d7GgVg9soPT35+IiKguup3f35KfLSU3108F53VuiIiIpMBw42TcUExERCQthhsn0/I6N0RERJJiuHEyXcnZUhaGGyIiIikw3DgZZ26IiIikxXDjZLprp4Jzzw0REZE0GG6cjBuKiYiIpMVw42QMN0RERNJiuHGyknDDKxQTERFJg+HGyUruLWW22sq9mScRERG5DsONk5XM3AA8Y4qIiEgKDDdOprsx3HBpioiIqNox3DhZybIUwHBDREQkBYYbJ1MqFdCoFAC4LEVERCQFhhsX0PIWDERERJJhuHEB3oKBiIhIOgw3LsBbMBAREUmH4cYFeCE/IiIi6TDcuABvwUBERCQdhhsXsG8oLrJKXAkREVHdw3DjApy5ISIikg7DjQvwbCkiIiLpMNy4gI4zN0RERJJhuHEBhhsiIiLpMNy4AJeliIiIpMNw4wK8/QIREZF0GG5cgDM3RERE0mG4cQFeoZiIiEg6DDcuwHtLERERSYfhxgV4ET8iIiLpMNy4AG+/QEREJB2GGxfgzA0REZF0GG5cQMezpYiIiCTDcOMCvEIxERGRdBhuXIDLUkRERNJhuHEBXsSPiIhIOgw3LqBVFV/nhrdfICIiqn4MNy5gv0IxZ26IiIiqHcONC3DPDRERkXQYblzg+tlSvIgfERFRdWO4cQFuKCYiIpIOw40L2G+/wA3FRERE1Y7hxgV4hWIiIiLpMNy4ADcUExERSYfhxgV06uLr3DDcEBERVT+GGxdw0xaHmyKbQCHPmCIiIqpWDDcu4KVTQ6Eo/j7HVCRtMURERHUMw40LKJUKeOrUAIDsAovE1RAREdUtDDcu4q3XAACMDDdERETViuHGRbzdroUbLksRERFVK4YbF/HWFy9LceaGiIioejHcuIjBPnPDcENERFSdGG5cpGRZihuKiYiIqhfDjYtc31DMPTdERETVieHGRbzdru254bIUERFRtWK4cRGeCk5ERCQNhhsXMfBUcCIiIkkw3LgINxQTERFJg+HGRUquc5PDcENERFStGG5cxJvXuSEiIpIEw42L2MNNQRGEEBJXQ0REVHcw3LhIyYZis9UGk8UmcTVERER1B8ONi3hoVVAqir/n0hQREVH1YbhxEYVCccPSFMMNERFRdWG4cSH7hfw4c0NERFRtGG5cyH4LBt5fioiIqNow3LiQgRfyIyIiqnaSh5vFixcjOjoaer0ebdu2xc6dOyvsX1hYiJkzZyIyMhI6nQ4NGzbEihUrqqna28NlKSIiouqnlvLD161bhylTpmDx4sXo0qULPv74Y/Tu3RvHjx9H/fr1yzxm4MCBuHLlCj799FPExMQgNTUVRUU1c9mHN88kIiKqfpKGm/nz52PUqFF47rnnAADvv/8+fv75ZyxZsgRz584t1X/Lli34/fffcfbsWfj5+QEAoqKiqrPk22Lfc8ObZxIREVUbyZalzGYz4uPj0atXL4f2Xr164Y8//ijzmA0bNqBdu3aYN28e6tWrh8aNG+PFF19EQUFBuZ9TWFgIo9Ho8KgunLkhIiKqfpLN3KSlpcFqtSI4ONihPTg4GCkpKWUec/bsWezatQt6vR7fffcd0tLSMG7cOGRkZJS772bu3LmYPXu20+uvDIM7NxQTERFVN8k3FCsUCofnQohSbSVsNhsUCgXWrFmDDh06oE+fPpg/fz5WrVpV7uzNjBkzkJ2dbX8kJSU5fQzl4YZiIiKi6lflmZusrCzs27cPqampsNkc7500bNiwWx4fEBAAlUpVapYmNTW11GxOidDQUNSrVw8Gg8He1rRpUwghcPHiRTRq1KjUMTqdDjqdrjJDcjpe54aIiKj6VSnc/Pjjj3jmmWeQl5cHLy8vh5kWhUJRqXCj1WrRtm1bxMXF4bHHHrO3x8XFYcCAAWUe06VLF3z99dfIzc2Fp6cnAODUqVNQKpUIDw+vylBcijM3RERE1a9Ky1IvvPACRo4ciZycHGRlZSEzM9P+yMjIqPT7TJs2DZ988glWrFiBEydOYOrUqbhw4QLGjh0LoHhJ6cagNHjwYPj7++PZZ5/F8ePHsWPHDkyfPh0jR46Em5tbVYbiUryIHxERUfWr0szNpUuXMGnSJLi7u9/Rhz/11FNIT0/HnDlzkJycjNjYWGzevBmRkZEAgOTkZFy4cMHe39PTE3FxcZg4cSLatWsHf39/DBw4EG+++eYd1eEqN944s6K9REREROQ8CiGEuN2DHn/8cQwaNAgDBw50RU0uZTQaYTAYkJ2dDW9vb5d+VoHZiqavbwEAHJ39EDx1kl5WiIiIqNa6nd/fVfpt27dvX0yfPh3Hjx9H8+bNodFoHF7v379/Vd5WdvQaJTQqBSxWAWOBheGGiIioGlTpt+3o0aMBAHPmzCn1mkKhgNVqvbOqZEKhUMBbr0F6nhlGkwVhqHn7goiIiOSmSuHm5lO/qXwGt+Jwk53PTcVERETVQfKL+MmdV8mmYt5fioiIqFpUOdz8/vvv6NevH2JiYtCoUSP0798fO3fudGZtsuCtL7mQH2duiIiIqkOVws3nn3+OBx98EO7u7pg0aRImTJgANzc39OjRA1988YWza6zV7KeD80J+RERE1aJKe27eeustzJs3D1OnTrW3TZ48GfPnz8f//d//YfDgwU4rsLa7fmdwLksRERFVhyrN3Jw9exb9+vUr1d6/f38kJibecVFywqsUExERVa8qhZuIiAj89ttvpdp/++03RERE3HFRcmK/eSaXpYiIiKpFlZalXnjhBUyaNAkJCQno3LkzFAoFdu3ahVWrVuGDDz5wdo212vVlKYYbIiKi6lClcPP8888jJCQE7733Hr766isAQNOmTbFu3bpy7+hdV3FDMRERUfWq8v0AHnvsMTz22GPOrEWWrp8Kzg3FRERE1YEX8XMxbigmIiKqXpWeufHz88OpU6cQEBAAX19fKBSKcvtmZGQ4pTg54LIUERFR9ap0uFmwYAG8vLzs31cUbui6kpmbHFMRiqw2qFWcLCMiInKlSoeb4cOH278fMWKEK2qRJV93LVRKBaw2gau5hQg18M7gRERErlSlaQSVSoXU1NRS7enp6VCpVHdclJyolAoEeekAACnZJomrISIikr8qhRshRJnthYWF0Gq1d1SQHIUY9ACAK0aGGyIiIle7rVPBP/zwQwCAQqHAJ598Ak9PT/trVqsVO3bsQJMmTZxboQyEeBeHG87cEBERud5thZsFCxYAKJ65Wbp0qcMSlFarRVRUFJYuXercCmUg+Fq4SebMDRERkcvdVrgpuSlm9+7dsX79evj6+rqkKLmxL0tx5oaIiMjlqnSF4m3btjm7DlmzL0tx5oaIiMjlqhRuRo4cWeHrK1asqFIxcnV9Q3GhxJUQERHJX5XCTWZmpsNzi8WCo0ePIisrCw888IBTCpOTGzcUCyF4AUQiIiIXqlK4+e6770q12Ww2jBs3Dg0aNLjjouSmZOamwGKFsaAIBneNxBURERHJl9PuBaBUKjF16lT7GVV0nV6jst+GgftuiIiIXMupNzo6c+YMioqKnPmWssFNxURERNWjSstS06ZNc3guhEBycjI2bdrkcA8qui7EoMfJKzk8HZyIiMjFqhRuDh065PBcqVQiMDAQ77333i3PpKqrOHNDRERUPXidm2oSbGC4ISIiqg5O3XND5eP9pYiIiKpHpWduWrduXenrsxw8eLDKBclViEEHgOGGiIjI1Sodbh599FEXliF/Id5uAIArXJYiIiJyqUqHm1mzZrmyDtkruZBfep4ZhUVW6NSqWxxBREREVVGlDcUl4uPjceLECSgUCjRr1gytW7d2Vl2y4+uugVathLnIhlRjISL83KUuiYiISJaqFG5SU1MxaNAgbN++HT4+PhBCIDs7G927d8eXX36JwMBAZ9dZ6ykUCgR765CUUYAUo4nhhoiIyEWqdLbUxIkTYTQacezYMWRkZCAzMxNHjx6F0WjEpEmTnF2jbIRe23fDTcVERESuU6WZmy1btuDXX39F06ZN7W3NmjXDRx99hF69ejmtOLkpudYNNxUTERG5TpVmbmw2GzSa0ne21mg0sNlsd1yUXIV483RwIiIiV6tSuHnggQcwefJkXL582d526dIlTJ06FT169HBacXITzFswEBERuVyVws2iRYuQk5ODqKgoNGzYEDExMYiOjkZOTg4WLlzo7BplI4TLUkRERC5XpT03EREROHjwIOLi4vC///0PQgg0a9YMDz74oLPrk5XQa+EmmctSRERELnNH17np2bMnevbsCQDIyspyRj2yVrIslWoshM0moFRW7nYWREREVHlVWpZ65513sG7dOvvzgQMHwt/fH/Xq1cNff/3ltOLkJsRbD41KAbPVhsvZBVKXQ0REJEtVCjcff/wxIiIiAABxcXGIi4vDTz/9hN69e2P69OlOLVBO1Colovw9AABnr+ZJXA0REZE8VWlZKjk52R5uNm7ciIEDB6JXr16IiopCx44dnVqg3DQI9MDp1FycuZqLexvzSs5ERETOVqWZG19fXyQlJQEovqBfyUZiIQSsVqvzqpOhhoGeAIAzV3MlroSIiEieqjRz8/jjj2Pw4MFo1KgR0tPT0bt3bwBAQkICYmJinFqg3NjDTSqXpYiIiFyhSuFmwYIFiIqKQlJSEubNmwdPz+Jf2MnJyRg3bpxTC5SbhkGcuSEiInKlKoUbjUaDF198sVT7lClT7rQe2WsQWLyhODWnEDkmC7z0pW9jQURERFVXpT03APDZZ5+ha9euCAsLw/nz5wEA77//Pn744QenFSdH3noNgryK7zHFM6aIiIicr0rhZsmSJZg2bRp69+6NrKws+yZiHx8fvP/++86sT5a4qZiIiMh1qhRuFi5ciOXLl2PmzJlQqVT29nbt2uHIkSNOK06uSpamGG6IiIicr0rhJjExEa1bty7VrtPpkJfHpZZb4RlTRERErlOlcBMdHY2EhIRS7T/99BOaNm16pzXJHs+YIiIicp0qnS01ffp0jB8/HiaTCUII7Nu3D2vXrsXbb7+NTz/91Nk1yk7Da8tS59PzUWS1Qa2q8r5uIiIiukmVws2zzz6LoqIivPTSS8jPz8fgwYNRr149LFy4EN26dXN2jbITZnCDXqOEyWLDxcwCRAV4SF0SERGRbFR5ymD06NE4f/48UlNTkZKSgn379uHQoUO8QnElKJUKNAjg0hQREZEr3Fa4ycrKwjPPPIPAwECEhYXhww8/hJ+fHz766CPExMRg7969WLFihatqlRXuuyEiInKN21qWevXVV7Fjxw4MHz4cW7ZswdSpU7FlyxaYTCZs3rwZ9913n6vqlJ0G15aieMYUERGRc91WuNm0aRNWrlyJBx98EOPGjUNMTAwaN27MC/dVQcnMzdk0ztwQERE5020tS12+fBnNmjUDADRo0AB6vR7PPfecSwqTu5Izpv5OZbghIiJyptsKNzabDRrN9Rs9qlQqeHjwTJ+qaBjoCZVSgcx8C1KyTVKXQ0REJBu3tSwlhMCIESOg0xXf+NFkMmHs2LGlAs769eudV6FM6TUqNA72wolkI/66mIUQQ4jUJREREcnCbYWb4cOHOzwfMmSIU4upa1pFGIrDTVIWHrqb4YaIiMgZbivcrFy50lV11Ektw32wdl8S/rqYJXUpREREsiH5df8XL16M6Oho6PV6tG3bFjt37qzUcbt374ZarUarVq1cW6ALtYzwAQAcTsqGzSakLYaIiEgmJA0369atw5QpUzBz5kwcOnQI3bp1Q+/evXHhwoUKj8vOzsawYcPQo0ePaqrUNRoFecJNo0JOYRHOpvF6N0RERM4gabiZP38+Ro0aheeeew5NmzbF+++/j4iICCxZsqTC48aMGYPBgwejU6dO1VSpa6hVSjSvZwAA/JWUJW0xREREMiFZuDGbzYiPj0evXr0c2nv16oU//vij3ONWrlyJM2fOYNasWa4usVq0jLgWbrjvhoiIyCmqdFdwZ0hLS4PVakVwcLBDe3BwMFJSUso85vTp03jllVewc+dOqNWVK72wsBCFhYX250ajsepFu0DJvhvO3BARETmH5BuKFQqFw3MhRKk2ALBarRg8eDBmz56Nxo0bV/r9586dC4PBYH9ERETccc3O1DLcBwBwPNkIk8UqbTFEREQyIFm4CQgIgEqlKjVLk5qaWmo2BwBycnJw4MABTJgwAWq1Gmq1GnPmzMFff/0FtVqNrVu3lvk5M2bMQHZ2tv2RlJTkkvFUVbivG/w8tLBYBU4k16xZJSIiotpIsnCj1WrRtm1bxMXFObTHxcWhc+fOpfp7e3vjyJEjSEhIsD/Gjh2Lu+66CwkJCejYsWOZn6PT6eDt7e3wqEkUCgVahnNTMRERkbNItucGAKZNm4ahQ4eiXbt26NSpE5YtW4YLFy5g7NixAIpnXS5duoTVq1dDqVQiNjbW4figoCDo9fpS7bVNywgfbDt5FX9dzJa6FCIiolpP0nDz1FNPIT09HXPmzEFycjJiY2OxefNmREZGAgCSk5Nvec0bOeCmYiIiIudRCCHq1KVxjUYjDAYDsrOza8wSVUaeGW3+r3h57sBrDyLAUydxRURERDXL7fz+lvxsKQL8PLRoGlr8F7X77zSJqyEiIqrdGG5qiG6NAgAAO08z3BAREd0JhpsaoiTc7Dqdhjq2UkhERORUDDc1RPsoP2jVSqQYTThzNVfqcoiIiGothpsaQq9RoWO0HwBgxykuTREREVUVw00N0jXm2tIUNxUTERFVGcNNDdKtUSAAYO/ZdJiLbBJXQ0REVDsx3NQgTUK8EOCpRb7ZioMXMqUuh4iIqFZiuKlBlErF9aUpnhJORERUJQw3NUzXa0tTO09flbgSIiKi2onhpoYpud7N4UvZSM0xSVwNERFR7cNwU8MEe+vRKsIHQgA/H02RuhwiIqJah+GmBnqkRSgAYOPhZIkrISIiqn0Ybmqg3s2Lw82+cxm4YuTSFBER0e1guKmB6vm4oU394qWpn45w9oaIiOh2MNzUUH1bhAEANjHcEBER3RaGmxqqT/MQAMD+c5lIyebSFBERUWUx3NRQoQY3tIv0BcDZGyIiotvBcFOD9b121tSmw5clroSIiKj2YLipwfo0D4VCARy8kIXEtDypyyEiIqoVGG5qsGBvPe5vXHw7hrX7LkhcDRERUe3AcFPDPdMxEgDw9YEkmCxWiashIiKq+RhuarjuTYIQZtAjM9+CLbwdAxER0S0x3NRwKqUCgzrUBwCs+fO8xNUQERHVfAw3tcBT7SOgUiqw/1wmTqbkSF0OERFRjcZwUwsEe+vRs2kwAOALzt4QERFViOGmlnjmnuKlqfUHL8FoskhcDRERUc3FcFNLdGkYgIaBHsgpLMJnezh7Q0REVB6Gm1pCqVRg3P0xAIAVuxJRYOZp4URERGVhuKlF+rcKQ4SfG9LzzLyoHxERUTkYbmoRjUqJsfc1BAAs23EWhUWcvSEiIroZw00t82TbcAR765BiNGH9wUuw2gT2nEnHDwmXsOdMOqw2IXWJREREklIIIerUb0Oj0QiDwYDs7Gx4e3tLXU6VfLLzLN7cdAIBnlqolQqkGAvtr4Ua9JjVrxkejg2VsEIiIiLnup3f35y5qYUGd6wPL50aablmh2ADACnZJjz/+UFsOZosUXVERETSYriphXRqFRSKsl8rmYab/eNxLlEREVGdxHBTC+1LzIDRVFTu6wJAcrYJ+xIzqq8oIiKiGoLhphZKzTE5tR8REZGcMNzUQkFeeqf2IyIikhOGm1qoQ7QfQg16lLPtBgoUnzXVIdqvOssiIiKqERhuaiGVUoFZ/ZoBQLkBZ1a/ZlApy3uViIhIvhhuaqmHY0OxZEgbhBgcl548tCosGdKG17khIqI6Sy11AVR1D8eGomezEOxLzMDW/13B8p2JMBXZEO7rLnVpREREkuHMTS2nUirQqaE/ZvZthr4tQmG1Cbz87WGYi2xSl0ZERCQJhhsZeaPf3TC4aXDsshHvxZ2UuhwiIiJJMNzISKCXDu880RwA8PHvZ7Hj1FWJKyIiIqp+DDcy83BsKJ7pWB8AMO2rv5CWW3iLI4iIiOSF4UaG/vVIMzQO9kRabiFe/Pov2HiPKSIiqkMYbmRIr1Fh4dNtoFMrsf3kVSz49ZTUJREREVUbhhuZuivEC28/Vrz/ZuHWv7HpcLLEFREREVUPhhsZe6JtOJ7rGg0AePHrv3D8slHiioiIiFyP4UbmXundBN0aBaDAYsXo1Qdwxcg7hRMRkbwx3MicWqXEoqfbIDrAA5eyCjB8xT5kF1ikLouIiMhlGG7qAIO7BqtHdkCglw7/S8nB6NUHYLJYpS6LiIjIJRhu6ogIP3eserY9vHRq7EvMwKS1h2Cx8hYNREQkPww3dcjdYQYsG9YOWpUSvxy/gilfJjDgEBGR7DDc1DGdGvpjyZA20KgU2HQkmQGHiIhkh+GmDurRNBhLh7SFVqXEpiPJmLT2EAqLuAeHiIjkgeGmjurRNBhLhrSBVqXET0dTMGrVAeQWFkldFhER0R1juKnDejQNxqcj2sFdq8Kuv9MwePle3miTiIhqPYabOq5bo0CsHX0P/Dy0OHwxG08u+QNnruZKXRYREVGVMdwQWkb44JuxnRDu64Zz6fl49KPd2HHqaoXHWG0Ce86k44eES9hzJh1W3nmciIhqCIUQok79VjIajTAYDMjOzoa3t7fU5dQoabmFGPtZPA6cz4RKqcBrfZtiROcoKBQKh35bjiZj9o/HkZx9/VYOoQY9ZvVrhodjQ6u7bCIiqgNu5/c3Z27ILsBThzWjO+KJNuGw2gRm/3gcU9YlIO+GjcZbjibj+c8POgQbAEjJNuH5zw9iy1HefZyIiKTFcEMOdGoV3v1HC7zWtylUSgV+SLiMAR/txukrOfbAU9ZUX0nb7B+Pc4mKiIgkxXBDpSgUCjzXrQG+/Oc9CPbW4e/UXPRftBtvbz5RasbmRgJAcrYJ+xIzqq9YIiKimzDcULnaR/lh06Ru6BoTgAKLFZ/uSqzUcak55QcgIiIiV2O4oQoFeOqwemQHzOzTFGql4tYHAAjy0ru4KiIiovJJHm4WL16M6Oho6PV6tG3bFjt37iy37/r169GzZ08EBgbC29sbnTp1ws8//1yN1dZNSqUCo+9tgO/Hd6kw4ChQfNZUh2i/6iuOiIjoJpKGm3Xr1mHKlCmYOXMmDh06hG7duqF37964cOFCmf137NiBnj17YvPmzYiPj0f37t3Rr18/HDp0qJorr5ti6xmw4KmWFfaZ1a8ZVJWc4SEiInIFSa9z07FjR7Rp0wZLliyxtzVt2hSPPvoo5s6dW6n3uPvuu/HUU0/h9ddfr1R/Xufmzm05mozXvj+KtFyzvU2tVGDKg40w4YFGElZGRERydTu/v9XVVFMpZrMZ8fHxeOWVVxzae/XqhT/++KNS72Gz2ZCTkwM/v/KXQQoLC1FYeP1+SUajsWoFk93DsaHo2SwEe86k4/uEi/jpSAryzFa8+8sp/J2aixl9miLYm/tuiIhIGpItS6WlpcFqtSI4ONihPTg4GCkpKZV6j/feew95eXkYOHBguX3mzp0Lg8Fgf0RERNxR3VRMpVSga6MAvPuPVtjxUnc83SECCgXwfcJlPPDudizZfgYmi1XqMomIqA6SfEPxzZf2F0KUaivL2rVr8cYbb2DdunUICgoqt9+MGTOQnZ1tfyQlJd1xzeTI31OHuY+3wA/ju6B1fR/kma14Z8v/cO+8bVi95xzMRTapSyQiojpEsnATEBAAlUpVapYmNTW11GzOzdatW4dRo0bhq6++woMPPlhhX51OB29vb4cHuUaLcB98O7Yz3vtHS9TzcUNqTiFe/+EYur+7HV8dSEKRlSGHiIhcT7Jwo9Vq0bZtW8TFxTm0x8XFoXPnzuUet3btWowYMQJffPEF+vbt6+oy6TYplQo80TYc2168H/834G4EeelwKasAL31zGD0X7MAPCZcYcoiIyKUkPVtq3bp1GDp0KJYuXYpOnTph2bJlWL58OY4dO4bIyEjMmDEDly5dwurVqwEUB5thw4bhgw8+wOOPP25/Hzc3NxgMhkp9Js+Wql4FZis+33sei7f/jcx8CwAg3NcNo7s1wD/ahcNdK9mediIiqkVu5/e3pOEGKL6I37x585CcnIzY2FgsWLAA9957LwBgxIgROHfuHLZv3w4AuP/++/H777+Xeo/hw4dj1apVlfo8hhtp5BYWYeWuRKz84xwy8opPIfd112BYpygM6xQJf0+dxBUSEVFNVqvCTXVjuJFWgdmKb+KTsHxnIi5k5AMA9BolnmwbjqH3ROGuEC+JKyQiopqI4aYCDDc1g9UmsOVoCj7ecQaHL2bb29tH+WLIPZF4ODYEOrVKwgqJiKgmYbipAMNNzSKEwN6zGVi95xx+OX4FVlvxP0d/Dy3+0S4Cz3Ssjwg/d4mrJCIiqTHcVIDhpua6YjThy31JWLvvAlKMJnv7PQ388ESbcPRuHgpPHTcgExHVRQw3FWC4qfmKrDb8eiIVa/48j11/p6HkX6ibRoXesSF4om04OjXwh5I36CQiqjMYbirAcFO7XMoqwPeHLuHb+Is4m5Znbw/x1qNvi1D0bRGK1hE+lbqqNRER1V4MNxVguKmdhBA4lJSFb+Mv4se/LsNoKrK/Vs/HDX2ah6BvizC0DDcw6BARyRDDTQUYbmo/k8WKnafTsOnwZcQdv4I88/UbdNbzcUPPZsHo2SwYHaL9oFFJfvs0IiJyAoabCjDcyIvJYsX2k1ex6UgyfjtxBfk3BB0vvRrd7wpCz2bBuO+uQHjrNRJWSkREd4LhpgIMN/JVYLZi199piDuegt9OpCL92pWQAUCtVKBNfV90axSAexsHIraeASpuSCYiqjUYbirAcFM3WG0CCUmZ+OX4FcQdv4KzV/McXvd116BLTHHQubdRIEIMeokqJSKiymC4qQDDTd2UlJGP309dxY5TV/HHmXTkFhY5vN442BPdGgWiY7QfOkT7wcddK1GlRERUFoabCjDckMVqQ0JSFnacuoodp9Nw+GIWbvyvQKEA7gr2wj0N/NEx2g/to/0QwBt7EhFJiuGmAgw3dLPMPDN2/Z2GP86k48/E9FJLWAAQE+Rpn9VpU98X4b5uPOWciKgaMdxUgOGGbiU1x4R9iRnYl5iBP89m4OSVnFJ9Ajx1aF3fB23q+6J1fR+0CDfAXctbQxARuQrDTQUYbuh2ZeSZi4NOYjoOns/EsctGFNkc/7NRKRW4K9gLbSJ90CrCF83rGdAw0ANqXmeHiMgpGG4qwHBDd8pkseLopWwcupCFQ0mZOHQhC8nZplL99BolmoV6I7aeAbH1DGhez4BGQZ63FXisNoF9iRlIzTEhyEuPDtF+PIWdiOokhpsKMNyQKyRnF+DQhSwcPJ+JwxezcexytsOVk0vo1Eo0DfXG3WHeaBLqjaYhXmgc4lXmBQa3HE3G7B+POwSnUIMes/o1w8OxoS4dDxFRTcNwUwGGG6oONptAYnoejl7KxpGL2ThyKRvHLhtLnYJeop6PG5qEeKFJqBeahHjjao4JczaeKNWvZM5myZA2DDhEVKcw3FSA4YakYrMJnEvPw5FL2TiebMTJlBz8LzkHKcbSS1q3EmrQY9fLD3CJiojqDIabCjDcUE2TlW/G/1JyisNOihEHzmXidGruLY/rGO2HjtF+aBjkiYaBnmgQ6MEztohIthhuKsBwQzXdDwmXMPnLhCodW8/HDQ2DPNEgwAPRAR6I9HdHpL8Hwn3deId0IqrVbuf3N/83j6iGCfKq3H2uhnSMhFXY8HdqLs5czUNGnhmXsgpwKasAO05ddeirUipQz8ftWthxR5S/ByL9PRDl744IP3foNSpXDIWISBIMN0Q1TIdoP4Qa9EjJNqGsaVUFgBCDHrMH3O2w5yYjz4wzV3Pxd2ouzl7Nxfn0/OJHRh5MFhsuZOTjQkY+dp6+6f0UQIi33h566vu7I9zXHfV83BDh64YATx2U3NtDRLUIl6WIaqAtR5Px/OcHAcAh4FTlbCkhBFJzCnEuLc8eds6l5+N8eh7Op+Ujp5wzuEpoVUrU83VDuK8b6vlc++rrZg9Awd56bmyuhXgNJaptuOemAgw3VFtUx3VuhBDIzLfgXHoezqfn4Vxa8ezOpczi5a3k7ALYbvETQq1UIMynOPjU83VDqEGPEIO++Kt38XMfdw3vxVWD8BpKVBsx3FSA4YZqE6n/79pitSEl24SLmQW4mJmPS1kFuJhZgEuZBbiYlY/kLFOpW1GURadW2kNPiLceIYabQpBBjwAPLn9Vh5JZwZv/1ngNJarpGG4qwHBD5DxWm8AVY3H4uZRVPOOTYjQhJduE5Ozir+l55kq9l1qpQLD39QAU6KVDkLcOgZ46BHnrr33Vwc9dyxBURVabQNd3tpZ5uxDg+n4uXkOJaiKeLUVE1UJ1bUkqzMcNgF+ZfQqLrEg1FiI524Tk7AKH4FMShFJzimeASs72utVnBnhqi8OPV3HocQxCOgR66hHkreNZYDfZl5hRbrABivd3JWebsC8xA50a+ldfYUROxnBDRC6lU6sQ4Vd8ynl5iqw2XM0tDkBXroWfq7mFuJpTiNSc4q9Xc4pngYpniwpxxVgIwFjhZ3vp1PD31MLPQwt/Tx0CSr730MHf88avWvh6aGV/LaDUnMpdDbuy/YhqKoYbIpKcWqVEqMENoQa3CvsVWW1IzzNfCz2m4q/GQlzNveFrjgmpxkIUFtmQU1iEnMIinEvPr1QdBjeNPezcGHz8PXXXAlJxu6+7Bj7uWmjVtSsMVfYaSpXtR1RTMdwQUa2hVikR7K1HsLcegKHcfkII5BQW4WpOITLyzEjPLURartn+fXqeGem5ZqTnFb+ekWeGTQDZBRZkF1hw9mpeperx0Krg466Fr4cGvu7a4u+vBR9f95K261993LXw1qslO3OsstdQ6hBd9hIjUW3BcENEsqNQKOCt18Bbr0HDwFv3t9oEsgssjiEor+T7wmtB6Howyi6wQAggz2xFnvnW+4RupFIq4OOmuSH0XAtCHteDkMGtuHaDmwbebmoY3DTw0mvueJOvSqnArH7N8PznB6FA2ddQmtWvGTcTU63Hs6WIiG6TzSZgNFmQmW9BZr4ZWflmZOaVfF/6a1a+GZn5FhRYrHf0uV46NbzdNPB208Dgpr4hAF37qlfD4H49GN34mk6ttM8Y8To3VBvxVPAKMNwQkVRMFqs99JQXgLLyi2eGjAVFxV9NFuSb7ywUAcVXmva+NhPkpdfAS6eG2WqDAoC/hxYxQZ7wdtPAS3/tdb0anrrr33vp1XDTqHgxRpIMTwUnIqqB9BoVQgwqhBhub8OuuciGHJPFvifIaLoWfEqeXwtBN4aikmBkLLDAJgCz1Ya03EKk5RZWuX6VUnEt8BQHH++SEKQvaSt+7l3SptPAQ1fc112ngqdODQ+dGu4aFa9VJFNSX3i0BMMNEVENp1Ur4e+pg7+n7raPtdkE8sxFDsEnx2RBbmERckxFyC0sgtFkKf7eVOTwWs4Nz23i+t6k7ALLHY/JQ6uCx7Ww46FTwUOrtoefkuclwajsNtW10MSwVFPUpOVOLksREVGFhBDIN1vtYSenJBiVPDcVXWuzXGsrQk5h8fe5hUXIK7Qir7AIeeaiW96rrCoUCsBdo3KYJXLXquGuVcFdq4KbpjgMuWlVcNcUt7tde82h37XnHjd8z83VlVMdt/XgshQRETmNQqGwz7Lc7pLajYQQMFls1wJPkf1rvtl6U5sV+eaiG9qs1/qVHZaun7lmRWpO1ZfdyqJVK4vDj0ZVPEukVcFNcz0YuWlV18LQzSHJMTjpNcXHuWlV0KuLv964ybs2s9oEZv94vMzLCwgUB5zZPx5Hz2Yh1RYWGW6IiKhaKBQKuF375R/odftLbDcrLyzlmYsDU77ZioJrX/Md2oq/L7BY7eGqwHJj/+szTOYiG8xFNmThzpfiyqLXKItDj6Y4AOlLAtC1dnubPRgpodde7+924zFqZfGf783vpVZC7cKrb9fE23ow3BARUa3k7LBUQgiBwiKbPRRdD0jXQ1JJCMq3FH+fV2hFgeXmUHX9uclSHKBMFiss1utzHCaLDSaLDZkuCk8lNCqFY1DSFAcovT0IKaFTO7bp1Moyv97c/6+LmZWqoTpv68FwQ0REdAOF4noQ8PPQOv39i6w2mIpsKLgWekqCT4HZWna7xXotBFntr5UEJZPFdsOxVpjM1/vfeF0li1XAYi3eDyWV6rytB8MNERFRNVKrlPBUKeGpc+2v4JIZKIfwdC30lISnfLPV3sdkKf6+0FIcskwWKwottuLQdC0wFRZdD1qFN/TJzDeXuecGkOa2Hgw3REREMnTjDJSPiz+r5GwpoGbc1qN23dKWiIiIapyHY0OxZEibUmfThRj0TjkN/HZx5oaIiIju2MOxoejZLIRXKCYiIiL5UCkV1Xa6d0W4LEVERESywnBDREREssJwQ0RERLLCcENERESywnBDREREssJwQ0RERLLCcENERESywnBDREREssJwQ0RERLLCcENERESywnBDREREslLn7i0lRPHN2I1Go8SVEBERUWWV/N4u+T1ekToXbnJycgAAEREREldCREREtysnJwcGg6HCPgpRmQgkIzabDZcvX4aXlxcUCufeht1oNCIiIgJJSUnw9vZ26nvXRByvvHG88lfXxszx1m5CCOTk5CAsLAxKZcW7aurczI1SqUR4eLhLP8Pb21sW/5Aqi+OVN45X/uramDne2utWMzYluKGYiIiIZIXhhoiIiGSF4caJdDodZs2aBZ1OJ3Up1YLjlTeOV/7q2pg53rqjzm0oJiIiInnjzA0RERHJCsMNERERyQrDDREREckKww0RERHJCsONkyxevBjR0dHQ6/Vo27Ytdu7cKXVJTjF37ly0b98eXl5eCAoKwqOPPoqTJ0869BFC4I033kBYWBjc3Nxw//3349ixYxJV7Fxz586FQqHAlClT7G1yG++lS5cwZMgQ+Pv7w93dHa1atUJ8fLz9dbmNt6ioCK+99hqio6Ph5uaGBg0aYM6cObDZbPY+tXnMO3bsQL9+/RAWFgaFQoHvv//e4fXKjK2wsBATJ05EQEAAPDw80L9/f1y8eLEaR1F5FY3XYrHg5ZdfRvPmzeHh4YGwsDAMGzYMly9fdngPuYz3ZmPGjIFCocD777/v0F6bxltVDDdOsG7dOkyZMgUzZ87EoUOH0K1bN/Tu3RsXLlyQurQ79vvvv2P8+PHYu3cv4uLiUFRUhF69eiEvL8/eZ968eZg/fz4WLVqE/fv3IyQkBD179rTfx6u22r9/P5YtW4YWLVo4tMtpvJmZmejSpQs0Gg1++uknHD9+HO+99x58fHzsfeQ0XgB45513sHTpUixatAgnTpzAvHnz8J///AcLFy6096nNY87Ly0PLli2xaNGiMl+vzNimTJmC7777Dl9++SV27dqF3NxcPPLII7BardU1jEqraLz5+fk4ePAg/vWvf+HgwYNYv349Tp06hf79+zv0k8t4b/T999/jzz//RFhYWKnXatN4q0zQHevQoYMYO3asQ1uTJk3EK6+8IlFFrpOamioAiN9//10IIYTNZhMhISHi3//+t72PyWQSBoNBLF26VKoy71hOTo5o1KiRiIuLE/fdd5+YPHmyEEJ+43355ZdF165dy31dbuMVQoi+ffuKkSNHOrQ9/vjjYsiQIUIIeY0ZgPjuu+/szysztqysLKHRaMSXX35p73Pp0iWhVCrFli1bqq32qrh5vGXZt2+fACDOnz8vhJDneC9evCjq1asnjh49KiIjI8WCBQvsr9Xm8d4OztzcIbPZjPj4ePTq1cuhvVevXvjjjz8kqsp1srOzAQB+fn4AgMTERKSkpDiMX6fT4b777qvV4x8/fjz69u2LBx980KFdbuPdsGED2rVrh3/84x8ICgpC69atsXz5cvvrchsvAHTt2hW//fYbTp06BQD466+/sGvXLvTp0weAPMdcojJji4+Ph8VicegTFhaG2NjYWj9+oPhnmEKhsM9Oym28NpsNQ4cOxfTp03H33XeXel1u4y1PnbtxprOlpaXBarUiODjYoT04OBgpKSkSVeUaQghMmzYNXbt2RWxsLADYx1jW+M+fP1/tNTrDl19+iYMHD2L//v2lXpPbeM+ePYslS5Zg2rRpePXVV7Fv3z5MmjQJOp0Ow4YNk914AeDll19GdnY2mjRpApVKBavVirfeegtPP/00APn9Hd+oMmNLSUmBVquFr69vqT61/WeayWTCK6+8gsGDB9tvJCm38b7zzjtQq9WYNGlSma/LbbzlYbhxEoVC4fBcCFGqrbabMGECDh8+jF27dpV6TS7jT0pKwuTJk/HLL79Ar9eX208u47XZbGjXrh3efvttAEDr1q1x7NgxLFmyBMOGDbP3k8t4geI9cp9//jm++OIL3H333UhISMCUKVMQFhaG4cOH2/vJacw3q8rYavv4LRYLBg0aBJvNhsWLF9+yf20cb3x8PD744AMcPHjwtmuvjeOtCJel7lBAQABUKlWpxJuamlrq/45qs4kTJ2LDhg3Ytm0bwsPD7e0hISEAIJvxx8fHIzU1FW3btoVarYZarcbvv/+ODz/8EGq12j4muYw3NDQUzZo1c2hr2rSpfTO83P5+AWD69Ol45ZVXMGjQIDRv3hxDhw7F1KlTMXfuXADyHHOJyowtJCQEZrMZmZmZ5fapbSwWCwYOHIjExETExcXZZ20AeY13586dSE1NRf369e0/v86fP48XXngBUVFRAOQ13oow3NwhrVaLtm3bIi4uzqE9Li4OnTt3lqgq5xFCYMKECVi/fj22bt2K6Ohoh9ejo6MREhLiMH6z2Yzff/+9Vo6/R48eOHLkCBISEuyPdu3a4ZlnnkFCQgIaNGggq/F26dKl1Kn9p06dQmRkJAD5/f0CxWfQKJWOP/pUKpX9VHA5jrlEZcbWtm1baDQahz7Jyck4evRorRx/SbA5ffo0fv31V/j7+zu8LqfxDh06FIcPH3b4+RUWFobp06fj559/BiCv8VZIoo3MsvLll18KjUYjPv30U3H8+HExZcoU4eHhIc6dOyd1aXfs+eefFwaDQWzfvl0kJyfbH/n5+fY+//73v4XBYBDr168XR44cEU8//bQIDQ0VRqNRwsqd58azpYSQ13j37dsn1Gq1eOutt8Tp06fFmjVrhLu7u/j888/tfeQ0XiGEGD58uKhXr57YuHGjSExMFOvXrxcBAQHipZdesvepzWPOyckRhw4dEocOHRIAxPz588WhQ4fsZwdVZmxjx44V4eHh4tdffxUHDx4UDzzwgGjZsqUoKiqSaljlqmi8FotF9O/fX4SHh4uEhASHn2GFhYX295DLeMty89lSQtSu8VYVw42TfPTRRyIyMlJotVrRpk0b+6nStR2AMh8rV66097HZbGLWrFkiJCRE6HQ6ce+994ojR45IV7ST3Rxu5DbeH3/8UcTGxgqdTieaNGkili1b5vC63MZrNBrF5MmTRf369YVerxcNGjQQM2fOdPhlV5vHvG3btjL/mx0+fLgQonJjKygoEBMmTBB+fn7Czc1NPPLII+LChQsSjObWKhpvYmJiuT/Dtm3bZn8PuYy3LGWFm9o03qpSCCFEdcwQEREREVUH7rkhIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4ISIiIllhuCEiIiJZYbghIiIiWWG4IaI6Jz09HUFBQTh37pxkNSxatAj9+/eX7POJ5IzhhojKNWLECCgUilKPhx9+WOrS7sjcuXPRr18/+80ES3z77bd44IEH4OvrC3d3d9x1110YOXIkDh06VKn3NZvNCAgIwJtvvlnu5wYEBMBsNmP06NHYv38/du3adafDIaKbMNwQUYUefvhhJCcnOzzWrl3r0s80m80ue++CggJ8+umneO655xzaX375ZTz11FNo1aoVNmzYgGPHjmHZsmVo2LAhXn311Uq9t1arxZAhQ7Bq1SqUdfH3lStXYujQodBqtdDpdBg8eDAWLlzolHER0Q0kvv0DEdVgw4cPFwMGDKiwDwCxfPly8eijjwo3NzcRExMjfvjhB4c+x44dE7179xYeHh4iKChIDBkyRFy9etX++n333SfGjx8vpk6dKvz9/cW9994rhBDihx9+EDExMUKv14v7779frFq1SgAQmZmZIjc3V3h5eYmvv/7a4bM2bNgg3N3dy73J5bfffisCAgIc2vbs2SMAiA8++KDMY2w2W6nPaNOmjdDpdCI6Olq88cYbwmKxCCGEOHz4sAAgtm/f7nDMjh07BACH+zht375daLVahxvREtGd48wNEd2x2bNnY+DAgTh8+DD69OmDZ555BhkZGQCA5ORk3HfffWjVqhUOHDiALVu24MqVKxg4cKDDe/z3v/+FWq3G7t278fHHH+PcuXN48skn8eijjyIhIQFjxozBzJkz7f09PDwwaNAgrFy50uF9Vq5ciSeffBJeXl5l1rpjxw60a9fOoW3t2rXw9PTEuHHjyjxGoVDYv//5558xZMgQTJo0CcePH8fHH3+MVatW4a233gIANG/eHO3bty9V14oVK9ChQwfExsba29q1aweLxYJ9+/aV+blEVEVSpysiqrmGDx8uVCqV8PDwcHjMmTPH3geAeO211+zPc3NzhUKhED/99JMQQoh//etfolevXg7vm5SUJACIkydPCiGKZ25atWrl0Ofll18WsbGxDm0zZ860z9wIIcSff/4pVCqVuHTpkhBCiKtXrwqNRlNq1uRGAwYMECNHjnRoe/jhh0WLFi0c2t577z2HMWdlZQkhhOjWrZt4++23Hfp+9tlnIjQ01P58yZIlwsPDQ+Tk5AghhMjJyREeHh7i448/LlWPr6+vWLVqVbn1EtHt48wNEVWoe/fuSEhIcHiMHz/eoU+LFi3s33t4eMDLywupqakAgPj4eGzbtg2enp72R5MmTQAAZ86csR9382zKyZMn0b59e4e2Dh06lHp+9913Y/Xq1QCAzz77DPXr18e9995b7ngKCgqg1+tLtd84OwMAI0eOREJCAj7++GPk5eXZ99DEx8djzpw5DuMZPXo0kpOTkZ+fDwB4+umnYbPZsG7dOgDAunXrIITAoEGDSn2um5ub/Tgicg611AUQUc3m4eGBmJiYCvtoNBqH5wqFAjabDQBgs9nQr18/vPPOO6WOCw0NdficGwkhSgUOUcYm3eeeew6LFi3CK6+8gpUrV+LZZ58tddyNAgICkJmZ6dDWqFEj7Nq1CxaLxT4WHx8f+Pj44OLFiw59bTYbZs+ejccff7zUe5eEJoPBgCeffBIrV67EqFGj7Etl3t7epY7JyMhAYGBgufUS0e3jzA0RuVSbNm1w7NgxREVFISYmxuFxc6C5UZMmTbB//36HtgMHDpTqN2TIEFy4cAEffvghjh07huHDh1dYT+vWrXH8+HGHtqeffhq5ublYvHhxpcZz8uTJUmOJiYmBUnn9R+qoUaOwe/dubNy4Ebt378aoUaNKvdeZM2dgMpnQunXrW34uEVUeww0RVaiwsBApKSkOj7S0tEofP378eGRkZODpp5/Gvn37cPbsWfzyyy8YOXIkrFZruceNGTMG//vf//Dyyy/j1KlT+Oqrr7Bq1SoAjktIvr6+ePzxxzF9+nT06tUL4eHhFdbz0EMP4dixYw6zN506dcILL7yAF154AdOmTcOuXbtw/vx57N27F59++ikUCoU9uLz++utYvXo13njjDRw7dgwnTpzAunXr8Nprrzl8zn333YeYmBgMGzYMMTExZS6V7dy5Ew0aNEDDhg1v+edIRJXHcENEFdqyZQtCQ0MdHl27dq308WFhYdi9ezesViseeughxMbGYvLkyTAYDA4zHTeLjo7GN998g/Xr16NFixZYsmSJ/WwpnU7n0HfUqFEwm80YOXLkLetp3rw52rVrh6+++sqh/d1338UXX3yBQ4cO4ZFHHkGjRo3wj3/8AzabDXv27LEvKT300EPYuHEj4uLi0L59e9xzzz2YP38+IiMjS33WyJEjkZmZWW5da9euxejRo29ZMxHdHoUoaxGbiKgGeuutt7B06VIkJSU5tK9ZswaTJ0/G5cuXodVqb/k+mzdvxosvvoijR49WGLBc6ejRo+jRowdOnToFg8EgSQ1EcsUNxURUYy1evBjt27eHv78/du/ejf/85z+YMGGC/fX8/HwkJiZi7ty5GDNmTKWCDQD06dMHp0+fxqVLlxAREeGq8it0+fJlrF69msGGyAU4c0NENdbUqVOxbt06ZGRkoH79+hg6dChmzJgBtbr4/8veeOMNvPXWW7j33nvxww8/wNPTU+KKiagmYLghIiIiWeGGYiIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikhWGGyIiIpIVhhsiIiKSFYYbIiIikpX/Bz1AAWgqfKgPAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(150),f(range(1,151),popt[0]))\n",
    "plt.scatter(energy_list,resolutions)\n",
    "plt.xlabel('Energy (GeV)')\n",
    "plt.ylabel('Resolution')\n",
    "plt.title('Energy Resolution')\n",
    "plt.savefig(\"linreg_res.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(obj=model_0.state_dict, f=\"/home/dmisra/eic/model_0\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "be0ad538c402c22927254208aecd010220eb94d12e6a2da33653e1d12172e2c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
